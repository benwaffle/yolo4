{
    "basePath": "",
    "baseUrl": "https://autopush-visionkit-pa.sandbox.googleapis.com/",
    "batchPath": "batch",
    "description": "",
    "discoveryVersion": "v1",
    "documentationLink": "",
    "icons": {
        "x16": "http://www.google.com/images/icons/product/search-16.gif",
        "x32": "http://www.google.com/images/icons/product/search-32.gif"
    },
    "id": "autopush_visionkit_pa_sandbox:v1",
    "kind": "discovery#restDescription",
    "name": "autopush_visionkit_pa_sandbox",
    "ownerDomain": "google.com",
    "ownerName": "Google",
    "parameters": {
        "$.xgafv": {
            "description": "V1 error format.",
            "enum": [
                "1",
                "2"
            ],
            "enumDescriptions": [
                "v1 error format",
                "v2 error format"
            ],
            "location": "query",
            "type": "string"
        },
        "access_token": {
            "description": "OAuth access token.",
            "location": "query",
            "type": "string"
        },
        "alt": {
            "default": "json",
            "description": "Data format for response.",
            "enum": [
                "json",
                "media",
                "proto"
            ],
            "enumDescriptions": [
                "Responses with Content-Type of application/json",
                "Media download with context-dependent Content-Type",
                "Responses with Content-Type of application/x-protobuf"
            ],
            "location": "query",
            "type": "string"
        },
        "callback": {
            "description": "JSONP",
            "location": "query",
            "type": "string"
        },
        "fields": {
            "description": "Selector specifying which fields to include in a partial response.",
            "location": "query",
            "type": "string"
        },
        "key": {
            "description": "API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.",
            "location": "query",
            "type": "string"
        },
        "oauth_token": {
            "description": "OAuth 2.0 token for the current user.",
            "location": "query",
            "type": "string"
        },
        "prettyPrint": {
            "default": "true",
            "description": "Returns response with indentations and line breaks.",
            "location": "query",
            "type": "boolean"
        },
        "quotaUser": {
            "description": "Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.",
            "location": "query",
            "type": "string"
        },
        "uploadType": {
            "description": "Legacy upload protocol for media (e.g. \"media\", \"multipart\").",
            "location": "query",
            "type": "string"
        },
        "upload_protocol": {
            "description": "Upload protocol for media (e.g. \"raw\", \"multipart\").",
            "location": "query",
            "type": "string"
        }
    },
    "protocol": "rest",
    "resources": {
        "image": {
            "methods": {
                "annotate": {
                    "description": "Image annotation endpoint extending the on-device vision capabilities\nsupported by VisionKit.",
                    "flatPath": "v1/image:annotate",
                    "httpMethod": "POST",
                    "id": "autopush_visionkit_pa_sandbox.image.annotate",
                    "parameterOrder": [],
                    "parameters": {},
                    "path": "v1/image:annotate",
                    "request": {
                        "$ref": "VisionkitV1AnnotateImageRequest"
                    },
                    "response": {
                        "$ref": "VisionkitV1AnnotateImageResponse"
                    }
                }
            }
        },
        "overlay": {
            "methods": {
                "create": {
                    "description": "Overlay creation endpoint.\n\nIt creates a CL from proposed overlay configuration and addresses it to\ng/overlay-eng for review.",
                    "flatPath": "v1/overlay:create",
                    "httpMethod": "POST",
                    "id": "autopush_visionkit_pa_sandbox.overlay.create",
                    "parameterOrder": [],
                    "parameters": {},
                    "path": "v1/overlay:create",
                    "request": {
                        "$ref": "VisionkitV1CreateOverlayRequest"
                    },
                    "response": {
                        "$ref": "VisionkitV1CreateOverlayResponse"
                    }
                },
                "get": {
                    "description": "This endpoint returns the requested overlay, if whitelisted for the API\nkey[1] which is part of the request. Please refer to the overlay\nconfiguration proto[2] for more details.\n\n[1]: go/visionkit-server/get-api-key\n[2]:\ngoogle3/photos/vision/visionkit/server/boq/proto/overlay_configuration.proto?q=api_key_whitelist",
                    "flatPath": "v1/overlay:get",
                    "httpMethod": "POST",
                    "id": "autopush_visionkit_pa_sandbox.overlay.get",
                    "parameterOrder": [],
                    "parameters": {},
                    "path": "v1/overlay:get",
                    "request": {
                        "$ref": "VisionkitV1GetOverlayRequest"
                    },
                    "response": {
                        "$ref": "VisionkitV1GetOverlayResponse"
                    }
                },
                "inspect": {
                    "description": "Overlay inspection endpoint meant for development or debugging purposes.\n\nIMPORTANT: This should NOT be exposed to clients outside of Google since it\nreturns internal configuration parameters. This will be enforced later on.",
                    "flatPath": "v1/overlay:inspect",
                    "httpMethod": "POST",
                    "id": "autopush_visionkit_pa_sandbox.overlay.inspect",
                    "parameterOrder": [],
                    "parameters": {},
                    "path": "v1/overlay:inspect",
                    "request": {
                        "$ref": "VisionkitV1InspectOverlayRequest"
                    },
                    "response": {
                        "$ref": "VisionkitV1InspectOverlayResponse"
                    }
                },
                "update": {
                    "description": "Overlay update endpoint.\n\nSimilarly to the CreateOverlay endpoint, creates a CL from proposed\noverlay configuration and addresses it to g/overlay-eng for review.",
                    "flatPath": "v1/overlay:update",
                    "httpMethod": "POST",
                    "id": "autopush_visionkit_pa_sandbox.overlay.update",
                    "parameterOrder": [],
                    "parameters": {},
                    "path": "v1/overlay:update",
                    "request": {
                        "$ref": "VisionkitV1UpdateOverlayRequest"
                    },
                    "response": {
                        "$ref": "VisionkitV1UpdateOverlayResponse"
                    }
                }
            }
        },
        "overlays": {
            "methods": {
                "list": {
                    "description": "The VisionKitService supports switching between multiple configurations\nmade to customize the server and (optionally) the client behavior to a\nspecific end-to-end visual search experience, a.k.a. \"overlay\".\n\nThis endpoint returns the list of available overlays for the API key[1]\nwhich is part of the request. Please refer to the overlay configuration\nproto[2] for more details.\n\n[1]: go/visionkit-server/get-api-key\n[2]:\ngoogle3/photos/vision/visionkit/server/boq/proto/overlay_configuration.proto?q=api_key_whitelist",
                    "flatPath": "v1/overlays:list",
                    "httpMethod": "POST",
                    "id": "autopush_visionkit_pa_sandbox.overlays.list",
                    "parameterOrder": [],
                    "parameters": {},
                    "path": "v1/overlays:list",
                    "request": {
                        "$ref": "VisionkitV1ListOverlaysRequest"
                    },
                    "response": {
                        "$ref": "VisionkitV1ListOverlaysResponse"
                    }
                }
            }
        }
    },
    "revision": "20190920",
    "rootUrl": "https://autopush-visionkit-pa.sandbox.googleapis.com/",
    "schemas": {
        "AccelerationAcceleration": {
            "description": "One possible acceleration configuration.\n\nSTATUS: in use.",
            "id": "AccelerationAcceleration",
            "properties": {
                "modelIdentifierForStatistics": {
                    "$ref": "AccelerationInferenceToUseFor",
                    "description": "Copy of the matching InferenceToUseFor for gathering statistics that can be\nused in telemetry, integration testing and benchmarking."
                },
                "preference": {
                    "description": "Which preference to use this accelerator for.",
                    "enum": [
                        "ANY_EXECUTION_PREFERENCE",
                        "NO_EXECUTION_PREFERENCE",
                        "LOW_LATENCY",
                        "LOW_POWER",
                        "FORCE_CPU"
                    ],
                    "enumDescriptions": [
                        "Match any selected preference. Whitelist (semantically - value is same as\non input).",
                        "Do not care about prefernce in whitelist. Input (semantically - value is\nsame as in whitelist).",
                        "Match low latency preference. Both whitelist and input.",
                        "Math low power preference. Both whitelist and input.",
                        "Never accelerate. Can be used for input to whitelist selection or for\nstandalone Acceleration configuration."
                    ],
                    "type": "string"
                },
                "tfliteSettings": {
                    "$ref": "AccelerationTFLiteSettings",
                    "description": "How to configure TFLite\nThis can grow to non-TFLite acceleration like Xeno or threading options for\ncustom inference."
                }
            },
            "type": "object"
        },
        "AccelerationFallbackSettings": {
            "description": "Whether to automatically fallback to TFLite CPU path on delegation errors.\n\nSTATUS: implemented, allow_automatic_fallback_on_execution_error should be\nconsidered experimental.",
            "id": "AccelerationFallbackSettings",
            "properties": {
                "allowAutomaticFallbackOnCompilationError": {
                    "description": "Whether to allow automatically falling back to TfLite CPU path on\ncompilation failure. Default is not allowing automatic fallback.\n\nThis is useful in naive production usecases where the caller would prefer\nfor the model to run even if it's not accelerated. More advanced users will\nimplement fallback themselves; e.g., by using a different model on CPU. See\ndiscussion in b/138941377. Testing the real error conditions is also\ntricky.\n\nNote that compilation errors may occur either at initial\nModifyGraphWithDelegate() time, or when calling AllocateTensors() after\nresizing.",
                    "type": "boolean"
                },
                "allowAutomaticFallbackOnExecutionError": {
                    "description": "Whether to allow automatically falling back to TfLite CPU path on\nexecution error. Default is not allowing automatic fallback.\n\nExperimental, use with care (only when you have complete control over the\nclient code).\n\nThe caveat above for compilation error holds.  Additionally, execution-time\nerrors are harder to handle automatically as they require invalidating the\nTfLite interpreter which most client code has not been designed to deal\nwith. See b/138941377.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "AccelerationHexagonDelegateSettings": {
            "description": "Hexagon Delegate settings",
            "id": "AccelerationHexagonDelegateSettings",
            "properties": {
                "noOfHexagonInstancesToCache": {
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "AccelerationInferenceToUseFor": {
            "description": "Identification of a model to match against.\n\nSTATUS: in use.",
            "id": "AccelerationInferenceToUseFor",
            "properties": {
                "model": {
                    "$ref": "AccelerationModelIdentifier",
                    "description": "Targeted TFLite model.\nVision Kit Pipeline stage the entry is for. May be empty if the\nwhitelist is for a standalone model.\n\nSTATUS: not implemented yet\noptional string pipeline_stage_id = 3;"
                },
                "modelNamespace": {
                    "description": "Namespace for this identifier. Currently model identifiers can be ad-hoc,\nso the namespacing is necessary to ensure different clients of whitelisting\nare not mixed up without having to separate the whitelists themselves. The\nnamespace should be in reverse domain notation.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "AccelerationModelIdentifier": {
            "description": "How to identify a model.\n\nSTATUS: in use.",
            "id": "AccelerationModelIdentifier",
            "properties": {
                "modelFingerprint": {
                    "description": "V1: \"v1:\" + farmhash of the whole tflite file.\n\nSTATUS: not implemented in support library but utility code exists for\ngeneration. Next step is to at least use for warnings when models change.",
                    "type": "string"
                },
                "modelId": {
                    "description": "Custom identifier. Clients need to make sure the identifier changes if\nacceleration settings should change by e.g., using semver as part of the\nidentifier.\n\nSTATUS: in use.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "AccelerationNNAPIDelegateSettings": {
            "description": "NNAPI delegate settings.\n\nSTATUS: in use.",
            "id": "AccelerationNNAPIDelegateSettings",
            "properties": {
                "acceleratorName": {
                    "description": "Which instance (NNAPI accelerator) to use. One driver may provide several\naccelerators (though a driver may also hide several back-ends behind one\nname, at the choice of the driver vendor).\nNote that driver introspection is only available in Android Q and later.",
                    "type": "string"
                },
                "cacheDirectory": {
                    "description": "NNAPI model compilation caching settings to be passed to\ntflite::StatefulNnApiDelegate",
                    "type": "string"
                },
                "executionPreference": {
                    "description": "NNAPI execution preference to pass. See\nhttps://developer.android.com/ndk/reference/group/neural-networks.htmlhttps://developer.android.com/ndk/reference/group/neural-networks.html",
                    "enum": [
                        "NNAPI_EXECUTION_PREFERENCE_UNDEFINED",
                        "NNAPI_EXECUTION_PREFERENCE_LOW_POWER",
                        "NNAPI_EXECUTION_PREFERENCE_FAST_SINGLE_ANSWER",
                        "NNAPI_EXECUTION_PREFERENCE_SUSTAINED_SPEED"
                    ],
                    "enumDescriptions": [
                        "Undefined.",
                        "Prefer executing in a way that minimizes battery drain.",
                        "Prefer returning a single answer as fast as possible, even if this causes\nmore power consumption.",
                        "Prefer maximizing the throughput of successive frames, for example when\nprocessing successive frames coming from the camera."
                    ],
                    "type": "string"
                },
                "fallbackSettings": {
                    "$ref": "AccelerationFallbackSettings",
                    "description": "Whether to automatically fall back to TFLite CPU path."
                },
                "modelToken": {
                    "type": "string"
                },
                "noOfNnapiInstancesToCache": {
                    "description": "Number of instances to cache for the same model (for input size\nchanges). This is mandatory for getting reasonable performance in that\ncase.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "AccelerationSystem": {
            "description": "Identification of a system configuration to match against.\n\nSTATUS: in use.",
            "id": "AccelerationSystem",
            "properties": {
                "androidApiLevel": {
                    "description": "Android version. Will be matched aginst the ro.build.version.sdk property.",
                    "format": "int32",
                    "type": "integer"
                },
                "androidBuildType": {
                    "description": "Android build type (eng, user, etc). Will match against the ro.build.type.",
                    "type": "string"
                },
                "androidDessertRelease": {
                    "description": "Android dessert release. Will match against first letter of ro.build.id.",
                    "type": "string"
                },
                "androidPrereleaseOnly": {
                    "description": "Android release type constraint.",
                    "type": "boolean"
                },
                "productName": {
                    "description": "Product name. Will be matched against the ro.product.name property.\nWe know some models work with NNAPI on specific device + Android version\nbut don't have a better characterization (e.g., Pixel 3 with Q has a\ndifferent Qualcomm driver than P, but there is no driver version available\non either to check).",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "soc": {
                    "description": "SystemOnChip (SoC). Will be matched against the ro.board.platform property.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "AccelerationTFLiteSettings": {
            "description": "How to configure TFLite.\n\nSTATUS: in use.",
            "id": "AccelerationTFLiteSettings",
            "properties": {
                "delegate": {
                    "description": "Which delegate to use.",
                    "enum": [
                        "DELEGATE_NONE",
                        "NNAPI",
                        "GPU",
                        "HEXAGON"
                    ],
                    "enumDescriptions": [
                        "",
                        "",
                        "",
                        ""
                    ],
                    "type": "string"
                },
                "hexagonSettings": {
                    "$ref": "AccelerationHexagonDelegateSettings",
                    "description": "optional GPUDelegateSettings gpu_settings = 3;\noptional CPUSettings cpu_settings = 4;"
                },
                "nnapiSettings": {
                    "$ref": "AccelerationNNAPIDelegateSettings"
                }
            },
            "type": "object"
        },
        "AccelerationWhitelist": {
            "description": "Acceleration whitelist - each entry identifies a model-system pair that can\nbe accelerated. Whitelists are meant to be useful as:\n- part of the configuration of ML features\n- bundled into ML libraries\n- updatable through e.g., ML Kit updates or remote config\n- merged from multiple sources\n- input to the support library to determine what accelerator to use\n\nMatching logic is as follow:\n- A field that has not been set is not checked for matching (e.g., an unset\n  soc field will match any System-on-Chip).\n- Most set fields will match exactly, except for:\n- ExecutionPreference::NO_EXECUTION_PREFERENCE (matches any)\n\nSTATUS: in use.",
            "id": "AccelerationWhitelist",
            "properties": {
                "entries": {
                    "items": {
                        "$ref": "AccelerationWhitelistEntry"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "AccelerationWhitelistEntry": {
            "description": "Single whitelist entry that matches a model against a system configuration,\ngiving a number of available acceleration settings.\n\nSTATUS: in use.",
            "id": "AccelerationWhitelistEntry",
            "properties": {
                "acceleration": {
                    "description": "What can be done on that hardware.",
                    "items": {
                        "$ref": "AccelerationAcceleration"
                    },
                    "type": "array"
                },
                "system": {
                    "$ref": "AccelerationSystem",
                    "description": "What system (hw and sw) is the entry applicable to."
                },
                "useFor": {
                    "$ref": "AccelerationInferenceToUseFor",
                    "description": "What model and stage this entry is for."
                }
            },
            "type": "object"
        },
        "GoogleTypeLatLng": {
            "description": "An object representing a latitude/longitude pair. This is expressed as a pair\nof doubles representing degrees latitude and degrees longitude. Unless\nspecified otherwise, this must conform to the\n<a href=\"http://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf\">WGS84\nstandard</a>. Values must be within normalized ranges.",
            "id": "GoogleTypeLatLng",
            "properties": {
                "latitude": {
                    "description": "The latitude in degrees. It must be in the range [-90.0, +90.0].",
                    "format": "double",
                    "type": "number"
                },
                "longitude": {
                    "description": "The longitude in degrees. It must be in the range [-180.0, +180.0].",
                    "format": "double",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "HumanSensingFaceAttribute": {
            "description": "Defines a generic attribute. The name field is the name of the attribute\n(for example beard, glasses, joy). The confidence defines how reliable the\ngiven annotation is. For binary attributes it is bounded between 0 and 1\nand can be interpreted as the posterior probability. The value field can be\nused for continuous attributes like age.\nInformation returned or stored in this message may be sensitive from a\nprivacy, policy, or legal point of view. Clients should consult with their\np-counsels and the privacy working group (go/pwg) to make sure their use\nrespects Google policies.",
            "id": "HumanSensingFaceAttribute",
            "properties": {
                "confidence": {
                    "format": "float",
                    "type": "number"
                },
                "name": {
                    "type": "string"
                },
                "type": {
                    "enum": [
                        "TYPE_UNKNOWN",
                        "FREE_FORM",
                        "FEMALE",
                        "MALE",
                        "AGE",
                        "NON_HUMAN",
                        "GLASSES",
                        "DARK_GLASSES",
                        "HEADWEAR",
                        "EYES_VISIBLE",
                        "MOUTH_OPEN",
                        "FACIAL_HAIR",
                        "LONG_HAIR",
                        "FRONTAL_GAZE",
                        "SMILING"
                    ],
                    "enumDescriptions": [
                        "",
                        "",
                        "Attribute types that describe the gender of a face. For an attribute\nif type FEMALE the confidence represent the probability of a face to\nbe from a female person. Similarly, for an attribute of type MALE\nthe confidence is the probability of a face to be from a male person.\n4 is reserved for OTHER_GENDER.",
                        "",
                        "Attribute type that represent the age of the face. For an attribute of\nthis type the field value represent the age. Values are assumed to be\nin the range [0, 95].",
                        "This attributes is used to distinguish actual human faces from other\npossible face detections like face of sculptures, cartoons faces, and\nsome false detections.",
                        "Attributes types that describes face appearances/configurations (mouth\nopen, eyes visibles and looking into the camera, smiling) and props\n(glasses, dark glasses, and headwear).",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        ""
                    ],
                    "type": "string"
                },
                "value": {
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "HumanSensingFaceAttributeIndexMap": {
            "id": "HumanSensingFaceAttributeIndexMap",
            "properties": {
                "indexToFaceAttributeMap": {
                    "additionalProperties": {
                        "$ref": "HumanSensingFaceAttribute"
                    },
                    "description": "The map key is the index in the output of the brain embedder, the map value\nis the corresponding face attribute.",
                    "type": "object"
                }
            },
            "type": "object"
        },
        "HumanSensingFaceAttributesClientOptions": {
            "description": "Options for setting up a FaceAttributesClient object.\nNext ID: 13\nDeprecated fields: 1, 6.",
            "id": "HumanSensingFaceAttributesClientOptions",
            "properties": {
                "ageAttributeResultType": {
                    "enum": [
                        "UNKNOWN",
                        "MOST_LIKELY_AGE",
                        "AGE_MASS_DISTRIBUTION"
                    ],
                    "enumDescriptions": [
                        "",
                        "Returns the most likely age over the mass distribution. The final results\nwill have type human_sensing::Face::Attribute:AGE and the value set to\nthe age.",
                        "Return the whole mass distribution. As many face attributes as elements\nof the mass distribution will be returned. Each attribute will be of type\nhuman_sensing::Face::Attribute::FREE_FORM with name like 'age_09' and\n'age_53' for the age 9 and age 53. The probabilities will be stored in\nthe confidence field of the returned face attributes."
                    ],
                    "type": "string"
                },
                "batchSize": {
                    "description": "Batch size. If set, faces will be processed in batches of this size. Note\nthat this is true even if only a single face is processed, so make sure\nthis is set appropriately for the use case. Batching will typically benefit\nhardware execution, where there is a high fixed cost per inference.",
                    "format": "int32",
                    "type": "integer"
                },
                "calibrationConfig": {
                    "$ref": "ImageContentAnnotationScoreCalibrationParameters"
                },
                "calibrationConfigPbtxtFile": {
                    "type": "string"
                },
                "faceAttributeClientName": {
                    "type": "string"
                },
                "faceAttributeIndexMap": {
                    "$ref": "HumanSensingFaceAttributeIndexMap"
                },
                "faceAttributeIndexMapPbtxtFile": {
                    "type": "string"
                },
                "modelPath": {
                    "description": "Path to the model file. Currently only used to support inference with\nTFLite's runtime interpreter; tf.mini implementation uses generated code\ninference.",
                    "type": "string"
                },
                "preferNnapiDelegate": {
                    "description": "Option to prefer hardware acceleration on Android. This currently prefers\nthe lowest power usage.",
                    "type": "boolean"
                },
                "returnMostConfidentGender": {
                    "description": "If true returns only the gender attribute (either male or female) with the\nhigher confidence. When false all the gender attributes will be returned.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "ImageContentAnnotationScoreCalibrationParameters": {
            "id": "ImageContentAnnotationScoreCalibrationParameters",
            "properties": {},
            "type": "object"
        },
        "MobileSsdAnchorGenerationOptions": {
            "description": "This is dervice from TensorFlow's SsdAnchorGenerator proto that is used to\nconfigures TensorFlow's anchor generator.\nhttps://cs.corp.google.com/piper///depot/google3/third_party/tensorflow_models/object_detection/protos/ssd_anchor_generator.proto",
            "id": "MobileSsdAnchorGenerationOptions",
            "properties": {
                "anchorAspectRatios": {
                    "description": "List of aspect ratios to generate anchors for. Aspect ratio is specified as\n(width/height)",
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "anchorOffsets": {
                    "description": "List of offset in pixels for each layer",
                    "items": {
                        "format": "int32",
                        "type": "integer"
                    },
                    "type": "array"
                },
                "anchorStrides": {
                    "description": "List of strides in pixels for each layer",
                    "items": {
                        "format": "int32",
                        "type": "integer"
                    },
                    "type": "array"
                },
                "baseAnchorHeight": {
                    "description": "The base anchor height in pixels",
                    "format": "int32",
                    "type": "integer"
                },
                "baseAnchorWidth": {
                    "description": "The base anchor width in pixels",
                    "format": "int32",
                    "type": "integer"
                },
                "imageHeight": {
                    "description": "The input image height in pixels",
                    "format": "int32",
                    "type": "integer"
                },
                "imageWidth": {
                    "description": "The input image width in pixels",
                    "format": "int32",
                    "type": "integer"
                },
                "maxAnchorScale": {
                    "description": "The maximum anchor scaling",
                    "format": "float",
                    "type": "number"
                },
                "minAnchorScale": {
                    "description": "The minimum anchor scaling (should be < 1.0)",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "MobileSsdBoxCoder": {
            "id": "MobileSsdBoxCoder",
            "properties": {
                "boxCornerOffsetCoder": {
                    "$ref": "MobileSsdBoxCornerOffsetCoder"
                },
                "centerSizeOffsetCoder": {
                    "$ref": "MobileSsdCenterSizeOffsetCoder"
                }
            },
            "type": "object"
        },
        "MobileSsdBoxCornerOffsetCoder": {
            "description": "The scaling value used to adjust predicted bounding box corners.\nFor example, given a prediction in BoxCornerEncoding and an anchor in\nCenterSizeEncoding, the decoded location is:\n  ymin = prediction.ymin * coder.stddev + anchor.y - anchor.h / 2\n  xmin = prediction.xmin * coder.stddev + anchor.x - anchor.w / 2\n  ymax = prediction.ymax * coder.stddev + anchor.y + anchor.h / 2\n  xmax = prediction.xmax * coder.stddev + anchor.x + anchor.w / 2\nThis coder doesn't support keypoints.\nSee mobile_ssd::DecodeBoxCornerBoxes for more details.\nThis coder is compatible with models trained using\nobject_detection.protos.MeanStddevBoxCoder.",
            "id": "MobileSsdBoxCornerOffsetCoder",
            "properties": {
                "stddev": {
                    "description": "The standard deviation used to encode and decode boxes.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "MobileSsdCenterSizeEncoding": {
            "description": "The bounding box representation by center location and width/height.\nAlso includes optional keypoint coordinates.\nIt is a default representation in modern object detection systems.",
            "id": "MobileSsdCenterSizeEncoding",
            "properties": {
                "h": {
                    "description": "Encoded anchor box height.",
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "keypointX": {
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "keypointY": {
                    "description": "Encoded keypoint coordinates.",
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "w": {
                    "description": "Encoded anchor box width.",
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "x": {
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "y": {
                    "description": "Encoded anchor box center.",
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "MobileSsdCenterSizeOffsetCoder": {
            "description": "The scaling factors for decoding predicted offsets with CenterSizeEncoding.\nFor example, given a prediction and an anchor in CenterSizeEncoding, the\ndecoded location is:\n  y = prediction.y / coder.y_scale() * anchor.h + anchor.y;\n  x = prediction.x / coder.x_scale() * anchor.w + anchor.x;\n  h = exp(prediction.h / coder.h_scale()) * anchor.h;\n  w = exp(prediction.w / coder.w_scale()) * anchor.w;\n  keypoint_y = prediction.keypoint_y / coder.keypoint_y_scale() * anchor.h\n               + anchor.y;\n  keypoint_x = prediction.keypoint_x / coder.keypoint_x_scale() * anchor.w\n               + anchor.x;\nSee mobile_ssd::DecodeCenterSizeBoxes for more details.\nThis coder is compatible with models trained using\nobject_detection.protos.FasterRcnnBoxCoder and\nobject_detection.protos.KeypointBoxCoder.",
            "id": "MobileSsdCenterSizeOffsetCoder",
            "properties": {
                "hScale": {
                    "description": "Scale factor for encoded box height offset.",
                    "format": "float",
                    "type": "number"
                },
                "keypointXScale": {
                    "format": "float",
                    "type": "number"
                },
                "keypointYScale": {
                    "description": "Scale factor for encoded keypoint coordinate offset.",
                    "format": "float",
                    "type": "number"
                },
                "wScale": {
                    "description": "Scale factor for encoded box width offset.",
                    "format": "float",
                    "type": "number"
                },
                "xScale": {
                    "format": "float",
                    "type": "number"
                },
                "yScale": {
                    "description": "Scale factor for encoded box center offset.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "MobileSsdClientOptions": {
            "description": "Next ID: 19",
            "id": "MobileSsdClientOptions",
            "properties": {
                "agnosticMode": {
                    "description": "SSD in single class agnostic model.",
                    "type": "boolean"
                },
                "anchorGenerationOptions": {
                    "$ref": "MobileSsdAnchorGenerationOptions",
                    "description": "Optional anchor generations options. This can be used to generate\nanchors for an SSD model. It is utilized in\nMobileSSDTfLiteClient::LoadAnchors()"
                },
                "boxCoder": {
                    "$ref": "MobileSsdBoxCoder",
                    "description": "Optional box coder specifications. This can be used for models trained\nwith a customized box coder. If unspecified, it will use\nCenterSizeOffsetCoder and its default parameters."
                },
                "classNameWhitelist": {
                    "description": "Optional whitelist of class names. If non-empty, detections whose class\nname is not in this set will be filtered out. Duplicate or unknown class\nnames are ignored.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "computeSettings": {
                    "$ref": "AccelerationAcceleration",
                    "description": "How to accelerate the model (e.g., via NNAPI). If this is set, automatic\nwhitelisting is not used."
                },
                "externalFiles": {
                    "$ref": "MobileSsdClientOptionsExternalFiles",
                    "description": "The external model files used to create the detector.\nThis is an alternative to registered models, where you specify external\nmodel via the following:\n- model using model_file_name or model_file_content\n- labelmap using label_map_file_content\n- anchors using anchor_generation_options,proto ("
                },
                "fullyConv": {
                    "description": "Fully convolutional mode, which requires on-the-fly anchor generation.",
                    "type": "boolean"
                },
                "iouThreshold": {
                    "description": "The threshold on intersection-over-union used by non-maxima suppression.",
                    "format": "float",
                    "type": "number"
                },
                "maxCategories": {
                    "description": "The maximum number of categories to return per detection.",
                    "format": "uint32",
                    "type": "integer"
                },
                "maxDetections": {
                    "description": "The maximum number of detections to return.",
                    "format": "uint32",
                    "type": "integer"
                },
                "mobileSsdClientName": {
                    "description": "The name of the Mobile SSD Client.",
                    "type": "string"
                },
                "numKeypoints": {
                    "description": "Number of keypoints.",
                    "format": "uint32",
                    "type": "integer"
                },
                "numThreads": {
                    "description": "Number of threads to be used by TFlite interpreter for SSD inference. Does\nsingle-threaded inference by default.",
                    "format": "int32",
                    "type": "integer"
                },
                "preferNnapiDelegate": {
                    "description": "Whether to use NNAPI (go/android-nn) delegate for hardware acceleration.\nIf it fails, it will fall back to the normal CPU execution.\nDeprecated, use compute_settings instead. If both are set, the\ninteraction is somewhat complicated (do not set both...):\n- setting either will disable automatic whitelisting\n- setting this to true will enable NNAPI, independent of the value in\n  compute_settings\n- setting this to false will still allow compute_settings select NNAPI",
                    "type": "boolean"
                },
                "quantize": {
                    "description": "Quantized model.",
                    "type": "boolean"
                },
                "scoreThreshold": {
                    "description": "The global score threshold below which detections are rejected.",
                    "format": "float",
                    "type": "number"
                },
                "ssdModel": {
                    "$ref": "MobileSsdMobileSsdModel",
                    "description": "Optional model data, including inference graph, anchors and labelmap."
                },
                "weightedNms": {
                    "description": "Perform weighted non-max suppression.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "MobileSsdClientOptionsExternalFiles": {
            "id": "MobileSsdClientOptionsExternalFiles",
            "properties": {
                "anchorFileContent": {
                    "description": "Content of the anchor file. If provided, this takes precedence over\nthe anchor_file_name field.",
                    "format": "byte",
                    "type": "string"
                },
                "anchorFileName": {
                    "description": "Path to the anchor file.",
                    "type": "string"
                },
                "labelMapFileContent": {
                    "description": "Content of the label map file. If provided, this takes precedence over\nthe label_map_file_name field.",
                    "format": "byte",
                    "type": "string"
                },
                "labelMapFileName": {
                    "description": "Path to the label map file.",
                    "type": "string"
                },
                "modelFileContent": {
                    "description": "Content of the model file. If provided, this takes precedence over the\nmodel_file_name field.",
                    "format": "byte",
                    "type": "string"
                },
                "modelFileName": {
                    "description": "Path to the model file in FlatBuffer format.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "MobileSsdMobileSsdModel": {
            "id": "MobileSsdMobileSsdModel",
            "properties": {
                "anchors": {
                    "$ref": "MobileSsdCenterSizeEncoding",
                    "description": "Embedded anchors."
                },
                "labelmap": {
                    "$ref": "MobileSsdStringIntLabelMapProto",
                    "description": "Embedded labelmap."
                },
                "modelData": {
                    "$ref": "MobileSsdModelData",
                    "description": "Path to the model file."
                }
            },
            "type": "object"
        },
        "MobileSsdModelData": {
            "id": "MobileSsdModelData",
            "properties": {
                "embeddedModel": {
                    "format": "byte",
                    "type": "string"
                },
                "modelFile": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "MobileSsdStringIntLabelMapItem": {
            "id": "MobileSsdStringIntLabelMapItem",
            "properties": {
                "childName": {
                    "description": "Optional list of children used to represent a hierarchy. Those are assumed\nto correspond to subcategories[1] in JFT (go/jft) terminology.\n\nE.g.:\n\nitem {\n  name: \"/m/02xwb\" # Fruit\n  child_name: \"/m/014j1m\" # Apple\n  child_name: \"/m/0388q\" # Grape\n  ...\n}\nitem {\n  name: \"/m/014j1m\" # Apple\n  ...\n}\n\n[1]: google3/photos/vision/mining/proto/visual_entity.proto?q=SUBCATEGORY",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "displayName": {
                    "type": "string"
                },
                "embedding": {
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "id": {
                    "format": "int32",
                    "type": "integer"
                },
                "name": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "MobileSsdStringIntLabelMapProto": {
            "id": "MobileSsdStringIntLabelMapProto",
            "properties": {
                "item": {
                    "items": {
                        "$ref": "MobileSsdStringIntLabelMapItem"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "TensorflowServingModelSpec": {
            "description": "Metadata for an inference request such as the model name and version.",
            "id": "TensorflowServingModelSpec",
            "properties": {
                "name": {
                    "description": "Required servable name.",
                    "type": "string"
                },
                "signatureName": {
                    "description": "A named signature to evaluate. If unspecified, the default signature will\nbe used.",
                    "type": "string"
                },
                "version": {
                    "description": "Use this specific version number.",
                    "format": "int64",
                    "type": "string"
                },
                "versionLabel": {
                    "description": "Use the version associated with the given label.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitAmbientOptions": {
            "description": "Defines configs for ambient sensing pipeline.",
            "id": "VisionkitAmbientOptions",
            "properties": {
                "liftDetectorOptions": {
                    "$ref": "VisionkitLiftDetectorOptions"
                },
                "outputFile": {
                    "description": "A file on-device for saving the sensor data and inference results. For\ndebugging purpose only.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitAnalyticsOptions": {
            "id": "VisionkitAnalyticsOptions",
            "properties": {
                "enableAccelerationAnalytics": {
                    "description": "Enable acceleration analytics. If this is set to true go/acceleration@scale\nlogging is enabled and Pipeline::GetAnalytics() fills in\nAnalytics.acceleration_log_events,",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitAsynchronousApiOptions": {
            "id": "VisionkitAsynchronousApiOptions",
            "properties": {
                "blockingMode": {
                    "description": "Specifies what the pipeline should block on before returning from the\nasynchronous API calls (ReceivePreviewFrame and ReceiveYuvFrame).\nFor production apps, BlockingMode.NONE should be used.\nFor prototyping or debugging, BlockingMode.BLOCK_ON_ALL is useful in order\nto ensure there is only one packet going through the graph at a time.",
                    "enum": [
                        "NONE",
                        "BLOCK_ON_ALL"
                    ],
                    "enumDescriptions": [
                        "",
                        "Blocks on all engines."
                    ],
                    "type": "string"
                },
                "resultsAccumulatorOptions": {
                    "$ref": "VisionkitResultsAccumulatorOptions"
                }
            },
            "type": "object"
        },
        "VisionkitBarcodeOptions": {
            "description": "Barcode recognition options.\nNext Id: 6",
            "id": "VisionkitBarcodeOptions",
            "properties": {
                "computeSettings": {
                    "$ref": "AccelerationAcceleration",
                    "description": "Optional acceleration settings to be passed to the engine.\nIt is supported only for versions greather than or equal to V2"
                },
                "engineVersion": {
                    "description": "Optional engine version. The original engine is used as a sane default.",
                    "enum": [
                        "UNSPECIFIED",
                        "V1",
                        "V2"
                    ],
                    "enumDescriptions": [
                        "",
                        "Original Barhopper Mobile engine.",
                        "Barhopper Mobile engine featuring a deep learning based detector stage.\nSee go/barhopper-mobile-v2 for more details."
                    ],
                    "type": "string"
                },
                "formats": {
                    "description": "Barcode formats to look for. If empty, read all supported barcodes with\nthe exception of YouTube codes (YT_CODE).",
                    "enumDescriptions": [
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "",
                        "NOTE: Reserve 14 (0x2000) for DATABAR."
                    ],
                    "items": {
                        "enum": [
                            "UNSPECIFIED",
                            "CODE_128",
                            "CODE_39",
                            "CODE_93",
                            "CODABAR",
                            "DATA_MATRIX",
                            "EAN_13",
                            "EAN_8",
                            "ITF",
                            "QR_CODE",
                            "UPC_A",
                            "UPC_E",
                            "PDF417",
                            "AZTEC",
                            "YT_CODE"
                        ],
                        "type": "string"
                    },
                    "type": "array"
                },
                "minFractionBarcodeSize": {
                    "description": "Minimum relative size of barcode to be output. If supplied, barcode results\nwith a size fraction less than (boxWidth / imageWidth) AND\n(boxHeight / imageHeight) are filtered out.",
                    "format": "float",
                    "type": "number"
                },
                "parseValue": {
                    "description": "When this field is set, the underlying engine will parse (aka. annotate)\nthe barcode value.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitBoxTrackerOptions": {
            "id": "VisionkitBoxTrackerOptions",
            "properties": {
                "minMergeIou": {
                    "description": "Merges a fresh detection with a tracked object if their\nintersection-over-union is more than this value.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitCaptureTrigger": {
            "description": "Defines a capture trigger for switching from a StreamAnnotator to a\nFrameAnnotator.",
            "id": "VisionkitCaptureTrigger",
            "properties": {
                "shouldStopStream": {
                    "description": "Whether the capture trigger should also stop the camera stream.",
                    "type": "boolean"
                },
                "type": {
                    "description": "Mandatory capture trigger type.",
                    "enum": [
                        "UNSPECIFIED",
                        "TAP",
                        "HOVER"
                    ],
                    "enumDescriptions": [
                        "Unspecified trigger type. Using this will cause sanity checks to fail.",
                        "Switching from the StreamAnnotator to the FrameAnnotator is triggered by\nthe user tapping on the screen.",
                        "Switching from the StreamAnnotator to the FrameAnnotator is triggered by\nhovering over the same object for long enough."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitClassThreshold": {
            "id": "VisionkitClassThreshold",
            "properties": {
                "className": {
                    "description": "The class name, as defined in the label map `name` field.",
                    "type": "string"
                },
                "scoreThreshold": {
                    "description": "The threshold as probit, i.e. in the [0,1] range.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitClassThresholds": {
            "id": "VisionkitClassThresholds",
            "properties": {
                "classThreshold": {
                    "description": "One threshold per supported class.",
                    "items": {
                        "$ref": "VisionkitClassThreshold"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitClassTriggerCondition": {
            "description": "Condition for triggering when a given label classification is above a certain\nconfidence threshold.",
            "id": "VisionkitClassTriggerCondition",
            "properties": {
                "className": {
                    "description": "Name of the label to match (e.g., '/m/015bv3'). Used on the\nCLASSIFICATION_RESULT input stream.",
                    "type": "string"
                },
                "classNamePrefix": {
                    "description": "Name prefix of the label to match (e.g., 'text' will match 'text0'). Used\non the COARSE_CLASSIFICATION_RESULT input stream.",
                    "type": "string"
                },
                "classifierName": {
                    "description": "The classifier name that is associated with the label.",
                    "type": "string"
                },
                "threshold": {
                    "description": "Minimum confidence the label should have to trigger a match [0.0..1.0).",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitClassificationCascadeOptions": {
            "id": "VisionkitClassificationCascadeOptions",
            "properties": {
                "classifierClientOptions": {
                    "$ref": "VisionkitClassifierClientOptions",
                    "description": "Required. Options for the classifier client."
                },
                "predictionFilterOptions": {
                    "$ref": "VisionkitPredictionFilterOptions",
                    "description": "Required. Options for the prediction filter."
                }
            },
            "type": "object"
        },
        "VisionkitClassifier": {
            "description": "Configures an annotator using Servomatic for classification.",
            "id": "VisionkitClassifier",
            "properties": {
                "servoClassifierParameters": {
                    "$ref": "VisionkitServoClassifierParameters",
                    "description": "The parameters needed to call Servomatic for classification."
                }
            },
            "type": "object"
        },
        "VisionkitClassifierCascade": {
            "description": "Configures an annotator using Servomatic for detection and\nclassification.",
            "id": "VisionkitClassifierCascade",
            "properties": {
                "allowClassificationWithoutDetection": {
                    "description": "If true, classification is performed on the entire frame if the detection\nstep returned no result. Otherwise, classification is skipped and this\nannotator returns no result at all.",
                    "type": "boolean"
                },
                "defaultServoClassifierParameters": {
                    "$ref": "VisionkitServoClassifierParameters",
                    "description": "The default ServoClassifierParameters to use if no key in\nservo_classifier_parameters_map matches the category returned by detection.\nRequired."
                },
                "servoClassifierParametersMap": {
                    "additionalProperties": {
                        "$ref": "VisionkitServoClassifierParameters"
                    },
                    "description": "Maps the categories returned by detection to the ServoClassifierParameters\nto use. Optional.",
                    "type": "object"
                },
                "servoDetectorParameters": {
                    "$ref": "VisionkitServoDetectorParameters",
                    "description": "The parameters needed to call Servomatic for detection. Required."
                }
            },
            "type": "object"
        },
        "VisionkitClassifierClientOptions": {
            "description": "Options for setting up a ClassifierClient object.\nNext Id: 10",
            "id": "VisionkitClassifierClientOptions",
            "properties": {
                "classNameBlacklist": {
                    "description": "Optional blacklist of class names. If non-empty, classifications whose\nclass name is in this set will be filtered out. Duplicate or unknown\nclass names are ignored. Mutually exclusive with class_name_whitelist.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "classNameWhitelist": {
                    "description": "Optional whitelist of class names. If non-empty, classifications whose\nclass name is not in this set will be filtered out. Duplicate or unknown\nclass names are ignored. Mutually exclusive with class_name_blacklist.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "classifierClientName": {
                    "description": "The class name corresponding to a registered classifier, e.g. \"FooNet\"\nfor a classifier registered via `REGISTER_CLASSIFIER_CLIENT(FooNet);`.\n\nThis is also known as bundled models since the model files are embedded in\nthe final binary. See external_files for an alternative. Mutually exclusive\nwith external_files.",
                    "type": "string"
                },
                "computeSettings": {
                    "$ref": "AccelerationAcceleration",
                    "description": "How to accelerate the model (e.g., via NNAPI). If this is set, automatic\nwhitelisting[1] provided by the VisionKit Pipeline[2] is not used.\n\n[1]: google3/photos/vision/visionkit/hardware/android_hardware_context.h\n[2]:\ngoogle3/photos/vision/visionkit/pipeline/scheduler.cc?q=AndroidHardwareContext"
                },
                "externalFiles": {
                    "$ref": "VisionkitClassifierClientOptionsExternalFiles"
                },
                "maxResults": {
                    "description": "Maximum number of top scored results to return. If < 0, all results will be\nreturned. If 0, an invalid argument error is returned.",
                    "format": "int32",
                    "type": "integer"
                },
                "preferNnapiDelegate": {
                    "description": "Whether to prefer the use of NNAPI (go/android-nn) for hardware\naccelerated inference. If it fails to use the NNAPI, it will fall back to\nnormal CPU execution.\nDeprecated, use compute_settings instead. If both are set, the interaction\nis somewhat complicated (do not set both...):\n- setting either will disable automatic whitelisting\n- setting this to true will enable NNAPI, independent of the value in\n  compute_settings\n- setting this to false will still allow compute_settings to select NNAPI",
                    "type": "boolean"
                },
                "pruneAncestors": {
                    "description": "If true, and if the label map specifies a hierarchy between labels[1], a\npost-processing stage is performed after inference to remove coarse-grained\nclasses from the classification results. A class is considered\ncoarse-grained if the results contain a direct descendant.\n\nExample: \"Food\" is an ancestor of \"Banana\". So if both classes are part of\nthe results, \"Food\" will be pruned out.\n\nDo not turn on this option if the label map is entirely flat, i.e. if no\nsub-classes have been specified via the `child_name`[1] field. Otherwise\nan error is returned at ClassifierClient creation time.\n\n[1]:\ngoogle3/photos/vision/object_detection/mobile/proto/labelmap.proto?q=child_name",
                    "type": "boolean"
                },
                "scoreThreshold": {
                    "description": "Score threshold in [0,1[. Results below this value are rejected.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitClassifierClientOptionsExternalFiles": {
            "description": "The external model and label map files used to create the classifier (both\nare required to be populated when using external files).\n\nThis is an alternative to registered models. Mutually exclusive with\nclassifier_client_name.",
            "id": "VisionkitClassifierClientOptionsExternalFiles",
            "properties": {
                "labelMapFile": {
                    "$ref": "VisionkitExternalFile"
                },
                "modelFile": {
                    "$ref": "VisionkitExternalFile"
                }
            },
            "type": "object"
        },
        "VisionkitCloudAnnotator": {
            "description": "Defines a cloud annotator.\nNext ID: 9.",
            "id": "VisionkitCloudAnnotator",
            "properties": {
                "classifier": {
                    "$ref": "VisionkitClassifier"
                },
                "classifierCascade": {
                    "$ref": "VisionkitClassifierCascade"
                },
                "matcher": {
                    "$ref": "VisionkitMatcher"
                },
                "metadataAugmenter": {
                    "description": "The optional MetadataAugmenter used to augment the labels returned by the\nCloudAnnotator.",
                    "enum": [
                        "METADATA_AUGMENTATION_NONE",
                        "METADATA_AUGMENTATION_GOOGLE_SEARCH_URL",
                        "METADATA_AUGMENTATION_KNOWLEDGE_GRAPH",
                        "METADATA_AUGMENTATION_REVGEO",
                        "METADATA_AUGMENTATION_MET_MUSEUM_URL"
                    ],
                    "enumDescriptions": [
                        "Do not perform any metadata augmentations.",
                        "Build a Google Search URL from the label and use it as `link` [1] field in\nthe returned ObjectMetadata objects. The `display_name` and `thumbnail_url`\nare simply picked from the AnnotatorContext. This assumes the labels are\nhuman-readable strings.\n\n[1]: google3/google/internal/visionkit/v1/annotate_image.proto?q=link",
                        "Extract a display name, thumbnail URL and English wikipedia URL from the\nKnowledge Graph.\nThis requires the CloudAnnotator to produce MIDs as labels.",
                        "Extract a display name, iconic image URL and Google Maps URL from the\nRevGeo (go/revgeo) and Places (go/places-api-stubby) APIs.\nThis requires the CloudAnnotator to produce S2 Cell Tokens (formatted as\nhexadecimal strings without the \"0x\" prefix) as labels.",
                        "Build a redirect URL to the Met Museum (www.metmuseum.org) from the label\nand use it as `link` [1] field in the returned ObjectMetadata objects."
                    ],
                    "type": "string"
                },
                "mixer": {
                    "$ref": "VisionkitMixer"
                },
                "reverseImageSearcher": {
                    "$ref": "VisionkitReverseImageSearcher"
                },
                "searcher": {
                    "$ref": "VisionkitSearcher"
                },
                "searcherCascade": {
                    "$ref": "VisionkitSearcherCascade"
                }
            },
            "type": "object"
        },
        "VisionkitCloudCascadeOptions": {
            "description": "Message to configure a cloud cascade powered by the VisionKit Server (see\ngo/visionkit-server for more details). In such a case the recognition is\nentirely performed on the server-side based on the query image and additional\nrequest parameters (e.g. the overlay name specifying which end-to-end vision\nexperience is targeted) sent by the VisionKit Pipeline.\nNEXT ID: 6",
            "id": "VisionkitCloudCascadeOptions",
            "properties": {
                "detectionType": {
                    "enum": [
                        "NONE",
                        "EXTERNAL"
                    ],
                    "enumDescriptions": [
                        "The cloud cascade will operate on the entire frame.",
                        "A detection is provided externally by a call to ReceiveDetections. It is\nused to crop the input frame before querying the server."
                    ],
                    "type": "string"
                },
                "maxImageSize": {
                    "description": "If > 0, scale down query image if its maximum dimension is higher than this\nsize before sending the request to the server. If <= 0 this has no effect.",
                    "format": "int32",
                    "type": "integer"
                },
                "overlayName": {
                    "description": "Name of the overlay to be queried by the server. See\ngo/visionkit-server/api.",
                    "type": "string"
                },
                "requiresLocationContext": {
                    "description": "Whether the corresponding overlay requires access to the user location.\nThat is, the latitude and longitude must be set as additional parameters in\nthe request sent to the server. This is handled by the VisionKit Pipeline\nprovided that the input repository is enabled[1] and lat/lng are set using\nthe ReceiveSensorEvent API[2].\n\n[1]: see `enable_input_repository` in\ngoogle3/photos/vision/visionkit/pipeline/proto/scheduler.proto\n[2]:\ngoogle3/photos/vision/visionkit/pipeline/pipeline.h?q=ReceiveSensorEvent",
                    "type": "boolean"
                },
                "rpcClientOptions": {
                    "$ref": "VisionkitRpcClientOptions",
                    "description": "Base options for the gRPC client configuration like the API key required\nfor authentication purpose."
                }
            },
            "type": "object"
        },
        "VisionkitCloudMatcherOptions": {
            "description": "Message to configure the cloud part of a Congas (go/congas) matching cascade.\nIt is powered by the VisionKit Server (see go/visionkit-server for more\ndetails).\nNEXT ID: 5",
            "id": "VisionkitCloudMatcherOptions",
            "properties": {
                "jpegQuality": {
                    "description": "Quality of the JPEG compression applied when converting the image pixels to\na JPEG image (before sending the image to the server).",
                    "format": "int32",
                    "type": "integer"
                },
                "maxImageSize": {
                    "description": "If > 0, scale down query image if its maximum dimension is higher than this\nsize before sending the request to the server. If <= 0 this has no effect.",
                    "format": "int32",
                    "type": "integer"
                },
                "overlayName": {
                    "description": "Name of the overlay to be queried by the server. See\ngo/visionkit-server/api.",
                    "type": "string"
                },
                "rpcClientOptions": {
                    "$ref": "VisionkitRpcClientOptions",
                    "description": "Base options for the gRPC client configuration like the API key required\nfor authentication purpose."
                }
            },
            "type": "object"
        },
        "VisionkitCloudObjectRecognitionCascade": {
            "description": "Defines a full server-side object recognition cascade.\n\nIt is the counterpart of visionkit.internal.cloud.ObjectRecognitionCascade\nwhich by opposition includes internal-only fields (e.g. server-side detector\nparameters) that are required to power such a cascade on the server-side.\nSuch parameters should not be exposed to the client consuming the Overlay\nproto by means of google3/google/internal/visionkit/v1/overlays.proto.\n\nPlease have a look at cloud_cascades_internal.proto for more details.\n\nNext ID: 7.",
            "id": "VisionkitCloudObjectRecognitionCascade",
            "properties": {
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the cloud cascade."
                },
                "trigger": {
                    "$ref": "VisionkitCaptureTrigger",
                    "description": "Mandatory trigger that specifies when the annotator runs."
                }
            },
            "type": "object"
        },
        "VisionkitCloudSearcherOptions": {
            "description": "Message to configure the cloud part of a searcher cascade. It is powered by\nthe VisionKit Server (see go/visionkit-server for more details).\nThe cloud searcher systematically uses the on-device extracted embedding to\npopulate the request to the server. There is no way to pass the image bytes\nlike in a regular cloud cascade.\nNEXT ID: 5",
            "id": "VisionkitCloudSearcherOptions",
            "properties": {
                "overlayName": {
                    "description": "Name of the overlay to be queried by the server. See\ngo/visionkit-server/api.",
                    "type": "string"
                },
                "requiresLocationContext": {
                    "description": "Whether the corresponding overlay requires access to the user location.\nThat is, the latitude and longitude must be set as additional parameters in\nthe request sent to the server. This is handled by the VisionKit Pipeline\nprovided that the input repository is enabled[1] and lat/lng are set using\nthe ReceiveSensorEvent API[2].\n\n[1]: see `enable_input_repository` in\ngoogle3/photos/vision/visionkit/pipeline/proto/scheduler.proto\n[2]:\ngoogle3/photos/vision/visionkit/pipeline/pipeline.h?q=ReceiveSensorEvent",
                    "type": "boolean"
                },
                "rpcClientOptions": {
                    "$ref": "VisionkitRpcClientOptions",
                    "description": "Base options for the gRPC client configuration like the API key required\nfor authentication purpose."
                },
                "searchRestrictOptions": {
                    "$ref": "VisionkitSearchRestrictOptions",
                    "description": "Options used to specify how search restricts should be set at query time.\nIf left empty, no search restricts are set in the request."
                }
            },
            "type": "object"
        },
        "VisionkitContextBasedOptimizationOptions": {
            "id": "VisionkitContextBasedOptimizationOptions",
            "properties": {
                "decisionTimeoutUs": {
                    "description": "Timeout for the existing decision. If set, the existing decision\nis invalid if its age surpasses decision_timeout_us.",
                    "format": "int64",
                    "type": "string"
                },
                "skipDetectionTrackingConfidence": {
                    "description": "Minimum confidence to trust the tracker. If the tracking confidence\nis lower than this value, the vision engine is triggered.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitDetectionCascadeOptions": {
            "description": "Detection cascade options. The cascaded detector is built by cascading\nbounding box generated by the detector to the classifier to obtain object\nlabels. The detector_name and classifier_name represent the name of the\ndetector and classifier used to the build cascaded detector, respectively.\n\nNext Id: 14",
            "id": "VisionkitDetectionCascadeOptions",
            "properties": {
                "classThresholds": {
                    "$ref": "VisionkitClassThresholds",
                    "description": "If provided, filters out detections whose score is below the threshold\nfor their class. The thresholds must be expressed as probits (i.e. in\n[0,1]).\nAt runtime, the calculator takes care of converting these thresholds into\nlogits in case `detection_scores_are_probits` is false.\n\nNOTES:\n* this takes precedence over detector_client_options.score_threshold,\n* if both `class_thresholds` and `global_score_threshold` are provided,\n  both apply. This implies that if a class threshold is set to X and a\n  global score threshold set to Y, then the effective threshold for\n  detections of this class will be max(X,Y)."
                },
                "classifierClientOptions": {
                    "$ref": "VisionkitClassifierClientOptions",
                    "description": "The classifier model that is run on each detected object boxes. If not\nspecified, the input detection boxes are returned verbatim in the output\nresults."
                },
                "detectionScoresAreProbits": {
                    "description": "If true, input detection scores are expected to be probits in (i.e. in\n[0,1]). Otherwise input detection scores are expected to be logits (i.e. in\n]-inf,inf[).",
                    "type": "boolean"
                },
                "detectorClientOptions": {
                    "$ref": "MobileSsdClientOptions",
                    "description": "If provided, the Mobile SSD model used to detect objects in the incoming\ncamera frames. Do not configure such a model if you set use_external_boxes\nto true (see below)."
                },
                "doNotDiscardDetections": {
                    "description": "If true, the cascade will not discard detection results for which the\nclassifier model returned no classes. This happens when:\n\n1/ no confident results could be inferred with respect to the score\nthreshold configured for the classifier,\n2/ or, a confident classification result has been found (e.g. \"plant\") but\nhas not been whitelisted in the classifier options (e.g. \"fashion good\"\nonly).\n\nWith this option turned on, and when this happens, the corresponding\ndetection result is returned in the output results. The detection label is\ntypically coarse-grained or not specific at all depending on the configured\ndetector model, e.g. go/mobile-object-localizer supports a single \"object\"\nclass.\n\nNOTE: this has no effect when no classifier model is specified since in\nsuch a case all detection results are returned in the output results.",
                    "type": "boolean"
                },
                "filterParasiticDetections": {
                    "description": "Filters out the most confident result if it looks \"parasitic\". Please refer\nto DetectionFilterCalculatorOptions[1] for more details.\n\n[1]:\ngoogle3/photos/vision/visionkit/pipeline/drishti/calculators/detection_filter_calculator.proto",
                    "type": "boolean"
                },
                "globalScoreThreshold": {
                    "description": "If provided, filters out any detection of any class whose score is below\nthis threshold. This threshold must be expressed as a probit (i.e. in\n[0,1]).\nAt runtime, the calculator takes care of converting this threshold into a\nlogit in case `detection_scores_are_probits` is false.\n\nNOTES:\n* this takes precedence over detector_client_options.score_threshold,\n* if both `class_thresholds` and `global_score_threshold` are provided,\n  both apply. This implies that if a class threshold is set to X and a\n  global score threshold set to Y, then the effective threshold for\n  detections of this class will be max(X,Y).",
                    "format": "float",
                    "type": "number"
                },
                "maxDetectionBoxIou": {
                    "description": "Maximum possible detection box IoU with the input image.\nFilters out the large detections, whose IoU with the input image is larger\nthan this variable.",
                    "format": "float",
                    "type": "number"
                },
                "maxDetections": {
                    "description": "The maximum number of detections (sorted by descending score) to consider.\nUse a negative value to return all detections that passed the previous\nfilters.",
                    "format": "int32",
                    "type": "integer"
                },
                "minContinuousDetections": {
                    "description": "Minimum number of continuous detections to be valid detection.\nIf this is set, the occurrence of the same detection should be larger than\nthis value to be considered as valid and sent as the output stream of the\ndetection result.",
                    "format": "int32",
                    "type": "integer"
                },
                "objectCentricOnly": {
                    "description": "If true, filters out detections that are not object-centric, i.e. that\ndon't contain the center of the frame.",
                    "type": "boolean"
                },
                "predictionFilterOptions": {
                    "$ref": "VisionkitPredictionFilterOptions",
                    "description": "Options to smooth the label of each detection box. If the option is not\nset, no smoothing is applied."
                },
                "useExternalBoxes": {
                    "description": "If true, then no detection model has to be configured. Instead the cascade\nwill use a list of external detection boxes provided by means of the\nReceiveDetections() API[1].\n\n[1]:\ngoogle3/photos/vision/visionkit/pipeline/pipeline.h?q=ReceiveDetections",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitDocumentMergerConfig": {
            "description": "Parameters controlling how OCR detections are merged.",
            "id": "VisionkitDocumentMergerConfig",
            "properties": {
                "editDistanceCacheSize": {
                    "description": "Edit distance cache parameters.",
                    "format": "int32",
                    "type": "integer"
                },
                "editDistanceMaxIdleSeconds": {
                    "format": "float",
                    "type": "number"
                },
                "geometricLineSimilarHorizontalSigma": {
                    "format": "float",
                    "type": "number"
                },
                "geometricLineSimilarVerticalSigma": {
                    "description": "The sigma to use for similar looking strings.",
                    "format": "float",
                    "type": "number"
                },
                "maxIdleLineDurationUs": {
                    "description": "Lines that were idle (were not matched to the latest OCR run) for more than\nthis will be removed. Default: 1000 second.",
                    "format": "int64",
                    "type": "string"
                },
                "maxJunkLineLength": {
                    "format": "int32",
                    "type": "integer"
                },
                "maxJunkLineWordConfidence": {
                    "description": "Will not track lines that have too low confidence and are too short.",
                    "format": "float",
                    "type": "number"
                },
                "maxNormalizedEditDistance": {
                    "description": "Maximum normalized edit distance for merging two lines.",
                    "format": "float",
                    "type": "number"
                },
                "maxStringLengthVariationForSimilarity": {
                    "description": "If the max / min length is above this number we will not attempt to compare\ntwo strings.",
                    "format": "float",
                    "type": "number"
                },
                "mergerConfig": {
                    "$ref": "VisionkitSymbolMergerConfig",
                    "description": "String alignment constants."
                },
                "minActiveLineMergeProbability": {
                    "description": "The match probability must be above this to merge two active lines.",
                    "format": "float",
                    "type": "number"
                },
                "minFractionPresent": {
                    "description": "The fraction of the times in the last 5 frames that a line needs to be\npresent to include in the output.",
                    "format": "float",
                    "type": "number"
                },
                "minOutputAvgWordConfidence": {
                    "description": "Will not emit lines with avg word confidence less than this.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitDutyCycleOptions": {
            "id": "VisionkitDutyCycleOptions",
            "properties": {
                "dutyCycleProfiles": {
                    "items": {
                        "$ref": "VisionkitDutyCycleOptionsDutyCycleProfile"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitDutyCycleOptionsDutyCycleProfile": {
            "id": "VisionkitDutyCycleOptionsDutyCycleProfile",
            "properties": {
                "engineNames": {
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "levels": {
                    "description": "The levels are expected to be sorted by max_duty_cycle in descending\norder.",
                    "items": {
                        "$ref": "VisionkitDutyCycleOptionsLevel"
                    },
                    "type": "array"
                },
                "profileName": {
                    "type": "string"
                },
                "validTimeWindowUs": {
                    "description": "Time window used for computing accumulated usage for level update.\nRecords exceeds this time window are ignored. Must be set to enable\nmultiple levels for the duty cycle profile.",
                    "format": "int64",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitDutyCycleOptionsLevel": {
            "id": "VisionkitDutyCycleOptionsLevel",
            "properties": {
                "backgroundIntervalUs": {
                    "description": "Background interval in microseconds for the current level. Applied when\nsemantic content is not detected, only start run again after this\nduration.",
                    "format": "int64",
                    "type": "string"
                },
                "maxActiveTimeUs": {
                    "description": "Max active time for the current level. If this field is not set, there is\nno limitation on the active time for current level.",
                    "format": "int64",
                    "type": "string"
                },
                "maxDutyCycle": {
                    "description": "The ratio of active time duration by the total time duration for a time\nperiod. For example, a max_duty_cycle of 1.0 means no sleep time is\nneeded after an active run. And a max_duty_cycle of 0.5 means the sleep\ntime as the same long as the active time is needed after a run.",
                    "format": "float",
                    "type": "number"
                },
                "minProcessIntervalUs": {
                    "description": "Minimum processing interval in microseconds for the current level.\nApplied when semantic content is detected, if processing completes\nquickly such that even with enforcing the maximum duty cycle, the\ninterval between active processing would be less than this. In that\ncase, additional sleep is added to ensure that two consecutive\nprocessings are separated by at least this duration.",
                    "format": "int64",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitDutyCycleSettings": {
            "id": "VisionkitDutyCycleSettings",
            "properties": {
                "targetDutyCycleUs": {
                    "description": "Minimum time interval (in us) between runs.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitEdgeClassifier": {
            "description": "Configures a classifier.",
            "id": "VisionkitEdgeClassifier",
            "properties": {
                "classifierParameters": {
                    "$ref": "VisionkitEdgeClassifierParameters",
                    "description": "Mandatory classifier parameters."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeClassifierParameters": {
            "description": "The parameters needed to perform a classification.\nNext ID: 9.",
            "id": "VisionkitEdgeClassifierParameters",
            "properties": {
                "classNameBlacklist": {
                    "description": "Optional blacklist of class names. If non-empty, classifications whose\nclass name is in this set will be filtered out. Duplicate or unknown\nclass names are ignored. Mutually exclusive with class_name_whitelist.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "classNameWhitelist": {
                    "description": "Optional whitelist of class names. If non-empty, classifications whose\nclass name is not in this set will be filtered out. Duplicate or unknown\nclass names are ignored. Mutually exclusive with class_name_blacklist.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "maxResults": {
                    "description": "Maximum number of top scored results to return. Use a negative value to\nreturn all results.",
                    "format": "int32",
                    "type": "integer"
                },
                "modelId": {
                    "description": "The tf-hub handle for this model in go/visionkit-zoo, formatted as\n\"@visionkit/group_name/model_type/model_name/version\".",
                    "type": "string"
                },
                "name": {
                    "description": "A unique identifier for a given object detection model.\n\nDEPRECATED: use `model_id` instead.",
                    "type": "string"
                },
                "pruneAncestors": {
                    "description": "If true, and if the label map specifies a hierarchy between labels[1], a\npost-processing stage is performed after inference to remove coarse-grained\nclasses from the classification results. A class is considered\ncoarse-grained if the results contain a direct descendant.\n\nExample: \"Food\" is an ancestor of \"Banana\". So if both classes are part of\nthe results, \"Food\" will be pruned out.\n\nDo not turn on this option if the label map is entirely flat, i.e. if no\nsub-classes have been specified via the `child_name`[1] field. Otherwise\nan error is returned at classifier creation time.\n\n[1]:\ngoogle3/photos/vision/object_detection/mobile/proto/labelmap.proto?q=child_name",
                    "type": "boolean"
                },
                "scoreThreshold": {
                    "description": "Score threshold in [0,1[. Results below this value are rejected.",
                    "format": "float",
                    "type": "number"
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "The superpack manifest name, version and URL to dynamically download the\nfiles required to power this model: see [1].\n\n[1]: go/visionkit-downloader/packaging-files"
                }
            },
            "type": "object"
        },
        "VisionkitEdgeEmbedderParameters": {
            "description": "The parameters needed to extract an embedding.\nNext ID: 5.",
            "id": "VisionkitEdgeEmbedderParameters",
            "properties": {
                "l2Normalize": {
                    "description": "Whether to normalize each feature vector with L2 norm. Use this option only\nif the model does not already contain a native L2_NORMALIZATION TF Lite Op.\nIn most cases, this is already the case and L2 norm is thus achieved\nthrough TF Lite inference.",
                    "type": "boolean"
                },
                "modelId": {
                    "description": "The tf-hub handle for this model in go/visionkit-zoo, formatted as\n\"@visionkit/group_name/model_type/model_name/version\".",
                    "type": "string"
                },
                "name": {
                    "description": "A unique identifier for a given object detection model.\n\nDEPRECATED: use `model_id` instead.",
                    "type": "string"
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "The superpack manifest name, version and URL to dynamically download the\nfiles required to power this model: see [1].\n\n[1]: go/visionkit-downloader/packaging-files"
                }
            },
            "type": "object"
        },
        "VisionkitEdgeMatcher": {
            "description": "Configures a matcher.",
            "id": "VisionkitEdgeMatcher",
            "properties": {
                "matcherParameters": {
                    "$ref": "VisionkitEdgeMatcherParameters",
                    "description": "Mandatory matcher parameters (local index, query time parameters, etc)."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeMatcherParameters": {
            "description": "The parameters needed to perform on-device image matching, i.e. exact and\nnear duplicate image retrieval against a local index using Mobile Congas\n(go/mobile-congas) feature extractor.\n\nSee go/visionkit-engines/matcher for more details.\nNext ID: 6.",
            "id": "VisionkitEdgeMatcherParameters",
            "properties": {
                "indexId": {
                    "description": "The index-hub handle for this model, formatted as\n\"@visionkit/dataset_name/dataset_version/model_name/model_version/index_version\".",
                    "type": "string"
                },
                "maxImageSize": {
                    "description": "If > 0, scale down query image if its maximum dimension is higher than this\nsize before doing the internal processing. If <= 0 this has no effect.",
                    "format": "int32",
                    "type": "integer"
                },
                "maxResults": {
                    "description": "Maximum number of top scored results to return. If <= 0 this has no effect\nwhich means all near duplicate results are returned.",
                    "format": "int32",
                    "type": "integer"
                },
                "name": {
                    "description": "A unique identifier for a given object detection model.\n\nDEPRECATED: use `index_id` instead.",
                    "type": "string"
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "The superpack manifest name, version and URL to dynamically download the\nfiles required to power this index: see [1].\n\n[1]: go/visionkit-downloader/packaging-files"
                }
            },
            "type": "object"
        },
        "VisionkitEdgeObjectDetector": {
            "description": "Configures an object detector.",
            "id": "VisionkitEdgeObjectDetector",
            "properties": {
                "boxClassifierParameters": {
                    "$ref": "VisionkitEdgeClassifierParameters",
                    "description": "Optional classifier to run on each detected box.\nIf this field is set, the labels returned by the detector are ignored and\nreplaced by those returned by this classifier. This classifier is typically\nused in combination with a localizer."
                },
                "detectorParameters": {
                    "$ref": "VisionkitEdgeObjectDetectorParameters",
                    "description": "Mandatory object detector parameters."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeObjectDetectorParameters": {
            "description": "The parameters needed to perform an object detection.\nNext ID: 11.",
            "id": "VisionkitEdgeObjectDetectorParameters",
            "properties": {
                "agnosticMode": {
                    "description": "Whether the object detection model is single class agnostic.",
                    "type": "boolean"
                },
                "classThresholds": {
                    "$ref": "VisionkitClassThresholds",
                    "description": "If provided, filters out detections whose score is below the threshold\nfor their class. The thresholds must be expressed as probits (i.e. in\n[0,1]). Conversion into logits is automatically handled at runtime in case\n`detection_scores_are_probits` is false.\n\nNOTE: if both `class_thresholds` and `global_score_threshold` are provided,\nboth apply. This implies that if a class threshold is set to X and a global\nscore threshold set to Y, then the effective threshold for detections of\nthis class will be max(X,Y)."
                },
                "detectionScoresAreProbits": {
                    "description": "If true, detection scores are expected to be probits in (i.e. in [0,1]).\nOtherwise detection scores are expected to be logits (i.e. in\n]-inf,inf[).",
                    "type": "boolean"
                },
                "filterParasiticDetections": {
                    "description": "Filters out the most confident result if it covers nearly the entire input\nframe. In such a case, this top-1 detection is not of great value anyway as\nit nearly represents the entire frame, and in practice it frequently\n\"hides\" the actual detected object returned with a slightly lower\nconfidence. Typical examples of such parasitic detections: \"unknown object\"\nor \"table\" detected on nearly the entire frame.\nThis does nothing if the input array is made of 0 or 1 result.",
                    "type": "boolean"
                },
                "globalScoreThreshold": {
                    "description": "If provided, filters out any detection of any class whose score is below\nthis threshold. This threshold must be expressed as a probit (i.e. in\n[0,1]). Conversion into logits is automatically handled at runtime in case\n`detection_scores_are_probits` is false.\n\nNOTE: if both `class_thresholds` and `global_score_threshold` are provided,\nboth apply. This implies that if a class threshold is set to X and a global\nscore threshold set to Y, then the effective threshold for detections of\nthis class will be max(X,Y).",
                    "format": "float",
                    "type": "number"
                },
                "maxDetections": {
                    "description": "The maximum number of detections (sorted by descending score) to consider.\nUse a negative value to return all detections that passed the previous\nfilters. This value must not be zero, or it will fail the sanity checks.",
                    "format": "int32",
                    "type": "integer"
                },
                "modelId": {
                    "description": "The tf-hub handle for this model in go/visionkit-zoo, formatted as\n\"@visionkit/group_name/model_type/model_name/version\".",
                    "type": "string"
                },
                "name": {
                    "description": "A unique identifier for a given object detection model.\n\nDEPRECATED: use `model_id` instead.",
                    "type": "string"
                },
                "objectCentricOnly": {
                    "description": "If true, filters out detections that are not object-centric, i.e. that\ndon't contain the center of the frame.",
                    "type": "boolean"
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "The superpack manifest name, version and URL to dynamically download the\nfiles required to power this model: see [1].\n\n[1]: go/visionkit-downloader/packaging-files"
                }
            },
            "type": "object"
        },
        "VisionkitEdgeObjectRecognitionCascade": {
            "description": "Defines a full client-side object recognition cascade.\nNext ID: 3.",
            "id": "VisionkitEdgeObjectRecognitionCascade",
            "properties": {
                "frameAnnotator": {
                    "$ref": "VisionkitEdgeObjectRecognitionCascadeFrameAnnotator",
                    "description": "Optional FrameAnnotator."
                },
                "streamAnnotator": {
                    "$ref": "VisionkitEdgeObjectRecognitionCascadeStreamAnnotator",
                    "description": "Optional StreamAnnotator."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeObjectRecognitionCascadeFrameAnnotator": {
            "description": "Defines an annotator running on the latest selected frame, when the capture\ntrigger is activated.\nNext ID: 7.",
            "id": "VisionkitEdgeObjectRecognitionCascadeFrameAnnotator",
            "properties": {
                "classifier": {
                    "$ref": "VisionkitEdgeClassifier",
                    "description": "Runs a classifier on the latest selected frame."
                },
                "detector": {
                    "$ref": "VisionkitEdgeObjectDetector",
                    "description": "Runs a detector on the latest frame. Specifying this is mututally\nexclusive with specifying a StreamAnnotator."
                },
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the FrameAnnotator."
                },
                "matcher": {
                    "$ref": "VisionkitEdgeMatcher",
                    "description": "Runs a matcher on the latest selected frame."
                },
                "searcher": {
                    "$ref": "VisionkitEdgeSearcher",
                    "description": "Runs a searcher on the latest selected frame."
                },
                "trigger": {
                    "$ref": "VisionkitCaptureTrigger",
                    "description": "Mandatory capture trigger that specifies when the frame annotation runs."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeObjectRecognitionCascadeStreamAnnotator": {
            "description": "Defines an annotator running continuously on the stream of frames provided\nby the device.\nNext ID: 5.",
            "id": "VisionkitEdgeObjectRecognitionCascadeStreamAnnotator",
            "properties": {
                "classifier": {
                    "$ref": "VisionkitEdgeClassifier",
                    "description": "Runs a classifier continuously on the stream of frames. Specifying this\nis mutually exclusive with specifying a FrameAnnotator."
                },
                "detector": {
                    "$ref": "VisionkitEdgeObjectDetector",
                    "description": "Runs a detector continuously on the stream of frames."
                },
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the StreamAnnotator."
                },
                "segmenter": {
                    "$ref": "VisionkitEdgeSegmenter",
                    "description": "Runs a segmenter continuously on the stream of frames."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeSearcher": {
            "description": "Configures a searcher.",
            "id": "VisionkitEdgeSearcher",
            "properties": {
                "embedderParameters": {
                    "$ref": "VisionkitEdgeEmbedderParameters",
                    "description": "Mandatory embedder parameters."
                },
                "searcherParameters": {
                    "$ref": "VisionkitEdgeSearcherParameters",
                    "description": "Mandatory searcher parameters (local index, query time parameters, etc)."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeSearcherParameters": {
            "description": "The parameters needed to perform an embedding-based KNN search against a\nlocal index.\n\nSee go/visionkit-engines/searcher for more details.\nNext ID: 8.",
            "id": "VisionkitEdgeSearcherParameters",
            "properties": {
                "indexId": {
                    "description": "The index-hub handle for this model, formatted as\n\"@visionkit/dataset_name/dataset_version/model_name/model_version/index_version\".",
                    "type": "string"
                },
                "indexType": {
                    "description": "Index type required at searcher initialization time.",
                    "enum": [
                        "DEFAULT",
                        "HASHED"
                    ],
                    "enumDescriptions": [
                        "The index is made of plain floating point embeddings.",
                        "The index is made of hashed embeddings and is thus more compact."
                    ],
                    "type": "string"
                },
                "maxResults": {
                    "description": "Maximum number of top scored results to return. This value must be >0.",
                    "format": "int32",
                    "type": "integer"
                },
                "name": {
                    "description": "A unique identifier for a given object detection model.\n\nDEPRECATED: use `index_id` instead.",
                    "type": "string"
                },
                "partitionerType": {
                    "description": "Index partitioner type required at searcher initialization time. None by\ndefault which means there is a single partition that groups all the\nembeddings in the index.",
                    "enum": [
                        "NONE",
                        "KMEANS"
                    ],
                    "enumDescriptions": [
                        "No partitioning. This is the default.",
                        "K-means partitioning. The embeddings have been clustered into multiple\nleaves (a.k.a. partitions). This is used to speed up the search."
                    ],
                    "type": "string"
                },
                "skipPartitionFraction": {
                    "description": "When a partitioner is used (e.g. KMEANS), this indicates the fraction of\nthe partitions that must be skipped at search time. In other words, only\nthe partitions nearest to the query embedding are searched against. This\nvalue must be in the range [0, 1[, where 0 (the default) indicates that all\npartitions must be searched against, and 1 indicates that all partitions\nshould be ignored.",
                    "format": "float",
                    "type": "number"
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "The superpack manifest name, version and URL to dynamically download the\nfiles required to power this index: see [1].\n\n[1]: go/visionkit-downloader/packaging-files"
                }
            },
            "type": "object"
        },
        "VisionkitEdgeSegmenter": {
            "description": "Configures a segmenter.",
            "id": "VisionkitEdgeSegmenter",
            "properties": {
                "segmenterParameters": {
                    "$ref": "VisionkitEdgeSegmenterParameters",
                    "description": "Mandatory segmenter parameters."
                }
            },
            "type": "object"
        },
        "VisionkitEdgeSegmenterParameters": {
            "description": "The parameters needed to perform a segmentation.\nNext ID: 6.",
            "id": "VisionkitEdgeSegmenterParameters",
            "properties": {
                "modelId": {
                    "description": "The tf-hub handle for this model in go/visionkit-zoo, formatted as\n\"@visionkit/group_name/model_type/model_name/version\".",
                    "type": "string"
                },
                "name": {
                    "description": "A unique identifier for a given object detection model.\n\nDEPRECATED: use `model_id` instead.",
                    "type": "string"
                },
                "outputType": {
                    "description": "Mandatory output mask type.",
                    "enum": [
                        "UNSPECIFIED_OUTPUT_TYPE",
                        "CATEGORY_MASK",
                        "CONFIDENCE_MASK"
                    ],
                    "enumDescriptions": [
                        "Unspecified output mask type. This value must not be used, as it will\nresult in the sanity checks rejecting this configuration.",
                        "Gives a single output mask where each pixel represents the class which\nthe pixel in the original image was predicted to belong to.",
                        "Gives a list of output masks where, for each mask, each pixel represents\nthe prediction confidence, usually in the [0, 1] range."
                    ],
                    "type": "string"
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "The superpack manifest name, version and URL to dynamically download the\nfiles required to power this model: see [1].\n\n[1]: go/visionkit-downloader/packaging-files"
                }
            },
            "type": "object"
        },
        "VisionkitEdgeTrigger": {
            "description": "Inspects an AnnotateImageRequest to determine if hybrid recognition can be\ntriggered, based on the annotations and context sent from the device.",
            "id": "VisionkitEdgeTrigger",
            "properties": {
                "objectCategoryMatcher": {
                    "$ref": "VisionkitObjectCategoryMatcher",
                    "description": "A trigger based on the object category inferred on-device."
                }
            },
            "type": "object"
        },
        "VisionkitEmbedderOptions": {
            "description": "Options for setting up an Embedder.\nNext Id: 9.",
            "id": "VisionkitEmbedderOptions",
            "properties": {
                "computeSettings": {
                    "$ref": "AccelerationAcceleration",
                    "description": "How to accelerate the model (e.g., via NNAPI)."
                },
                "embedderName": {
                    "description": "The class name corresponding to a registered embedder, e.g. \"FooNet\"\nfor an embedder registered via `REGISTER_EMBEDDER(FooNet);`.\nThis is also known as bundled models since the model files are embedded in\nthe final binary. See external_files for an alternative. Mutually exclusive\nwith external_files.",
                    "type": "string"
                },
                "externalModelFile": {
                    "$ref": "VisionkitExternalFile",
                    "description": "External file for the TF Lite model used to create an embedder. Mutually\nexclusive with embedder_name."
                },
                "keepUnquantizedEmbedding": {
                    "description": "For eval/debugging purposes, the user can request that both the quantized\nand unquantized embeddings be returned. This option has an effect only if\nquantize is set to true.",
                    "type": "boolean"
                },
                "l2Normalize": {
                    "description": "Whether to normalize each feature vector with L2 norm. Use this option only\nif the model does not already contain a native L2_NORMALIZATION TF Lite Op.\nIn most cases, this is already the case and L2 norm is thus achieved\nthrough TF Lite inference.",
                    "type": "boolean"
                },
                "numThreads": {
                    "description": "The number of threads allowed for model inference. This value is used in\nbuilding the TF Lite interpreter.",
                    "format": "int32",
                    "type": "integer"
                },
                "preferNnapiDelegate": {
                    "description": "Whether to use NNAPI (go/android-nn) delegate for hardware acceleration.\nIf it fails, it will fall back to the normal CPU execution.\nDeprecated, use |compute_settings| instead.",
                    "type": "boolean"
                },
                "quantize": {
                    "description": "Whether the returned embedding should be quantized to bytes via scalar\nquantization. Embeddings are implicitly assumed to be unit-norm and\ntherefore any dimension is guaranteed to have a value in [-1.0, 1.0]. Use\nthe l2_normalize option if this is not the case.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitExternalFile": {
            "description": "Represents external files used by the engines (e.g. TF Lite flatbuffers). The\nfiles can be specified by one of the following three ways:\n\n(1) file path in `file_name`.\n(2) file content loaded in `file_content`.\n(3) file descriptor by `file_descriptor` as returned by open(2).\n\nIf more than one field is provided, `file_content`, `file_name`, and\n`file_descriptor` are used in the descending precedence order.",
            "id": "VisionkitExternalFile",
            "properties": {
                "fileContent": {
                    "format": "byte",
                    "type": "string"
                },
                "fileDescriptor": {
                    "format": "int32",
                    "type": "integer"
                },
                "fileName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitFaceCascadeOptions": {
            "description": "Face cascade options.",
            "id": "VisionkitFaceCascadeOptions",
            "properties": {
                "detectorClientOptions": {
                    "$ref": "MobileSsdClientOptions"
                },
                "faceAttributesClientOptions": {
                    "description": "Face engine options",
                    "items": {
                        "$ref": "HumanSensingFaceAttributesClientOptions"
                    },
                    "type": "array"
                },
                "useExternalFaces": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitFrameSelectorOptions": {
            "description": "Frame selector options. Supports IMU-based frame selector config and screen\nselector config.",
            "id": "VisionkitFrameSelectorOptions",
            "properties": {
                "imageBasedFrameSelectorConfig": {
                    "$ref": "VisionkitImageBasedFrameSelectorConfig"
                },
                "imuBasedFrameSelectorConfig": {
                    "$ref": "VisionkitImuBasedFrameSelectorConfig"
                },
                "screenSelectorConfig": {
                    "$ref": "VisionkitScreenSelectorConfig"
                }
            },
            "type": "object"
        },
        "VisionkitHybridAnnotator": {
            "description": "Defines the annotation logic to apply to a request, based on the annotations\nalready inferred on-device.",
            "id": "VisionkitHybridAnnotator",
            "properties": {
                "cloudAnnotator": {
                    "$ref": "VisionkitCloudAnnotator",
                    "description": "The annotator used to augment the annotations already inferred on-device."
                },
                "edgeTrigger": {
                    "$ref": "VisionkitEdgeTrigger",
                    "description": "The trigger that determines if this HybridAnnotator applies to the request,\nbased on the content of the incoming AnnotateImageRequest."
                }
            },
            "type": "object"
        },
        "VisionkitHybridMatcher": {
            "description": "Configures the edge part of a hybrid matcher.\n\nThis only specifies what should operate on the device. As this is part of a\nhybrid cascade, the client is responsible to perform a call to the cloud\nusing a given camera frame as query.\n\nThis should be achieved by properly configuring the matcher_cascade_options\nfrom the VisionKit Pipeline's matcher cascade:\n\ngoogle3/photos/vision/visionkit/pipeline/proto/matcher_cascade_options.proto\n\nNext ID: 2.",
            "id": "VisionkitHybridMatcher",
            "properties": {
                "edgeMatcherParameters": {
                    "$ref": "VisionkitEdgeMatcherParameters",
                    "description": "Mandatory edge matcher parameters. The underlying matcher index is used as\na local cache, i.e. a call to the cloud is made only if no confident\nresults (based on spatial matching) are found on the device."
                }
            },
            "type": "object"
        },
        "VisionkitHybridObjectRecognitionCascade": {
            "description": "Defines a hybrid object recognition cascade.\n\nBy definition of hybrid, the first stages of the cascade are performed on the\ndevice (e.g. detection & tracking, embedding extraction) and the final stages\nare handled server-side (e.g. k-NN search in a large index). This follows a\ncoarse-to-fine approach where the finest grained recognition is handled in\nthe cloud.\n\nIt is the counterpart of visionkit.internal.hybrid.ObjectRecognitionCascade\nwhich by opposition includes internal-only fields (e.g. server-side index\nparameters) that are required to power such a cascade on the server-side.\nSuch parameters should not be exposed to the client consuming the Overlay\nproto by means of google3/google/internal/visionkit/v1/overlays.proto.\n\nPlease have a look at hybrid_cascades_internal.proto for more details.\n\nNext ID: 3.",
            "id": "VisionkitHybridObjectRecognitionCascade",
            "properties": {
                "frameAnnotator": {
                    "$ref": "VisionkitHybridObjectRecognitionCascadeFrameAnnotator",
                    "description": "Mandatory FrameAnnotator."
                },
                "streamAnnotator": {
                    "$ref": "VisionkitHybridObjectRecognitionCascadeStreamAnnotator",
                    "description": "Mandatory StreamAnnotator."
                }
            },
            "type": "object"
        },
        "VisionkitHybridObjectRecognitionCascadeFrameAnnotator": {
            "description": "Defines an annotator running on the latest selected frame, when the capture\ntrigger is activated.",
            "id": "VisionkitHybridObjectRecognitionCascadeFrameAnnotator",
            "properties": {
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the FrameAnnotator."
                },
                "matcher": {
                    "$ref": "VisionkitHybridMatcher",
                    "description": "Runs an hybrid matcher on the latest selected frame."
                },
                "searcher": {
                    "$ref": "VisionkitHybridSearcher",
                    "description": "Runs an hybrid searcher on the latest selected frame."
                },
                "trigger": {
                    "$ref": "VisionkitCaptureTrigger",
                    "description": "Mandatory capture trigger that specifies when the frame annotation runs."
                }
            },
            "type": "object"
        },
        "VisionkitHybridObjectRecognitionCascadeStreamAnnotator": {
            "description": "Defines an annotator running continuously on the stream of frames provided\nby the device.",
            "id": "VisionkitHybridObjectRecognitionCascadeStreamAnnotator",
            "properties": {
                "detector": {
                    "$ref": "VisionkitEdgeObjectDetector",
                    "description": "Mandatory. Runs a detector continuously on the stream of frames."
                },
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the StreamAnnotator."
                }
            },
            "type": "object"
        },
        "VisionkitHybridSearcher": {
            "description": "Configures the edge part of a hybrid searcher.\n\nThis only specifies what should operate on the device. As this is part of a\nhybrid cascade, the client is responsible to perform a call to the cloud\nusing the features obtained from the edge embedder as query.\n\nThis should be achieved by properly configuring the cloud_searcher_options\nfrom the VisionKit Pipeline's searcher cascade:\n\ngoogle3/photos/vision/visionkit/pipeline/proto/searcher_cascade_options.proto\n\nNext ID: 4.",
            "id": "VisionkitHybridSearcher",
            "properties": {
                "edgeEmbedderParameters": {
                    "$ref": "VisionkitEdgeEmbedderParameters",
                    "description": "Mandatory edge embedder parameters. Computed features are used as query by\nthe edge searcher (if any) and the cloud searcher (cloud-based ScaM index)."
                },
                "edgeSearcherParameters": {
                    "$ref": "VisionkitEdgeSearcherParameters",
                    "description": "Optional edge searcher parameters. The underlying searcher index is used as\na local cache, i.e. a call to the cloud is made only if no confident enough\nresults are found on the device."
                },
                "searchRestrictParameters": {
                    "$ref": "VisionkitSearchRestrictParameters",
                    "description": "Optional search restrict parameters used to enable and specify how search\nrestricts should be set at query time. If left empty, KNN searches will be\nperformed without restricts.\n\nNote: search restricts are only supported by cloud-based searchers so far.\nDO NOT USE this option if you intend to configure an edge searcher."
                }
            },
            "type": "object"
        },
        "VisionkitImageBasedFrameSelectorConfig": {
            "id": "VisionkitImageBasedFrameSelectorConfig",
            "properties": {
                "maxAllowableBlurriness": {
                    "description": "If the blurriness of the frame is higher than this value, the frame is\nconsidered as blurry. The blurriness corresponds to a ratio of blur score\nchange from previous frames. The range of the blurriness value is\nunbounded and positive (> 0). More detail is described in\nRunImageBasedSelection() in frame_selector_calculator.cc.\nThe default value is 0 which allows all the frames and the user needs to\nexplicitly set the value to filter frames based on blurriness.\nThe empirically suggested value to start is 0.3.\nIf this value is set to higher value, it allows more blurry frames.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitImuBasedFrameSelectorConfig": {
            "description": "LINT.IfChange",
            "id": "VisionkitImuBasedFrameSelectorConfig",
            "properties": {
                "imuBasedFrameSelectionMode": {
                    "description": "Frame selection mode.",
                    "enum": [
                        "SELECT_UNKNOWN",
                        "SELECT_UNGATED",
                        "SELECT_MOST_RECENT",
                        "SELECT_BALANCED",
                        "SELECT_LEAST_JITTER"
                    ],
                    "enumDescriptions": [
                        "Force the frame selector to always return FRAME_SELECTION_RESULT_UNKNOWN.\nThis is for debugging purposes to easily simulate a frame selector that\nnever selecting frames.",
                        "Select latest frame and ignore jitter limits - (the same as not using\nframe selection).",
                        "Select latest frame but reject frames above the maximum jitter limit.",
                        "Select with balance between low latency and jitter. In this mode, the\npreferred_jitter_deg_per_sec threshold is enabled to select the latest\nframe whose jitter is below the threshold. Use this mode to favor low\nlatency over minimizing jitter. The IMU based frame selector uses this mode\nby default.",
                        "Select frame with least jitter but reject frames above the maximum jitter\nlimit."
                    ],
                    "type": "string"
                },
                "maximumJitterDegPerSec": {
                    "description": "Maximum jitter threshold in deg/sec. Frame with jitter above this threshold\nwill be rejected except for SELECT_UNGATED mode. Must be greater than 0.\nMust also be equal or greater than the preferred jitter threshold. The\ndefault value 1.0e5 represents a threshold which is large enough that every\nframe is acceptable.",
                    "format": "float",
                    "type": "number"
                },
                "preferredJitterDegPerSec": {
                    "description": "Preferred jitter threshold in deg/sec. Frame with jitter below this\nthreshold will be selected as the best frame in SELECT_BALANCED mode. Must\nbe equal or greater than 0. Must also be equal or less than the maximum\njitter threshold. The default value is 0.\nLINT.ThenChange(//depot/google3/java/com/google/android/libraries/vision/\\\n    visionkit/frameselection/ImuBasedFrameSelector.java)",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudClassifier": {
            "description": "Configures a Servomatic-based classifier.",
            "id": "VisionkitInternalCloudClassifier",
            "properties": {
                "classifierParameters": {
                    "$ref": "VisionkitInternalCloudServoClassifierParameters",
                    "description": "Classifier parameters for running on Servomatic."
                },
                "metadataAugmenter": {
                    "description": "Optional MetadataAugmenter used to augment the labels returned by the\nclassifier.",
                    "enum": [
                        "METADATA_AUGMENTATION_NONE",
                        "METADATA_AUGMENTATION_GOOGLE_SEARCH_URL",
                        "METADATA_AUGMENTATION_KNOWLEDGE_GRAPH",
                        "METADATA_AUGMENTATION_REVGEO",
                        "METADATA_AUGMENTATION_MET_MUSEUM_URL"
                    ],
                    "enumDescriptions": [
                        "Do not perform any metadata augmentations.",
                        "Build a Google Search URL from the label and use it as `link` [1] field in\nthe returned ObjectMetadata object. The `display_name` and `thumbnail_url`\nare simply picked from the AnnotatorContext. This assumes the labels are\nhuman-readable strings.\n\n[1]: google3/google/internal/visionkit/v1/annotate_image.proto?q=link",
                        "Extract a display name, thumbnail URL and English wikipedia URL from the\nKnowledge Graph.\nThis requires the CloudAnnotator to produce MIDs as labels.",
                        "Extract a display name, iconic image URL and Google Maps URL from the\nRevGeo (go/revgeo) and Places (go/places-api-stubby) APIs.\nThis requires the CloudAnnotator to produce S2 Cell Tokens (formatted as\nhexadecimal strings without the \"0x\" prefix) as labels.",
                        "Build a redirect URL to the Met Museum (www.metmuseum.org) from the label\nand use it as `link` [1] field in the returned ObjectMetadata objects."
                    ],
                    "type": "string"
                },
                "vssClassifierParameters": {
                    "$ref": "VisionkitInternalCloudVssClassifierParameters",
                    "description": "Classifier parameters for running on VSS."
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudMatcher": {
            "description": "Configures a VSS-based local feature matcher.",
            "id": "VisionkitInternalCloudMatcher",
            "properties": {
                "matcherParameters": {
                    "$ref": "VisionkitInternalCloudVssMatcherParameters",
                    "description": "Mandatory matcher parameters."
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudObjectDetector": {
            "description": "Configures a Servomatic-based object detector.",
            "id": "VisionkitInternalCloudObjectDetector",
            "properties": {
                "detectorParameters": {
                    "$ref": "VisionkitInternalCloudServoDetectorParameters",
                    "description": "Detector parameters for running on Servomatic."
                },
                "vssDetectorParameters": {
                    "$ref": "VisionkitInternalCloudVssDetectorParameters",
                    "description": "Detector parameters for running on VSS."
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudObjectRecognitionCascade": {
            "description": "Defines a full server-side object recognition cascade.\n\nIt is the counterpart of visionkit.cloud.ObjectRecognitionCascade which by\nopposition does not include internal-only fields (e.g. detector) that should\nnot be exposed to the client consuming this proto by means of\ngoogle3/google/internal/visionkit/v1/overlays.proto.\n\nNext ID: 8.",
            "id": "VisionkitInternalCloudObjectRecognitionCascade",
            "properties": {
                "classifier": {
                    "$ref": "VisionkitInternalCloudClassifier"
                },
                "detector": {
                    "$ref": "VisionkitInternalCloudObjectDetector",
                    "description": "Optional detector."
                },
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the cascade."
                },
                "matcher": {
                    "$ref": "VisionkitInternalCloudMatcher"
                },
                "searcher": {
                    "$ref": "VisionkitInternalCloudSearcher"
                },
                "segmenter": {
                    "$ref": "VisionkitInternalCloudSegmenter"
                },
                "trigger": {
                    "$ref": "VisionkitCaptureTrigger",
                    "description": "Mandatory trigger that specifies when the annotator runs."
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudScamIndexParameters": {
            "description": "The parameters needed to perform a k-NN search in a ScaM index. The index is\nassumed to be configured [1] to use 'SquaredL2Distance' as distance measure.\nNext ID: 10.\n\n[1]: http://cnsviewer/placer/prod/home/visionkit-prod/scam/configs",
            "id": "VisionkitInternalCloudScamIndexParameters",
            "properties": {
                "datasetName": {
                    "description": "The name of the ScaM dataset, as specified in the VisionKit ScaM\nconfiguration [1].\n\n[1]: /placer/prod/home/visionkit-prod/scam/configs/scam.config",
                    "type": "string"
                },
                "featureDimension": {
                    "description": "The dimension of features expected as input. This is used for sanity and\nconsistency checks at query time.",
                    "format": "int32",
                    "type": "integer"
                },
                "featureType": {
                    "description": "The type of features expected as input. See also ServoEmbedderParameters.",
                    "enum": [
                        "FEATURE_TYPE_UNSPECIFIED",
                        "INT64",
                        "FLOAT"
                    ],
                    "enumDescriptions": [
                        "Unspecified feature type.",
                        "Integer features. This assumes the features have been scalar quantized\ninto bytes (int8) using a function like FixedByteQuantizedCompress [1].\n\n[1]:\ngoogle3/photos/vision/facenet/face_template_utils.h?q=FixedByteQuantizedCompress",
                        "Floating-point features."
                    ],
                    "type": "string"
                },
                "filterDuplicateLabels": {
                    "description": "If set to true, only the first result with a given label will be retained.\nThis allows getting rid of duplicate results in the final Annotation-s.",
                    "type": "boolean"
                },
                "maxNumberResults": {
                    "description": "The maximum number of results to return.",
                    "format": "uint32",
                    "type": "integer"
                },
                "scamNumberNeighbors": {
                    "description": "The number of neighbors to request from ScaM. If set to zero or omitted,\nthe same value as `max_number_results` is used. We recommend using\nover-retrieval and thus setting a large enough value (e.g. 10 times\n`max_number_results`) if `filter_duplicate_labels` is set to true, in order\nto account for the neighbors that will be discarded from the results by the\nduplicate filter.",
                    "format": "uint32",
                    "type": "integer"
                },
                "scamRestrictsV3Parameters": {
                    "$ref": "VisionkitInternalCloudScamRestrictsV3Parameters",
                    "description": "The ScaM restricts V3 parameters to use with this ScaM dataset. If not set,\nfilter queries are not supported for this dataset."
                },
                "similarityThreshold": {
                    "description": "A cosine similarity threshold in [-1,1) below which low confidence results\nare rejected. Set to -1 to keep all results.\nInternally this threshold is converted to an L2 distance threshold : it is\nexpressed here as a cosine similarity for convenience, as this is a common\nmetric used by most research teams.",
                    "format": "float",
                    "type": "number"
                },
                "userinfoMetadataType": {
                    "description": "The type of the metadata expected to be serialized in the `userinfo` field\nof the GenericFeatureVector-s of the searched ScaM dataset. If unspecified,\nthe routing rule is considered invalid and will trigger a runtime error.",
                    "enum": [
                        "LABEL_DEFAULT",
                        "PRODUCT_META_DATA_LIST",
                        "PLACE_METADATA",
                        "ANNOTATION_METADATA",
                        "OBJECT_METADATA_WITH_PLACE_INFO",
                        "USER_METADATA"
                    ],
                    "enumDescriptions": [
                        "By default, metadata is assumed to be a plain string label, typically a\nKnowledge Graph MID or a human-readable display name.",
                        "Metadata of type ProductMetaDataList [1]. Each of its ProductMetaData\nfields must at least contain a name and an iconic image with a non-empty\nthumbnail URL.\n\n[1]: google3/photos/vision/products/proto/product_meta_data.proto",
                        "Metadata of type PlaceMetadata [1]. Each PlaceMetadata must contain an\nimage_url and place.feature_id field.\n\n[1]:\ngoogle3/knowledge/cerebra/sense/im2query/model/s2place/distillation/scam_metadata.proto",
                        "General purpose metadata proto [1].\n\n[1]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=Annotation",
                        "Metadata of type ObjectMetaDataWithPlaceInfo [1]. Each\nObjectMetaDataWithPlaceInfo must contain PlaceMetaData info. Useful\nfor objects associated with places (e.g. a dish associated with a\nrestaurant).\n\n[1]:\ngoogle3/photos/vision/fine_grained/food/local_dishes/mining/place_and_object_metadata.proto",
                        "Metadata of type UserMetadata [1]. UserMetadata is made of several kinds\nof metadata, e.g. raw_label or ObjectMetadata [2].\n\n[1]:\ngoogle3/photos/vision/visionkit/indexing/proto/user_metadata.proto\n[2]:\ngoogle3/google/internal/visionkit/v1/metadata.proto"
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudScamRestrictsV3Parameters": {
            "id": "VisionkitInternalCloudScamRestrictsV3Parameters",
            "properties": {
                "category": {
                    "description": "If set, automatically appends a restrict with 'category' namespace using\nthis field as string value.",
                    "type": "string"
                },
                "s2cell15": {
                    "description": "DEPRECATED, please use `s2cell_level` below.\n\nIf true, automatically appends a restrict with 's2cell_15' namespace using\nan S2Cell ID level 15 [1] computed from the `location_context` [2] field of\nthe AnnotateImageRequest as uint64 value.\n\n[1]: http://g3doc/util/geometry/g3doc/devguide/s2cell_hierarchy.md\n[2]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=location_context",
                    "type": "boolean"
                },
                "s2cellLevel": {
                    "description": "If positive, automatically appends a restrict with 's2cell_%d' (e.g.\n's2cell_15' for level = 15) namespace using an S2Cell ID of this level [1]\ncomputed from the `location_context` [2] field of the AnnotateImageRequest\nas uint64 value. Should be in the range 1..kMaxCellLevel [3].\n\n[1]: http://g3doc/util/geometry/g3doc/devguide/s2cell_hierarchy.md\n[2]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=location_context\n[3]: google3/util/geometry/s2coords.h?q=kMaxCellLevel",
                    "format": "uint32",
                    "type": "integer"
                },
                "supportedKeys": {
                    "description": "The set of supported keys in the FilterQuery [1] provided in the\nAnnotateImageRequest.\n\n[1]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=FilterQuery",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudSearcher": {
            "description": "Configures a searcher performing embedding extraction through Servomatic,\nfollowed by kNN-search in a ScaM index.",
            "id": "VisionkitInternalCloudSearcher",
            "properties": {
                "attributeClassifierParameters": {
                    "$ref": "VisionkitInternalCloudServoAttributeClassifierParameters",
                    "description": "Optional attribute classifier parameters. This is only supported for image\nbytes as input: providing a query embedding will return a runtime error as\nattributes cannot be inferred from it."
                },
                "embedderParameters": {
                    "$ref": "VisionkitInternalCloudServoEmbedderParameters",
                    "description": "Mandatory embedder parameters."
                },
                "indexParameters": {
                    "$ref": "VisionkitInternalCloudScamIndexParameters",
                    "description": "Mandatory index parameters."
                },
                "metadataAugmenter": {
                    "description": "Optional MetadataAugmenter used to augment the labels returned by the\nsearcher.",
                    "enum": [
                        "METADATA_AUGMENTATION_NONE",
                        "METADATA_AUGMENTATION_GOOGLE_SEARCH_URL",
                        "METADATA_AUGMENTATION_KNOWLEDGE_GRAPH",
                        "METADATA_AUGMENTATION_REVGEO",
                        "METADATA_AUGMENTATION_MET_MUSEUM_URL"
                    ],
                    "enumDescriptions": [
                        "Do not perform any metadata augmentations.",
                        "Build a Google Search URL from the label and use it as `link` [1] field in\nthe returned ObjectMetadata object. The `display_name` and `thumbnail_url`\nare simply picked from the AnnotatorContext. This assumes the labels are\nhuman-readable strings.\n\n[1]: google3/google/internal/visionkit/v1/annotate_image.proto?q=link",
                        "Extract a display name, thumbnail URL and English wikipedia URL from the\nKnowledge Graph.\nThis requires the CloudAnnotator to produce MIDs as labels.",
                        "Extract a display name, iconic image URL and Google Maps URL from the\nRevGeo (go/revgeo) and Places (go/places-api-stubby) APIs.\nThis requires the CloudAnnotator to produce S2 Cell Tokens (formatted as\nhexadecimal strings without the \"0x\" prefix) as labels.",
                        "Build a redirect URL to the Met Museum (www.metmuseum.org) from the label\nand use it as `link` [1] field in the returned ObjectMetadata objects."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudSegmenter": {
            "description": "Configures a Servomatic-based segmenter.",
            "id": "VisionkitInternalCloudSegmenter",
            "properties": {
                "segmenterParameters": {
                    "$ref": "VisionkitInternalCloudServoSegmenterParameters",
                    "description": "Mandatory segmenter parameters."
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudServoAttributeClassifierParameters": {
            "description": "The parameters needed to perform attribute classification through Servo. Note\nthat the number of returned results is configured when the model is deployed\non Servo, and cannot be overridden here.",
            "id": "VisionkitInternalCloudServoAttributeClassifierParameters",
            "properties": {
                "filters": {
                    "description": "The list of whitelisted (there may be more available) attribute filters.\nEach of those must have three dedicated outputs in the exported model [1],\nwith the suffixes \"_classes\", \"_scores\" and \"_display_names\". If empty, all\nattributes available from Servomatic response will be returned.\n\n[1]:\ngoogle3/photos/vision/visionkit/server/servo/utils/export_multihead_classification_model.py",
                    "items": {
                        "$ref": "VisionkitInternalCloudServoAttributeClassifierParametersFilter"
                    },
                    "type": "array"
                },
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to send requests to.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                },
                "scoreThreshold": {
                    "description": "A score threshold below which attributes are rejected. Optional.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudServoAttributeClassifierParametersFilter": {
            "description": "Defines a single attribute filter.",
            "id": "VisionkitInternalCloudServoAttributeClassifierParametersFilter",
            "properties": {
                "attributeName": {
                    "description": "The attribute name, e.g. \"color\".",
                    "type": "string"
                },
                "labelBlacklist": {
                    "description": "Optional blacklist of attribute labels (e.g. [\"red\", \"blue\"]). If\nnon-empty, results whose label is in this set will be filtered out.\nDuplicate or unknown labels are silently ignored.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "restricts": {
                    "description": "The list of restricted categories for this attribute. Those should\nmatch the categories returned by the detection step. If non-empty,\nresults for this attribute are returned when the most-confident category\ninferred from the detection step is part of this list. If left empty,\nresults are always returned for this attribute.\n\nExample (Text Format proto):\n\nattributes {\n  name: \"color\"\n}\nattributes {\n  name: \"fit\"\n  restricts: \"shorts\"\n  restricts: \"pants\"\n}\nattributes {\n  name: \"length\"\n  restricts: \"shorts\"\n}\n\nWhen doing inference:\n* detected category = \"pants\": \"color\", \"fit\" and \"length\" results are\n                               returned,\n* detected category = \"shorts\" : only \"color\" and \"fit\" results are\n                                 returned.\n* detected category = \"bag\" : only \"color\" results are returned.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudServoClassifierParameters": {
            "description": "The parameters needed to perform classification through Servo. Note that the\nnumber of returned results is configured when the model is deployed on Servo,\nand cannot be overridden here.",
            "id": "VisionkitInternalCloudServoClassifierParameters",
            "properties": {
                "labelBlacklist": {
                    "description": "Optional blacklist of labels. If non-empty, results whose label is in this\nset will be filtered out. Duplicate or unknown labels are silently ignored.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to send requests to.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                },
                "scoreThreshold": {
                    "description": "A score threshold below which classifications are rejected. Optional.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudServoDetectorParameters": {
            "description": "The parameters needed to perform detection through Servo. Returns the single\nmost confident detection fitting the criterions defined below, if any.\nTODO (b/118370039): add support for multiple detections.\nNext ID: 6.",
            "id": "VisionkitInternalCloudServoDetectorParameters",
            "properties": {
                "classNameWhitelist": {
                    "description": "Optional whitelist of class names. If non-empty, detections whose\nclass name is not in this set will be filtered out. Duplicate or unknown\nclass names are ignored.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "classThresholds": {
                    "$ref": "VisionkitClassThresholds",
                    "description": "Per-class thresholds below which detections are rejected. If no threshold\nis set for a given class, it will directly be accepted."
                },
                "isDetectionMandatory": {
                    "description": "Whether detection results are mandatory or not. If set to false and the\ndetector returned no results, the detector will return a bounding box with\nno category covering the entire frame so that the cascade can continue.\nOtherwise the cascade will abort and return no results.",
                    "type": "boolean"
                },
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to send requests to.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                },
                "objectCentric": {
                    "description": "Whether or not to filter-out non object-centric detections. If true,\nbounding boxes that don't contain the center of the frame are filtered-out.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudServoEmbedderParameters": {
            "description": "The parameters needed to perform embedding extraction through Servo.",
            "id": "VisionkitInternalCloudServoEmbedderParameters",
            "properties": {
                "featureType": {
                    "description": "The type of features which determines how the floating-point values\nreturned in the Servo response should be post-processed:\n\n- If set to FLOAT, there is no post-processing and the Servo response\nvalues are used as-is,\n- If set to INT64, a scalar quantization is performed on the Servo response\nvalues to convert those into integers (see scalar_quantization_factor).\n\nAs the output feature values, after the post-processing, are used as input\nquery for ScaM, this feature type is expected to be the same than the one\nconfigured in ScamIndexParameters (see below). If there is a mismatch, a\nruntime error is returned.",
                    "enum": [
                        "FEATURE_TYPE_UNSPECIFIED",
                        "INT64",
                        "FLOAT"
                    ],
                    "enumDescriptions": [
                        "Unspecified feature type.",
                        "Integer features. This assumes the features have been scalar quantized\ninto bytes (int8) using a function like FixedByteQuantizedCompress [1].\n\n[1]:\ngoogle3/photos/vision/facenet/face_template_utils.h?q=FixedByteQuantizedCompress",
                        "Floating-point features."
                    ],
                    "type": "string"
                },
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to send requests to.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                },
                "scalarQuantizationFactor": {
                    "description": "If feature_type is set to INT64: the quantization factor used to convert\nfrom float to scalar-quantized integer embedding. Must be in [1, 128]: the\nrecommended default value used at indexing time [1] is 128, which quantizes\nvalues in [-128, 127].\n\n[1]:\ngoogle3/photos/vision/visionkit/indexing/build/utils/scam_utils.cc?q=IweToQuantizedGfv",
                    "format": "uint32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudServoSegmenterParameters": {
            "description": "The parameters needed to perform segmentation through Servo.",
            "id": "VisionkitInternalCloudServoSegmenterParameters",
            "properties": {
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to send requests to.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudVssClassifierParameters": {
            "description": "The parameters needed to call VSS for classification.",
            "id": "VisionkitInternalCloudVssClassifierParameters",
            "properties": {
                "vssClassifierModule": {
                    "enum": [
                        "VSS_CLASSIFIER_MODULE_UNSPECIFIED",
                        "ICA_HNET",
                        "ICA_INET",
                        "ICA_KNET"
                    ],
                    "enumDescriptions": [
                        "Unspecified VSS module. Specifying this as module will cause an\ninitialization-time error.",
                        "Calls the \"ica\" module with `ICA_HNET` as version context.",
                        "Calls the \"ica\" module with `ICA_INET` as version context.",
                        "Calls the \"ica\" module with `ICA_KNET` as version context."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudVssDetectorParameters": {
            "description": "The parameters needed to call VSS for detection.",
            "id": "VisionkitInternalCloudVssDetectorParameters",
            "properties": {
                "vssDetectorModule": {
                    "enum": [
                        "VSS_DETECTOR_MODULE_UNSPECIFIED",
                        "RAID_V2"
                    ],
                    "enumDescriptions": [
                        "Unspecified VSS module. Specifying this as module will cause an\ninitialization-time error.",
                        "Calls the \"localized_boxes\" module with `RAID_V2` as version context."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitInternalCloudVssMatcherParameters": {
            "description": "The parameters needed to call VSS for local feature-based matching.",
            "id": "VisionkitInternalCloudVssMatcherParameters",
            "properties": {
                "vssMatcherModule": {
                    "enum": [
                        "VSS_MODULE_UNSPECIFIED",
                        "VE_ART_RECOGNITION",
                        "DELF_PHILEAS_LANDMARK_RECOGNITION"
                    ],
                    "enumDescriptions": [
                        "Unspecified VSS module. Specifying this as module will cause an\ninitialization-time error.",
                        "Calls the \"ve\" module for artworks recognition by specifying\n`OUTPUT_TYPE_ART` as OutputType [1].\n\n[1]:\ngoogle3/vision/visualsearch/proto/visual_search_service.proto?q=output_type_wanted",
                        "Calls the \"delf_phileas\" module for landmark recognition."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitInternalHybridMatcher": {
            "description": "Configures an end-to-end hybrid matcher.\n\nNext ID: 3.",
            "id": "VisionkitInternalHybridMatcher",
            "properties": {
                "edgeMatcherParameters": {
                    "$ref": "VisionkitEdgeMatcherParameters",
                    "description": "Mandatory edge matcher parameters. The underlying matcher index is used as\na local cache, i.e. a call to the cloud is made only if no confident\nresults (based on spatial matching) are found on the device."
                },
                "matcherParameters": {
                    "$ref": "VisionkitInternalCloudVssMatcherParameters",
                    "description": "Mandatory matcher parameters. If an edge matcher is specified, this is used\nas fallback when it returns no results. Otherwise it is systematically\nused."
                }
            },
            "type": "object"
        },
        "VisionkitInternalHybridObjectRecognitionCascade": {
            "description": "Defines a hybrid object recognition cascade.\n\nBy definition of hybrid, the first stages of the cascade are performed on the\ndevice (e.g. detection & tracking, embedding extraction) and the final stages\nare handled server-side (e.g. k-NN search in a large index). This follows a\ncoarse-to-fine approach where the finest grained recognition is handled in\nthe cloud.\n\nIt is the counterpart of visionkit.hybrid.ObjectRecognitionCascade which by\nopposition does not include internal-only fields (e.g. server-side ScaM index\nparameters) that should not be exposed to the client consuming this proto by\nmeans of google3/google/internal/visionkit/v1/overlays.proto.\n\nNext ID: 3.",
            "id": "VisionkitInternalHybridObjectRecognitionCascade",
            "properties": {
                "frameAnnotator": {
                    "$ref": "VisionkitInternalHybridObjectRecognitionCascadeFrameAnnotator",
                    "description": "Mandatory FrameAnnotator."
                },
                "streamAnnotator": {
                    "$ref": "VisionkitInternalHybridObjectRecognitionCascadeStreamAnnotator",
                    "description": "Mandatory StreamAnnotator."
                }
            },
            "type": "object"
        },
        "VisionkitInternalHybridObjectRecognitionCascadeFrameAnnotator": {
            "description": "Defines an annotator running on the latest selected frame, when the capture\ntrigger is activated.",
            "id": "VisionkitInternalHybridObjectRecognitionCascadeFrameAnnotator",
            "properties": {
                "classifier": {
                    "$ref": "VisionkitInternalCloudClassifier",
                    "description": "Runs a server-side classifier on the latest selected frame."
                },
                "cloudMatcher": {
                    "$ref": "VisionkitInternalCloudMatcher",
                    "description": "Runs a cloud matcher on the latest selected frame."
                },
                "cloudSearcher": {
                    "$ref": "VisionkitInternalCloudSearcher",
                    "description": "Runs a cloud searcher on the latest selected frame."
                },
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the FrameAnnotator."
                },
                "matcher": {
                    "$ref": "VisionkitInternalHybridMatcher",
                    "description": "Runs an hybrid matcher on the latest selected frame."
                },
                "searcher": {
                    "$ref": "VisionkitInternalHybridSearcher",
                    "description": "Runs an hybrid searcher on the latest selected frame."
                },
                "trigger": {
                    "$ref": "VisionkitCaptureTrigger",
                    "description": "Mandatory capture trigger that specifies when the frame annotation runs."
                }
            },
            "type": "object"
        },
        "VisionkitInternalHybridObjectRecognitionCascadeStreamAnnotator": {
            "description": "Defines an annotator running continuously on the stream of frames provided\nby the device.",
            "id": "VisionkitInternalHybridObjectRecognitionCascadeStreamAnnotator",
            "properties": {
                "detector": {
                    "$ref": "VisionkitEdgeObjectDetector",
                    "description": "Mandatory. Runs a detector continuously on the stream of frames."
                },
                "display": {
                    "$ref": "VisionkitResultsDisplay",
                    "description": "Mandatory. How to show the results of the StreamAnnotator."
                }
            },
            "type": "object"
        },
        "VisionkitInternalHybridSearcher": {
            "description": "Configures an end-to-end hybrid searcher.\n\nNext ID: 6.",
            "id": "VisionkitInternalHybridSearcher",
            "properties": {
                "edgeEmbedderParameters": {
                    "$ref": "VisionkitEdgeEmbedderParameters",
                    "description": "Mandatory edge embedder parameters. Computed features are used as query by\nthe edge searcher (if any) and the cloud searcher (cloud-based ScaM index)."
                },
                "edgeSearcherParameters": {
                    "$ref": "VisionkitEdgeSearcherParameters",
                    "description": "Optional edge searcher parameters. The underlying searcher index is used as\na local cache, i.e. a call to the cloud is made only if no confident enough\nresults are found on the device."
                },
                "indexParameters": {
                    "$ref": "VisionkitInternalCloudScamIndexParameters",
                    "description": "Mandatory index parameters."
                },
                "metadataAugmenter": {
                    "description": "Optional MetadataAugmenter used to augment the labels returned by the\nsearcher.",
                    "enum": [
                        "METADATA_AUGMENTATION_NONE",
                        "METADATA_AUGMENTATION_GOOGLE_SEARCH_URL",
                        "METADATA_AUGMENTATION_KNOWLEDGE_GRAPH",
                        "METADATA_AUGMENTATION_REVGEO",
                        "METADATA_AUGMENTATION_MET_MUSEUM_URL"
                    ],
                    "enumDescriptions": [
                        "Do not perform any metadata augmentations.",
                        "Build a Google Search URL from the label and use it as `link` [1] field in\nthe returned ObjectMetadata object. The `display_name` and `thumbnail_url`\nare simply picked from the AnnotatorContext. This assumes the labels are\nhuman-readable strings.\n\n[1]: google3/google/internal/visionkit/v1/annotate_image.proto?q=link",
                        "Extract a display name, thumbnail URL and English wikipedia URL from the\nKnowledge Graph.\nThis requires the CloudAnnotator to produce MIDs as labels.",
                        "Extract a display name, iconic image URL and Google Maps URL from the\nRevGeo (go/revgeo) and Places (go/places-api-stubby) APIs.\nThis requires the CloudAnnotator to produce S2 Cell Tokens (formatted as\nhexadecimal strings without the \"0x\" prefix) as labels.",
                        "Build a redirect URL to the Met Museum (www.metmuseum.org) from the label\nand use it as `link` [1] field in the returned ObjectMetadata objects."
                    ],
                    "type": "string"
                },
                "searchRestrictParameters": {
                    "$ref": "VisionkitSearchRestrictParameters",
                    "description": "Optional search restrict parameters used to enable and specify how search\nrestricts should be set at query time. If left empty, KNN searches will be\nperformed without restricts.\n\nNote: search restricts are only supported by cloud-based searchers so far.\nDO NOT USE this option if you intend to configure an edge searcher."
                }
            },
            "type": "object"
        },
        "VisionkitKnnOptions": {
            "description": "Specifies the parameters of the search, e.g. number of results to return and\noptions to trade off performance and quality of the search.",
            "id": "VisionkitKnnOptions",
            "properties": {
                "fractionLeavesToSearch": {
                    "description": "When a partitioner is being used (KMEANS), this controls how many leaves\n(i.e. partitions) are taken into account at query time.\n\nExample: if this is set to 0.05, only the top 5% leaves with closest\ncentroids to the query are used to compute the KNN results. All other data\npoints belonging to other centroids are ignored.\n\nReducing this fraction has thus the effect to speed up the search, but also\npotentially decreases the accuracy. When KMEANS is used it is recommended\nto tune this value and pick one < 1.0 to take benefit of the partitioning.\n\nNOTE: If no partitioner is used, this fraction must be kept to 1.0 as all\ndata points are part of a single partition in this case.",
                    "format": "float",
                    "type": "number"
                },
                "numNearestNeighbors": {
                    "description": "The maximum number of results to return at query time.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitLeafSearcherOptions": {
            "description": "Contains the options related to the leaf searcher.",
            "id": "VisionkitLeafSearcherOptions",
            "properties": {
                "leafSearcherType": {
                    "enum": [
                        "UNDEFINED",
                        "LINEAR_SEARCH",
                        "PRODUCT_QUANTIZER"
                    ],
                    "enumDescriptions": [
                        "",
                        "Performs linear search on float embeddings.",
                        "Performs linear search on product quantized embeddings. The search is\ndone in two steps: a preprocessor computes a lookup table based on the\ncodebook vectors and the query, and then this lookup table is used to\ncompute the (approximate) distances between query and index points. Since\nthe query is not hashed, this corresponds to a \"asymmetric distance\ncomputation\" scheme."
                    ],
                    "type": "string"
                },
                "productQuantizerFilename": {
                    "description": "Absolute path to the file containing a AsymmetricHashingProto [1] binary\nproto with the codebook vectors used for product quantization.\n\n[1]: g3doc/photos/vision/visionkit/g3doc/engines/apis/searcher/indexing",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitLensForeignLanguageDetectionOptions": {
            "description": "Options for extracting OCR foreign language detection info.",
            "id": "VisionkitLensForeignLanguageDetectionOptions",
            "properties": {
                "enableForeignLanguageDetection": {
                    "description": "Return ForeignLanguageDetection as output of photo_ocr_calculator or not.",
                    "type": "boolean"
                },
                "minimumAggregateBoundingBoxRatio": {
                    "description": "An aggregate bounding box is defined as the sum of the area of all text\ndetection bounding boxes detected by OCR in pixels. This field represents\nthe minimum ratio of the aggregate bounding box to the image frame size.\nAny frame with a smaller ratio will not generate a foreign language\ndetection result.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitLensPersonNameExtractionOptions": {
            "description": "Options for the Person Name Extraction graph.",
            "id": "VisionkitLensPersonNameExtractionOptions",
            "properties": {
                "confidenceThreshold": {
                    "description": "Minimum OCR confidence needed before a person name is considered valid.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitLensTextSelectionOptions": {
            "description": "Options for the Text Selection graph.",
            "id": "VisionkitLensTextSelectionOptions",
            "properties": {
                "confidenceThreshold": {
                    "description": "Minimum OCR confidence needed",
                    "format": "float",
                    "type": "number"
                },
                "copyTextEnabled": {
                    "description": "Enable copy text feature",
                    "type": "boolean"
                },
                "minLongLineCount": {
                    "description": "Minimum number of long lines needed",
                    "format": "int32",
                    "type": "integer"
                },
                "minLongLineLength": {
                    "description": "Minimum word length for a line to be considered \"long\"",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitLensWifiExtractionOptions": {
            "description": "Options for the Wifi Extraction graph. Defaults were chosen using the current\neval set (http://google3/photos/vision/visionkit/lens/wifi/testdata/).",
            "id": "VisionkitLensWifiExtractionOptions",
            "properties": {
                "confidenceThreshold": {
                    "description": "Minimum OCR confidence needed before a word is considered valid.",
                    "format": "float",
                    "type": "number"
                },
                "maxEditDistanceThreshold": {
                    "description": "The maximum allowed Levenshtein distance from a SSID when fuzzy-matching.",
                    "format": "int32",
                    "type": "integer"
                },
                "shortSsidEditDistanceRatio": {
                    "description": "The multiplier applied to SSIDs to compute maximum fuzzy matching edit\ndistance if larger than max_edit_distance_threshold,\nmax_edit_distance_threshold will be used instead.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitLiftDetectorOptions": {
            "description": "Defines configs for lift detector calculator.",
            "id": "VisionkitLiftDetectorOptions",
            "properties": {
                "confidenceThreshold": {
                    "description": "A heuristic confidence threshold to obtain the binary lift detected\nclassification result. The lift detected is set to true if the inference\nconfidence is equal to or higher than this threshold.",
                    "format": "float",
                    "type": "number"
                },
                "windowStride": {
                    "description": "Number of sensor data samples as the stride between the start position of\nthe sensor data windows for two consecutive inferences. Must be a positive\ninteger.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitMatcher": {
            "description": "Configures a local feature-based matcher based on VSS.",
            "id": "VisionkitMatcher",
            "properties": {
                "vssMatcherParameters": {
                    "$ref": "VisionkitVssMatcherParameters"
                }
            },
            "type": "object"
        },
        "VisionkitMatcherCascadeOptions": {
            "description": "Message to configure a matcher cascade.\nNEXT ID: 4",
            "id": "VisionkitMatcherCascadeOptions",
            "properties": {
                "cloudMatcherOptions": {
                    "$ref": "VisionkitCloudMatcherOptions",
                    "description": "Options to configure the cloud matcher. If specified in addition to\nedge_matcher_options, hybrid retrieval is performed: first a recognition\ntakes place on the device. If no result is found with the edge matcher\n(which acts as a local cache), a call to the cloud is made.\nSome clients will want to crop the image with a detection and fall back\ndirectly to the cloud (without any on-device matching). For such a case,\nleave edge_matcher_options empty."
                },
                "detectionType": {
                    "enum": [
                        "NONE",
                        "EXTERNAL"
                    ],
                    "enumDescriptions": [
                        "The matcher will operate on the entire frame.",
                        "A detection is provided externally by a call to ReceiveDetections."
                    ],
                    "type": "string"
                },
                "edgeMatcherOptions": {
                    "$ref": "VisionkitMatcherOptions",
                    "description": "Options to configure the on-device matcher. This is where the path to the\ndatabase and other parameters to the search are specified. For hybrid\nretrieval (fallback to the cloud), see comments for\n`cloud_matcher_options`."
                }
            },
            "type": "object"
        },
        "VisionkitMatcherOptions": {
            "description": "Next Id: 5",
            "id": "VisionkitMatcherOptions",
            "properties": {
                "customCongasConfig": {
                    "$ref": "VisionkitExternalFile",
                    "description": "Advanced option used to specify a custom CongasConfig[1] for the Congas\nfeatures extractor used at query time. This is usually needed to optimize\naccuracy on some use cases. It is assumed a compatible config has been used\nto generate the index file (see index_options).\n\nJust leave this field empty to use the default config that is embedded by\nthe Matcher and should be good enough for most use cases.\n\nThe config file is expected to be in binary proto format.\n\n[1]: google3/image/content/feature/congas/v2/congas_config.proto"
                },
                "customSpatialMatcherConfig": {
                    "$ref": "VisionkitExternalFile",
                    "description": "Advanced option used to specify a custom SpatialMatcherV2Config[1] for the\npairwise matching stage performed at query time. This is usually needed to\noptimize accuracy on some use cases.\n\nJust leave this field empty to use the default config that is embedded by\nthe Matcher and should be good enough for most use cases.\n\nThe config file is expected to be in binary proto format.\n\n[1]:\ngoogle3/photos/vision/objectrec/visual_words/spatial_matcher/spatial_matcher_v2_config.proto"
                },
                "indexOptions": {
                    "$ref": "VisionkitMatcherOptionsIndexOptions"
                },
                "knnOptions": {
                    "$ref": "VisionkitMatcherOptionsKnnOptions"
                }
            },
            "type": "object"
        },
        "VisionkitMatcherOptionsIndexOptions": {
            "description": "Parameters for the index that holds the on-device database to search\nagainst.",
            "id": "VisionkitMatcherOptionsIndexOptions",
            "properties": {
                "externalIndexFile": {
                    "$ref": "VisionkitExternalFile",
                    "description": "External file for the serialized index (see below for more details). This\nis used at initialization time to load the search index data in memory.\n\nThe index is made of several files, using distinct extensions (typically:\n\".meta\", \".258\" and \".259\"). What you must specify via this option is the\ncommon prefix.\n\nExample: after index building[1], you get the following files:\n\n    /var/local/myindex.meta\n    /var/local/myindex.258\n    /var/local/myindex.259\n\nTo create a Matcher, you must specify index options as follows:\n\n    external_index_file: { file_name: \"/var/local/myindex\" }\n\n[1]: go/visionkit-engines/matcher#indexing"
                }
            },
            "type": "object"
        },
        "VisionkitMatcherOptionsKnnOptions": {
            "description": "Defaults parameters for query time.",
            "id": "VisionkitMatcherOptionsKnnOptions",
            "properties": {
                "distanceThreshold": {
                    "description": "The distance threshold above which results should be rejected. It must be\nstrictly positive otherwise an error is returned at initialization time.",
                    "format": "float",
                    "type": "number"
                },
                "maxImageSize": {
                    "description": "If > 0, scale down query image if bigger than this size before doing the\ninternal processing. If <= 0 this has no effect.\n\nNOTE: The coordinates in the output k-NN results (bounding polygon: see\nknn.proto) remain expressed in the original input image dimensions.",
                    "format": "int32",
                    "type": "integer"
                },
                "maxResults": {
                    "description": "The maximum number of results to return. If <= 0 this has no effect.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitMixer": {
            "description": "Configures an annotator using the VSL Server v1 API.",
            "id": "VisionkitMixer",
            "properties": {
                "vslMixerParameters": {
                    "$ref": "VisionkitVslMixerParameters",
                    "description": "The parameters needed to call the VSL Server v1 API."
                }
            },
            "type": "object"
        },
        "VisionkitObjectCategoryMatcher": {
            "description": "Inspects the `category` field of an AnnotateImageRequest, as inferred\non-device, to determine if it matches a list of predefined categories.",
            "id": "VisionkitObjectCategoryMatcher",
            "properties": {
                "category": {
                    "description": "The categories that trigger a match. This is a list of synonym categories\nthat should all use the same CloudAnnotator, e.g. \"shoe\" and \"sneaker\".\nComparison with the input `category` is done by exact string matching.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitObjectManagerOptions": {
            "description": "Options for ObjectManager.\nNEXT ID: 18",
            "id": "VisionkitObjectManagerOptions",
            "properties": {
                "boxCoordinateClamping": {
                    "description": "If true, the coordinate of the output bounding box will be clamped to be\nwithin the range.",
                    "type": "boolean"
                },
                "boxUpdateThreshold": {
                    "description": "If the IoU between the tracked box and the new box is less than the\nthreshold, that indicates that the size of the box got changed or the\nlocation of the box drifted. Thus, the tracked box will be updated to the\nnew box. The default value is empirically selected.\nIf no update is preferred, set to 0.",
                    "format": "float",
                    "type": "number"
                },
                "maxConsecutiveDetectionUs": {
                    "description": "Deletes a tracked object if it hasn\u2019t been tracked/detected for this\namount of time (in us). This will be used when tracking is available.\nThis will be disabled when it's set to -1.",
                    "format": "int32",
                    "type": "integer"
                },
                "maxNumObjectsToBeTracked": {
                    "description": "Maximum number of objects to be tracked. Default is 5. Use a negative value\nto disable this limit and track all available objects.",
                    "format": "int32",
                    "type": "integer"
                },
                "mergeSameObjectLabelOnly": {
                    "description": "If true, the new detection should have the same object label and overlap\nwith the existing object to be considered as \"already tracked object\".\nIn other words, if false, ignore the object label when the new detection is\ncompared with the existing object. Note that this doesn't honor hierarchy\ninformation.",
                    "type": "boolean"
                },
                "minDetectionScore": {
                    "description": "Minimum detection score to be added as a new object to be tracked.",
                    "format": "float",
                    "type": "number"
                },
                "minMergeIou": {
                    "description": "If IoU between two objects are larger than this, they will be considered\nas the same object.",
                    "format": "float",
                    "type": "number"
                },
                "minMergeOverlappingRatio": {
                    "description": "Minimum overlapping ratio to be merged. If the object overlaps with other\nobjects more than this, it will be considered as a duplicate and merged.",
                    "format": "float",
                    "type": "number"
                },
                "minTrackingConfidence": {
                    "description": "Mimimum tracking confidence to trust. Otherwise, it will be deleted from\nthe list to add a new box from the detection.",
                    "format": "float",
                    "type": "number"
                },
                "objectMapCleanUpIntervalUs": {
                    "description": "object_map in ObjectManager will be regularly cleaned up based on the\nlatest timestamp of the object and this determines how often the object map\nhas to be cleaned up. Default is set to 1000000. That is, object_map will\nbe cleaned up every 1 sec.",
                    "format": "int32",
                    "type": "integer"
                },
                "objectSelectionPreference": {
                    "description": "When there are more objects than the max number of objects to track, we\ndetermine which objects to be kept based on this preference.",
                    "enum": [
                        "NONE",
                        "CONFIDENCE_BASED",
                        "LOCATION_BASED"
                    ],
                    "enumDescriptions": [
                        "",
                        "Prefer the object with high confidence.",
                        "Prefer the centered object."
                    ],
                    "type": "string"
                },
                "smallObjectAreaIgnoreTrackingConfidence": {
                    "description": "Returns a high tracking confidence if the area of the tracked object is\nsmaller than the value so that the object is not filtered by the tracking\nconfidence. The tracking confidence is intentionally artifically high.\nThe pursuit tracking confidence is computed based on the number of inliers\nin a heuristic way, so if the object is too small (e.g. # of inliers is\nless than 10), the confidence becomes 0 even if it's correctly tracked.\nThis option helps deal with this corner case.",
                    "format": "float",
                    "type": "number"
                },
                "textObjectManagerOptions": {
                    "$ref": "VisionkitTextObjectManagerOptions"
                },
                "weightToPreviousDetections": {
                    "description": "Not implemented. Do not use.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitObjectTrackerOptions": {
            "description": "Options commonly used for every tracker and tracking management.",
            "id": "VisionkitObjectTrackerOptions",
            "properties": {
                "boxTrackerOptions": {
                    "$ref": "VisionkitBoxTrackerOptions",
                    "description": "Options specific to the tracker used."
                },
                "minBoxEdgeLength": {
                    "description": "Minimum normalized length of the edge of the box to be tracked. [0.f, 1.f].",
                    "format": "float",
                    "type": "number"
                },
                "minMergeIou": {
                    "description": "Merges a fresh detection with a tracked object if their\nintersection-over-union is more than this value.",
                    "format": "float",
                    "type": "number"
                },
                "minMergeOverlappingRatio": {
                    "description": "Minimum overlapping ratio to be merged. If the object overlaps with other\nobjects more than this, it will be considered as a duplicate and merged.",
                    "format": "float",
                    "type": "number"
                },
                "nicePriorityLevel": {
                    "description": "The nice priority level of the tracker threads.\nThe nice priority level is 0 by default, and lower value means higher\npriority. The valid thread nice priority level value range varies by OS.\nRefer to system documentation for more details.",
                    "format": "int32",
                    "type": "integer"
                },
                "numThreads": {
                    "description": "Number of threads for motion analysis. It is 4 by default (suggested by\nthe pursuit team).",
                    "format": "int32",
                    "type": "integer"
                },
                "trackingModule": {
                    "description": "Tracking module to run.",
                    "enum": [
                        "UNKNOWN_MODULE",
                        "PURSUIT_4DOF"
                    ],
                    "enumDescriptions": [
                        "Default type.",
                        "2D box tracker used in Pursuit."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitOcrOptions": {
            "description": "Ocr options.\nNext ID: 10",
            "id": "VisionkitOcrOptions",
            "properties": {
                "configLabel": {
                    "description": "[Required] PhotoOcr config label.",
                    "type": "string"
                },
                "flushTensorCache": {
                    "description": "The rest of the fields below are optional.\nFlush the OCR engine's cache for every frame to measure the recognizer\nruntime correctly. Note that it is only used for testing purposes,\nespecially when there is a small number (like <100) of test images.",
                    "type": "boolean"
                },
                "minFractionOcrLineSize": {
                    "description": "Options only available in Camera OCR\nMinimum relative size of OCR line to be processed. If supplied, smaller\nlines would not be recognized by the OCR engine.\nSpecified as min(boxWidth / imageWidth, boxHeight / imageHeight)",
                    "format": "float",
                    "type": "number"
                },
                "minFractionScriptAndLangMajority": {
                    "description": "Minimum fraction of OCR lines required to be considered majority script or\nlanguage. (numOfLinesWithScriptX / totalNumOfLines)",
                    "format": "float",
                    "type": "number"
                },
                "minResolutionPx": {
                    "description": "Options only available in Screen understanding OCR\nMin resolution required on the image. This is used to decide\n\"down_scale_power_of_two\" when calling OCR engine.",
                    "format": "int32",
                    "type": "integer"
                },
                "modelDir": {
                    "description": "[Required] Directory of the model data. To run OCR engine with preloaded\nmodel files, `mode_dir` is set to the directory storing the model files\ndirectly. To run OCR engine with dynamically downloaded model files,\n`model_dir` must be set to a writable directory for extracting the zipped\nOCR model files.",
                    "type": "string"
                },
                "modelFileDescriptor": {
                    "description": "[Optional] File descriptor of a zip file which contains the required model\nfiles. The model files will be extracted to the path specified in\n`model_dir` when configuring OCR engine.",
                    "format": "int32",
                    "type": "integer"
                },
                "useAsynchronousStateTransitions": {
                    "description": "If true, starts, cancels, and stops OCR on a background thread. This\nis typically only needed when OCR initialization is slow. For example,\nQualcomm driver model preparation for OCR on a LG G8 can take over 600ms.\nSee the comments in ocr_life_cycle_calculator.cc for more details.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitOcrProcessorOptions": {
            "description": "Ocr processor options.\nNext tag: 7",
            "id": "VisionkitOcrProcessorOptions",
            "properties": {
                "documentMergerConfig": {
                    "$ref": "VisionkitDocumentMergerConfig"
                },
                "mergeDocuments": {
                    "description": "When set to false, we will simply apply extracted transform and overlay\ndetections on each other.\nWhen set to true, we will eliminate overlaps and extract a unique document.",
                    "type": "boolean"
                },
                "pairwiseRegistrationConfig": {
                    "$ref": "VisionkitPairwiseRegistrationConfig"
                },
                "registerDocuments": {
                    "description": "When true, we will spatially register subsequent OCR runs.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitOverlayConfiguration": {
            "description": "Defines a configuration made to customize the server and (optionally) the\nclient behavior to a specific end-to-end visual search experience, a.k.a.\n\"overlay\". For example, a end-to-end visual search experience focussed on\nnatural world recognition would want to use an embedder dedicated to natural\nworld, perform kNN search in a dedicated dataset of plants and animals, and\nrestrict on-device models to a whitelist of natural world categories.\nNext ID: 19",
            "id": "VisionkitOverlayConfiguration",
            "properties": {
                "apiKeyWhitelist": {
                    "description": "The list of whitelisted API keys for this overlay. This controls the\nvisibility of each overlay based on the API key used at query time. At\nruntime, calling the AnnotateImage() endpoint with an API key not in this\nlist will return a permission denied error, and calling the ListOverlays()\nendpoint will only return the overlays containing the provided API key in\nthis list.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "base64Icon": {
                    "description": "A base64-encoded icon for this overlay. The string only contains the raw\nbase64-encoded icon (i.e. without \"data:image/png;base64\" prefix). The\nimage is either in JPEG or PNG format. This is sent to the client through\nthe ListOverlays() endpoint, and should be used by clients that need to\nsupport multiple different overlays.",
                    "type": "string"
                },
                "clientConfiguration": {
                    "$ref": "VisionkitV1ClientConfiguration",
                    "description": "The (optional) client-side configuration associated with this overlay. This\nis sent to the client through the ListOverlays() endpoint."
                },
                "cloudObjectRecognitionCascade": {
                    "$ref": "VisionkitInternalCloudObjectRecognitionCascade"
                },
                "description": {
                    "description": "An optional description for this configuration. This is sent to the client\nthrough the ListOverlays() endpoint, and should be used by clients that\nneed to support multiple different overlays.",
                    "type": "string"
                },
                "displayName": {
                    "description": "A display name for this configuration, e.g. \"Storefront\" or \"iNaturalist\".\nThis is sent to the client through the ListOverlays() endpoint, and should\nbe used by clients that need to support multiple different overlays.",
                    "type": "string"
                },
                "edgeObjectRecognitionCascade": {
                    "$ref": "VisionkitEdgeObjectRecognitionCascade",
                    "description": "Edge, hybrid or cloud cascades dedicated to object recognition.\nTypical use cases: products, natural world or landmark recognition."
                },
                "hybridObjectRecognitionCascade": {
                    "$ref": "VisionkitInternalHybridObjectRecognitionCascade"
                },
                "internal": {
                    "description": "Whether this overlay is reserved for internal use or not. This is sent to\nthe client through the ListOverlays() endpoint, and should be used by\ninternal clients that need to support multiple different overlays.",
                    "type": "boolean"
                },
                "name": {
                    "description": "A unique identifier for this configuration. This is sent to the client\nthrough the ListOverlays() endpoint, and expected by the AnnotateImage()\nendpoint to trigger using the ServerConfiguration below at query time.",
                    "type": "string"
                },
                "requiresLocationContext": {
                    "description": "Whether this overlay requires access to the user location. This is sent to\nthe client through the ListOverlays() endpoint. When true, the client is\nexpected to fill the `location_context` [1] field at query time; failing to\ndo so will cause the AnnotateImage() endpoint to return an\n\"InvalidArgument\" error.\n\n[1]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=location_context",
                    "type": "boolean"
                },
                "serverConfiguration": {
                    "$ref": "VisionkitServerConfiguration",
                    "description": "The server-side configuration associated with this overlay. This defines\nhow requests sent to the server are handled."
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "An optional superpack configuration for dynamic model downloading.\n\nDEPRECATED: each edge annotator now specifies its own `Superpack`. See [1].\n\n[1]:\ngoogle3/photos/vision/visionkit/server/boq/proto/edge_annotators.proto?q=superpack\n"
                },
                "tag": {
                    "description": "An optional list of tags describing this overlay based on the Mobile RAID\nset of labels (go/mobile-object-labeler). Those are meant to power an\n\"overlay suggestion\" feature, i.e. extract labels from the live viewfinder\nusing Mobile RAID, and find the list of overlays that share some of them.\n\nPlease refer to the best-so-far labelmap[1] and use lowercase display names\nExample:\n\n    tag: \"animal\" # which corresponds to \"/m/0jbk\"\n\n[1]:\ngoogle3/photos/vision/visionkit/engines/model_zoo/classifiers/mobile_object_labeler_v0_1_x_labelmap.pbtxt",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitPairwiseRegistrationConfig": {
            "description": "Parameters controlling how OCR detections are registered to each other.",
            "id": "VisionkitPairwiseRegistrationConfig",
            "properties": {
                "maxHeightDifferenceFraction": {
                    "format": "float",
                    "type": "number"
                },
                "maxPixelDistanceFraction": {
                    "description": "The maximum pixel distance for matching.\nThis number is specified as the fraction of the diagonal size in pixels\nof the input image.",
                    "format": "float",
                    "type": "number"
                },
                "maxRansacIterations": {
                    "format": "int32",
                    "type": "integer"
                },
                "maxRansacSamples": {
                    "format": "int32",
                    "type": "integer"
                },
                "minInlierFraction": {
                    "description": "We will not accept a registration if the number of inlier fraction (wrt.\nthe number of words we are matching) is lower than this.",
                    "format": "float",
                    "type": "number"
                },
                "minRansacConfidence": {
                    "format": "float",
                    "type": "number"
                },
                "minRansacIterations": {
                    "description": "Ransac parameters.",
                    "format": "int32",
                    "type": "integer"
                },
                "minRansacSamples": {
                    "description": "The ransac sample ranges",
                    "format": "int32",
                    "type": "integer"
                },
                "ransacSampleFraction": {
                    "format": "float",
                    "type": "number"
                },
                "refineSolutionUsingInliers": {
                    "description": "If true, we will compute a transform using the inliers to provide a more\naccurate homography estimate,",
                    "type": "boolean"
                },
                "useHomography": {
                    "description": "Whether we should use Homography for transform. If not we will use simple\naffine.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitParticleExtractorOptions": {
            "description": "ParticleExtractorOptions controls the behavior of particle extractor applied\non the screen input.",
            "id": "VisionkitParticleExtractorOptions",
            "properties": {
                "cellSize": {
                    "description": "Parameters that control particle extraction:\nWidth and height of a grid cell in pixels.",
                    "format": "int32",
                    "type": "integer"
                },
                "cellThreshold": {
                    "description": "Classify cell as positive iff the count of unique colors of selected pixels\nin the cell is equal to or greater than this.",
                    "format": "int32",
                    "type": "integer"
                },
                "pixelsPerCell": {
                    "description": "Number of pixels per cell to process.",
                    "format": "int32",
                    "type": "integer"
                },
                "subdivideGrids": {
                    "description": "Revisit the generated boxes, identify enclosed grids, and split them\nfurther.",
                    "type": "boolean"
                },
                "topN": {
                    "description": "If set, the particle extractor generates at most top_n particles\nsorted by size/area.",
                    "format": "int32",
                    "type": "integer"
                },
                "tuneBorders": {
                    "description": "Whether to fine tune borders of extracted photo particles.",
                    "type": "boolean"
                },
                "tuneBordersThreshold": {
                    "description": "Classify a line on a photo particle border as positive iff the count of\nunique colors of selected pixels on the line is equal to or greater than\nthis.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitPartitionerOptions": {
            "description": "Contains the options used for partitioning the data into smaller chunks\ncalled leaves.",
            "id": "VisionkitPartitionerOptions",
            "properties": {
                "kmeansCentroidsFilename": {
                    "description": "Absolute path to the file containing a PartitionerProto [1] binary proto\nwith the centroids.\n\n[1]: g3doc/photos/vision/visionkit/g3doc/engines/apis/searcher/indexing",
                    "type": "string"
                },
                "partitionerType": {
                    "enum": [
                        "NONE",
                        "KMEANS"
                    ],
                    "enumDescriptions": [
                        "",
                        ""
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitPipelineConfig": {
            "description": "VisionKit Pipeline configuration proto.\nFor more details on the synchronous and asynchronous APIs, see\ngo/visionkit-pipeline-asynchronous-synchronous-apis.",
            "id": "VisionkitPipelineConfig",
            "properties": {
                "analyticsOptions": {
                    "$ref": "VisionkitAnalyticsOptions"
                },
                "asynchronousApiOptions": {
                    "$ref": "VisionkitAsynchronousApiOptions",
                    "description": "Options for the asynchronous API (ReceivePreviewFrame and ReceiveYuvFrame)."
                },
                "enableMobileiqTracing": {
                    "description": "Enable MobileIQ tracing.",
                    "type": "boolean"
                },
                "schedulerOptions": {
                    "$ref": "VisionkitSchedulerOptions",
                    "description": "Options that contain details for which vision engines to run and\nhow to schedule them."
                },
                "synchronousApiOptions": {
                    "$ref": "VisionkitSynchronousApiOptions",
                    "description": "Options for the synchronous API (ProcessFrame)."
                },
                "useExternalSoFile": {
                    "description": "If true, the .so file is loaded by another source. This is useful if an\napplication has a master .so file that combines many different libraries.",
                    "type": "boolean"
                },
                "useStub": {
                    "description": "If true, stubs the native pipeline. No .so file is loaded. This is useful\nfor tests.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitPredictionFilterOptions": {
            "description": "PredictionFilterOptions controls the behavior of filter applied on the\nstream of classification results.\nSee PredictionFilterCalculator in\nphotos/vision/visionkit/drishti/prediction_filter_calculator.cc for more\ninformation.",
            "id": "VisionkitPredictionFilterOptions",
            "properties": {
                "alpha": {
                    "description": "For exponential moving average (the only type of filtering supported as of\nAug 21, 2018), alpha is the coefficient used in the update equation:\nstate_new = alpha * X + (1 - alpha) * state_prev.\n\nAlpha is a real value constant in range [0.0; 1.0].\n\nMaking alpha = 1 is the same as not doing any smoothing at all (zero delay,\nlow output stability).\n\nMaking alpha = 0 is the same as using the first input in the stream as the\napproximation for all values in the stream (infinite delay, perfect output\nstability).\n\nIn general, the higher the alpha, the more weight new incoming data has.",
                    "format": "double",
                    "type": "number"
                },
                "minMatchIou": {
                    "description": "Minimum IoU to be considered as same when a new detection box is given.\nIf the IoU between the new detection box and the existing box is larger\nthan the number, they are considered same and update the detection state\nusing the new detection result. The default value is empirically selected.",
                    "format": "float",
                    "type": "number"
                },
                "outputMaxItems": {
                    "description": "No more that output_max_items results will be output by the filter at one\ntime. If 0, no restrictions will apply. Note that this might result in\nfilter outputting a large number of items. For example, if decaying is slow\n(alpha is close to 0), the filter's state is likely to keep growing over\ntime.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitProcessingIntervalOptions": {
            "id": "VisionkitProcessingIntervalOptions",
            "properties": {
                "perEngineOptions": {
                    "items": {
                        "$ref": "VisionkitProcessingIntervalOptionsPerEngineOptions"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitProcessingIntervalOptionsPerEngineOptions": {
            "id": "VisionkitProcessingIntervalOptionsPerEngineOptions",
            "properties": {
                "engineName": {
                    "type": "string"
                },
                "minIntervalBetweenRunsUs": {
                    "description": "Minimum sleeping time (in microseconds) between each processing.",
                    "format": "int64",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitQuadDetectionOptions": {
            "description": "Quad detection options.",
            "id": "VisionkitQuadDetectionOptions",
            "properties": {
                "allowedDocumentRatios": {
                    "description": "Width:height ratios for documents to detect",
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                },
                "classifierTriggerCondition": {
                    "items": {
                        "$ref": "VisionkitClassTriggerCondition"
                    },
                    "type": "array"
                },
                "dimensionRatioMaxSlack": {
                    "description": "Allowable + or - slack in width:height ratio",
                    "format": "float",
                    "type": "number"
                },
                "minHeightPercent": {
                    "description": "Minimum height a quad must have in order to be considered a document.\nMeasured relative to the frame size from [0...1]. This filtering will be\nignored if it is not set or if the minimum width is met instead.",
                    "format": "float",
                    "type": "number"
                },
                "minRelativeKeypointEdgeOffset": {
                    "description": "Minimum relative amount a keypoint must be from an edge to be considered a\nvalid corner of a document. A keypoint is a normalized x,y coordinate of a\nquad corner.",
                    "format": "float",
                    "type": "number"
                },
                "minWidthPercent": {
                    "description": "Minimum width a quad must have in order to be considered a document.\nMeasured relative to the frame size from [0...1]. This filtering will be\nignored if it is not set or if the minimum height is met instead.",
                    "format": "float",
                    "type": "number"
                },
                "shouldBeClassifierTriggered": {
                    "description": "Gate the running of the quad detector by a whitelist of class names.",
                    "type": "boolean"
                },
                "ssdOptions": {
                    "$ref": "VisionkitSsdQuadDetectionOptions"
                }
            },
            "type": "object"
        },
        "VisionkitQuimbyReverseImageSearcherParameters": {
            "description": "The parameters needed to perform a reverse image search through Quimby.",
            "id": "VisionkitQuimbyReverseImageSearcherParameters",
            "properties": {
                "useCategoryAsUserSpecifiedQuery": {
                    "description": "If set to true, the `user_specified_query` field [1] of the QuimbyRequest\nis automatically populated with the `category` field from the input\nAnnotateImageRequest. Otherwise, it is left empty.\n\n[1]: google3/image/repository/api/quimby.proto?q=user_specified_query",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitRectF": {
            "description": "Rectangle that defines edges with floats. This can be used to represent a\nnormalized bounding box, with values in the range of [0, 1].",
            "id": "VisionkitRectF",
            "properties": {
                "bottom": {
                    "format": "float",
                    "type": "number"
                },
                "left": {
                    "format": "float",
                    "type": "number"
                },
                "right": {
                    "format": "float",
                    "type": "number"
                },
                "top": {
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitResourcePreferences": {
            "description": "Next ID: 3",
            "id": "VisionkitResourcePreferences",
            "properties": {
                "numThreads": {
                    "description": "Number of worker threads to use in Drishti. If 0, then no background\nthreads are created and the main thread is used.",
                    "format": "int32",
                    "type": "integer"
                },
                "powerHint": {
                    "enum": [
                        "DEFAULT",
                        "LOWEST_ENERGY",
                        "LOWEST_LATENCY",
                        "DEBUG_CPU_ONLY"
                    ],
                    "enumDescriptions": [
                        "",
                        "Leverage low cores and lowest power hardware acceleration is available.\nLatency may be high.",
                        "Leverage highest performance hardware, regardless of its power\nconsumption. This is useful for performance benchmarks.",
                        "For debugging only. This overrides all general preferences."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitResultsAccumulatorOptions": {
            "description": "Options for how results are returned by the results accumulator.\nFor more details, see go/visionkit-pipeline-results-accumulator.",
            "id": "VisionkitResultsAccumulatorOptions",
            "properties": {
                "mode": {
                    "enum": [
                        "UNSPECIFIED",
                        "PERIODIC",
                        "FLUSH_IMMEDIATELY",
                        "SYNCHRONIZED"
                    ],
                    "enumDescriptions": [
                        "",
                        "Flush results periodically, at the same rate that frames are received.",
                        "As soon as an engine is complete, flush results back to the client.",
                        "All outputs and callbacks for a given image frame are synchronized.\nIn this mode, the onResult callback is guaranteed to be called for every\nframe.\nThis is a beta feature. Please contact visionkit-pipeline@ if you\nencounter any issues."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitResultsDisplay": {
            "description": "Defines how and if results from a StreamAnnotator or FrameAnnotator should be\ndisplayed.",
            "id": "VisionkitResultsDisplay",
            "properties": {
                "customHtml": {
                    "description": "Optional custom HTML to load in a webview overlaying the camera to lay out\nthe results. If the type is not CUSTOM, this is ignored.",
                    "type": "string"
                },
                "type": {
                    "description": "Mandatory type for displaying annotation results.",
                    "enum": [
                        "NONE",
                        "NATIVE",
                        "CUSTOM"
                    ],
                    "enumDescriptions": [
                        "Do not display the results. This is useful when cascading annotators\nwhere intermediary results should not be displayed.",
                        "Display the results using the client native UI.",
                        "Display the results using custom HTML (defined below)."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitReverseImageSearcher": {
            "description": "Configures an annotator using Quimby for reverse image search.",
            "id": "VisionkitReverseImageSearcher",
            "properties": {
                "quimbyReverseImageSearcherParameters": {
                    "$ref": "VisionkitQuimbyReverseImageSearcherParameters",
                    "description": "The parameters needed to call Quimby."
                }
            },
            "type": "object"
        },
        "VisionkitRpcClientOptions": {
            "description": "Contains the options to configure the gRPC client to the VisionKit Server.\n\nNext Id: 4",
            "id": "VisionkitRpcClientOptions",
            "properties": {
                "apiKey": {
                    "description": "The API key to make calls to the server. It is mandatory to fill this field\nwith a valid API key. See go/visionkit-server/test-with-your-own-client for\nmore information.",
                    "type": "string"
                },
                "metadata": {
                    "description": "An optional collection of metadata key-value pairs sent from the gRPC\nclient. The `api_key` value (see above) is automatically converted into a\nkey-value pair behind the scenes (with meta_key = \"x-goog-api-key\") so it\nshould not be part of this collection as it would be redundant.",
                    "items": {
                        "$ref": "VisionkitRpcClientOptionsMetadata"
                    },
                    "type": "array"
                },
                "serverTarget": {
                    "description": "Address of the server to query.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitRpcClientOptionsMetadata": {
            "description": "A custom metadata key-value to be propagated to the server-side.\nSee grpc::ClientContext for more details, in particular if the value is\nbinary data.",
            "id": "VisionkitRpcClientOptionsMetadata",
            "properties": {
                "metaKey": {
                    "type": "string"
                },
                "metaValue": {
                    "format": "byte",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitScamIndexParameters": {
            "description": "The parameters needed to perform a k-NN search in a ScaM index. The index is\nassumed to be configured [1] to use 'SquaredL2Distance' as distance measure.\nNext ID: 10.\n\n[1]: http://cnsviewer/placer/prod/home/visionkit-prod/scam/configs",
            "id": "VisionkitScamIndexParameters",
            "properties": {
                "datasetName": {
                    "description": "The name of the ScaM dataset, as specified in the VisionKit ScaM\nconfiguration [1].\n\n[1]: /placer/prod/home/visionkit-prod/scam/configs/scam.config",
                    "type": "string"
                },
                "featureDimension": {
                    "description": "The dimension of features expected as input. This is used for sanity and\nconsistency checks at query time.",
                    "format": "int32",
                    "type": "integer"
                },
                "featureType": {
                    "description": "The type of features expected as input.",
                    "enum": [
                        "FEATURE_TYPE_UNSPECIFIED",
                        "INT64",
                        "FLOAT"
                    ],
                    "enumDescriptions": [
                        "Unspecified feature type.",
                        "Integer features. This assumes the features have been scalar quantized\ninto bytes (int8) using a function like FixedByteQuantizedCompress [1].\n\n[1]:\ngoogle3/photos/vision/facenet/face_template_utils.h?q=FixedByteQuantizedCompress",
                        "Floating-point features."
                    ],
                    "type": "string"
                },
                "filterDuplicateLabels": {
                    "description": "If set to true, only the first result with a given label will be retained.\nThis allows getting rid of duplicate results in the final Annotation-s.",
                    "type": "boolean"
                },
                "maxNumberResults": {
                    "description": "The maximum number of results to return.",
                    "format": "uint32",
                    "type": "integer"
                },
                "scamNumberNeighbors": {
                    "description": "The number of neighbors to request from ScaM. If set to zero or omitted,\nthe same value as `max_number_results` is used. We recommend using\nover-retrieval and thus setting a large enough value (e.g. 10 times\n`max_number_results`) if `filter_duplicate_labels` is set to true, in order\nto account for the neighbors that will be discarded from the results by the\nduplicate filter.",
                    "format": "uint32",
                    "type": "integer"
                },
                "scamRestrictsV3Parameters": {
                    "$ref": "VisionkitScamRestrictsV3Parameters",
                    "description": "The ScaM restricts V3 parameters to use with this ScaM dataset. If not set,\nfilter queries are not supported for this dataset."
                },
                "similarityThreshold": {
                    "description": "A cosine similarity threshold in [-1,1) below which low confidence results\nare rejected. Set to -1 to keep all results.\nInternally this threshold is converted to an L2 distance threshold : it is\nexpressed here as a cosine similarity for convenience, as this is a common\nmetric used by most research teams.",
                    "format": "float",
                    "type": "number"
                },
                "userinfoMetadataType": {
                    "description": "The type of the metadata expected to be serialized in the `userinfo` field\nof the GenericFeatureVector-s of the searched ScaM dataset. If unspecified,\nthe routing rule is considered invalid and will trigger a runtime error.",
                    "enum": [
                        "LABEL_DEFAULT",
                        "PRODUCT_META_DATA_LIST",
                        "PLACE_METADATA",
                        "ANNOTATION_METADATA"
                    ],
                    "enumDescriptions": [
                        "By default, metadata is assumed to be a plain string label, typically a\nKnowledge Graph MID or a human-readable display name.",
                        "Metadata of type ProductMetaDataList [1]. Each of its ProductMetaData\nfields must at least contain a name and an iconic image with a non-empty\nthumbnail URL.\n\n[1]: google3/photos/vision/products/proto/product_meta_data.proto",
                        "Metadata of type PlaceMetadata [1]. Each PlaceMetadata must contain an\nimage_url and place.feature_id field.\n\n[1]:\ngoogle3/knowledge/cerebra/sense/im2query/model/s2place/distillation/scam_metadata.proto",
                        "General purpose metadata proto [1].\n\n[1]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=Annotation"
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitScamRestrictsV3Parameters": {
            "id": "VisionkitScamRestrictsV3Parameters",
            "properties": {
                "category": {
                    "description": "If set, automatically appends a restrict with 'category' namespace using\nthis field as string value.",
                    "type": "string"
                },
                "s2cell15": {
                    "description": "DEPRECATED, please use `s2cell_level` below.\n\nIf true, automatically appends a restrict with 's2cell_15' namespace using\nan S2Cell ID level 15 [1] computed from the `location_context` [2] field of\nthe AnnotateImageRequest as uint64 value.\n\n[1]: http://g3doc/util/geometry/g3doc/devguide/s2cell_hierarchy.md\n[2]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=location_context",
                    "type": "boolean"
                },
                "s2cellLevel": {
                    "description": "If positive, automatically appends a restrict with 's2cell_%d' namespace\nusing an S2Cell ID of this level [1] computed from the `location_context`\n[2] field of the AnnotateImageRequest as uint64 value. Should be in the\nrange 1..kMaxCellLevel [3].\n\n[1]: http://g3doc/util/geometry/g3doc/devguide/s2cell_hierarchy.md\n[2]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=location_context\n[3]: google3/util/geometry/s2coords.h?q=kMaxCellLevel",
                    "format": "uint32",
                    "type": "integer"
                },
                "supportedKeys": {
                    "description": "The set of supported keys in the FilterQuery [1] provided in the\nAnnotateImageRequest. The namespaces specified by 'category' or 's2cell_15'\n(used below) are reserved keys and must not be used here. If not set, any\nkey, except reserved ones, may be used.\n\n[1]:\ngoogle3/google/internal/visionkit/v1/annotate_image.proto?q=FilterQuery",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitSchedulerDebuggingOptions": {
            "id": "VisionkitSchedulerDebuggingOptions",
            "properties": {
                "schedulerStatsRequired": {
                    "description": "If true, the pipeline outputs scheduler runtime stats.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitSchedulerOptions": {
            "description": "Options for scheduler.\nNext tag: 45",
            "id": "VisionkitSchedulerOptions",
            "properties": {
                "acceleratorWhitelist": {
                    "$ref": "AccelerationWhitelist",
                    "description": "Accelerator whitelist, see\nhttp://google3/intelligence/mobile_acceleration/."
                },
                "ambientOptions": {
                    "$ref": "VisionkitAmbientOptions",
                    "description": "PART 5: Ambient related options"
                },
                "barcodeOptions": {
                    "$ref": "VisionkitBarcodeOptions"
                },
                "baseDir": {
                    "description": "Base directory for downloaded files.",
                    "type": "string"
                },
                "classificationCascadeOptions": {
                    "description": "Enables classification cascade with predictions filter.\nIf this option is set, classifier_client_options must be empty.",
                    "items": {
                        "$ref": "VisionkitClassificationCascadeOptions"
                    },
                    "type": "array"
                },
                "classifierClientOptions": {
                    "description": "PART 1: Engines with output used by applications directly.",
                    "items": {
                        "$ref": "VisionkitClassifierClientOptions"
                    },
                    "type": "array"
                },
                "cloudCascadeOptions": {
                    "$ref": "VisionkitCloudCascadeOptions"
                },
                "coarseClassifierClientOptions": {
                    "description": "Coarse classifier options. Only PassThroughCoarseClassifier is supported\nat the moment. If not specified, VerticalOrientationCoarseClassifier will\nbe used as default.",
                    "items": {
                        "$ref": "VisionkitClassifierClientOptions"
                    },
                    "type": "array"
                },
                "customSubgraph": {
                    "items": {
                        "$ref": "VisionkitSubgraph"
                    },
                    "type": "array"
                },
                "customSubgraphName": {
                    "description": "Name of custom subgraph. See go/visionkit-pipeline-custom-subgraph.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "detectionCascadeOptions": {
                    "items": {
                        "$ref": "VisionkitDetectionCascadeOptions"
                    },
                    "type": "array"
                },
                "detectorClientOptions": {
                    "items": {
                        "$ref": "MobileSsdClientOptions"
                    },
                    "type": "array"
                },
                "embedderOptions": {
                    "items": {
                        "$ref": "VisionkitEmbedderOptions"
                    },
                    "type": "array"
                },
                "enableAudioProcessing": {
                    "type": "boolean"
                },
                "enableDrishtiProfiling": {
                    "description": "Enable Drishti profiling.",
                    "type": "boolean"
                },
                "enableInputRepository": {
                    "description": "PART 2: Vision-related features to improve efficiency\nEnable input repository",
                    "type": "boolean"
                },
                "enableOptimizations": {
                    "description": "Legacy flag to enable optimizations.",
                    "type": "boolean"
                },
                "externalStream": {
                    "description": "Names of any external streams provided from outside of VisionKit.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "faceCascadeOptions": {
                    "$ref": "VisionkitFaceCascadeOptions"
                },
                "foreignLanguageDetectionOptions": {
                    "$ref": "VisionkitLensForeignLanguageDetectionOptions",
                    "description": "Options to extract foreign language detection info from OCR."
                },
                "frameSelectorOptions": {
                    "$ref": "VisionkitFrameSelectorOptions"
                },
                "matcherCascadeOptions": {
                    "$ref": "VisionkitMatcherCascadeOptions"
                },
                "objectManagerOptions": {
                    "$ref": "VisionkitObjectManagerOptions",
                    "description": "Contains the parameters related to object tracking/management."
                },
                "ocrOptions": {
                    "$ref": "VisionkitOcrOptions",
                    "description": "PART 6: Options directly related to understanding text\nOptions for OCR engine."
                },
                "ocrProcessorOptions": {
                    "$ref": "VisionkitOcrProcessorOptions",
                    "description": "Options for OCR processor."
                },
                "personNameExtractionOptions": {
                    "$ref": "VisionkitLensPersonNameExtractionOptions",
                    "description": "Options to extract person name info from OCR. At the moment, this only\nsupports Korean names."
                },
                "quadDetectionOptions": {
                    "$ref": "VisionkitQuadDetectionOptions",
                    "description": "Contains the parameters for quad detection."
                },
                "resourcePreferences": {
                    "$ref": "VisionkitResourcePreferences",
                    "description": "PART 3: Non-vision features\nPreferences for resource to use."
                },
                "schedulerDebuggingOptions": {
                    "$ref": "VisionkitSchedulerDebuggingOptions",
                    "description": "PART 4: Debug options\nOptions for debug setting."
                },
                "schedulingOptimizationOptions": {
                    "$ref": "VisionkitSchedulingOptimizationOptions",
                    "description": "Options to optimize vision engine scheduling.\nSee go/visionkit-pipeline-scheduling for more details."
                },
                "screenDetectionCascadeOptions": {
                    "$ref": "VisionkitScreenDetectionCascadeOptions"
                },
                "screenOptions": {
                    "$ref": "VisionkitScreenOptions"
                },
                "searcherCascadeOptions": {
                    "$ref": "VisionkitSearcherCascadeOptions"
                },
                "segmenterOptions": {
                    "items": {
                        "$ref": "VisionkitSegmenterOptions"
                    },
                    "type": "array"
                },
                "textOrientationOptions": {
                    "$ref": "VisionkitTextOrientationTrackerOptions",
                    "description": "Options for text orientation tracker."
                },
                "textSelectionOptions": {
                    "$ref": "VisionkitLensTextSelectionOptions",
                    "description": "Options to extract text selection info from OCR."
                },
                "trackerOptions": {
                    "$ref": "VisionkitObjectTrackerOptions",
                    "description": "Contains the parameters for tracking."
                },
                "wifiExtractionOptions": {
                    "$ref": "VisionkitLensWifiExtractionOptions",
                    "description": "Options to extract WiFi info from OCR."
                }
            },
            "type": "object"
        },
        "VisionkitSchedulingOptimizationOptions": {
            "description": "The VisionKit Pipeline's scheduling optimizer, if enabled, decides when to\nrun engines. This is critical for decreasing power usage. Different policies\nare supported, as described on go/visionkit-pipeline-scheduling.\n\nNext availabie id: 7",
            "id": "VisionkitSchedulingOptimizationOptions",
            "properties": {
                "classifierDutyCycleSettings": {
                    "$ref": "VisionkitDutyCycleSettings",
                    "description": "[Deprecated] Duty cycles for classifier calculators. Please use\nProcessingIntervalOptions instead."
                },
                "contextBasedSchedulingOptions": {
                    "$ref": "VisionkitContextBasedOptimizationOptions",
                    "description": "[Beta] Settings for context-based scheduling. This allows detectors to run\nless frequently if the tracker is confident. This is only relevant when the\ntracker is enabled. Please contact eunyoungkim@ before using."
                },
                "dutyCycleManagerOptions": {
                    "$ref": "VisionkitDutyCycleOptions",
                    "description": "Settings for duty cycle scheduling. This allows an engine to be active for\na percentage of a time period."
                },
                "processingIntervalOptions": {
                    "$ref": "VisionkitProcessingIntervalOptions",
                    "description": "Setting for simple interval scheduling. This allows clients to configure\nengines to sleep for a fixed period of time between runs."
                },
                "subpipelineOptions": {
                    "$ref": "VisionkitSubpipelineOptions",
                    "description": "Settings for subpipeline management, which is designed for runtime graph\nadjustment. WARNING: This feature is under active development."
                }
            },
            "type": "object"
        },
        "VisionkitScreenDetectionCascadeOptions": {
            "description": "Screen detection cascade options. The cascaded detector is built on top of\nthe output from ParticleExtractor and CoarseClassifier.\nNote that by the existing implementation, the final output detection results\nare in a flat manner, i.e., each detection result is not linked to its\ncorresponding region extracted by the particle extractor.\nNext ID: 5",
            "id": "VisionkitScreenDetectionCascadeOptions",
            "properties": {
                "boxClassifierOptions": {
                    "$ref": "VisionkitClassifierClientOptions",
                    "description": "Box labeling options used to run a classifier model on each detected object\nboxes. The detection labels are ignored: only the labels inferred by the\nclassifier are used for the returned results."
                },
                "coarseClassifierOptions": {
                    "$ref": "VisionkitClassifierClientOptions",
                    "description": "If provided, coarse classifier is used to filter out content without\ninterests."
                },
                "mobileSsdOptions": {
                    "$ref": "MobileSsdClientOptions",
                    "description": "If provided, the Mobile SSD model is used to detect objects within the\nregions inferred by the particle extractor."
                },
                "particleExtractorOptions": {
                    "$ref": "VisionkitParticleExtractorOptions",
                    "description": "If provided, particle extractor is used to detect image/photo like regions\nfrom the screen input."
                }
            },
            "type": "object"
        },
        "VisionkitScreenOptions": {
            "description": "Screen options for all screen processings in VisionKit Pipeline.",
            "id": "VisionkitScreenOptions",
            "properties": {
                "processContextRequired": {
                    "description": "If true, a ProcessContext is required to be passed in attached with each\ninput frame.",
                    "type": "boolean"
                },
                "screenDetectionCascadeOptions": {
                    "$ref": "VisionkitScreenDetectionCascadeOptions",
                    "description": "Screen detection cascade options."
                },
                "screenOcrOptions": {
                    "$ref": "VisionkitOcrOptions",
                    "description": "Screen ocr options."
                }
            },
            "type": "object"
        },
        "VisionkitScreenSelectorConfig": {
            "id": "VisionkitScreenSelectorConfig",
            "properties": {
                "diffRateThreshold": {
                    "description": "This field is used to define changing screens. Screens with number of diff\nhash rows > diff_rate_threshold * image_height are considered as\nchanging.",
                    "format": "float",
                    "type": "number"
                },
                "stableScreenTimeThresholdMs": {
                    "description": "This field is used to define stable screens, i.e., screens without changing\nfor a time period of stable_screen_time_threshold_ms are considered as\nstable.",
                    "format": "int64",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSearchRestrictOptions": {
            "description": "Message to configure search restrict options to be taken into account by the\nsearcher to narrow down the results.\n\nNOTE: so far, those are only supported by the cloud-based searcher. The\non-device searcher does not yet support such restrict capabilities.\n\nNEXT ID: 2",
            "id": "VisionkitSearchRestrictOptions",
            "properties": {
                "detectionOptions": {
                    "$ref": "VisionkitSearchRestrictOptionsDetectionOptions",
                    "description": "Detection specific options. If left empty, detection results (if any) will\nnot be used part of the search restricts."
                }
            },
            "type": "object"
        },
        "VisionkitSearchRestrictOptionsDetectionOptions": {
            "description": "Specific options that apply to detection results when those have been\ncomputed or received (see ReceiveDetections API) part of the cascade.",
            "id": "VisionkitSearchRestrictOptionsDetectionOptions",
            "properties": {
                "restrictKey": {
                    "description": "Under which filter query key (a.k.a. namespace) the label(s) from the\ndetection result(s) should be associated.\n\nE.g. for a given detection `{ label: \"bag\" score: 0.98 ... }`, specifying\n`restrict_key: \"category\"` will have for effect to build a search\nrestrict with key = \"category\" and value = \"bag\".\n\nIf multiple confident labels have been inferred for a given detection\nthen multiple restrict values will be set for the same key.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSearchRestrictParameters": {
            "description": "Message to configure search restrict parameters. Those are intended for edge\nor hybrid cascades1 featuring a searcher. In both cases, those\nparameters have to be used by the client while configuring the VisionKit\nPipeline, e.g. via the searcher cascade options:\n\ngoogle3/photos/vision/visionkit/pipeline/proto/searcher_cascade_options.proto\n\nExample: for a hybrid cascade featuring client-side detection and server-side\nretrieval, the client can turn the detection labels into search restricts (if\na \"shoe\" has been detected, restrict KNN search to shoes-only) thanks to\nthese parameters.\n\n[1]:\ngoogle3/photos/vision/visionkit/server/boq/proto/edge_annotators.proto\n[2]:\ngoogle3/photos/vision/visionkit/server/boq/proto/hybrid_annotators.proto\n\nNext ID: 2.",
            "id": "VisionkitSearchRestrictParameters",
            "properties": {
                "detectionParameters": {
                    "$ref": "VisionkitSearchRestrictParametersDetectionParameters",
                    "description": "Detection specific parameters. If left empty, detection results (if any)\nwill not be used part of the search restricts."
                }
            },
            "type": "object"
        },
        "VisionkitSearchRestrictParametersDetectionParameters": {
            "description": "Specific parameters that apply to detection results when those have been\ncomputed on the client-side, part of the recognition cascade.",
            "id": "VisionkitSearchRestrictParametersDetectionParameters",
            "properties": {
                "restrictKey": {
                    "description": "Under which filter query key (a.k.a. namespace) the label(s) from the\ndetection result(s) should be associated.\n\nE.g. for a given detection `{ label: \"bag\" score: 0.98 ... }`, specifying\n`restrict_key: \"category\"` will have for effect to build a search\nrestrict with key = \"category\" and value = \"bag\".\n\nIf multiple confident labels have been inferred for a given detection\nthen multiple restrict values will be set for the same key.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSearcher": {
            "description": "Configures an annotator using Servomatic for embedding computation,\nfollowed by a k-NN search in a ScaM index.",
            "id": "VisionkitSearcher",
            "properties": {
                "searcherParameters": {
                    "$ref": "VisionkitSearcherParameters",
                    "description": "The parameters needed to call Servomatic for embedding extraction followed\nby k-NN search in a ScaM index."
                }
            },
            "type": "object"
        },
        "VisionkitSearcherCascade": {
            "description": "Configures an annotator using Servomatic for detection and\nembedding computation, followed by a k-NN search in a ScaM index.",
            "id": "VisionkitSearcherCascade",
            "properties": {
                "allowSearchWithoutDetection": {
                    "description": "If true, search is performed on the entire frame if the detection step\nreturned no result. Otherwise, search is skipped and this annotator returns\nno result at all.",
                    "type": "boolean"
                },
                "defaultSearcherParameters": {
                    "$ref": "VisionkitSearcherParameters",
                    "description": "The default SearcherParameters to use if no key in\nsearcher_parameters_map matches the category returned by detection.\nRequired."
                },
                "searcherParametersMap": {
                    "additionalProperties": {
                        "$ref": "VisionkitSearcherParameters"
                    },
                    "description": "Maps the categories returned by detection to the SearcherParameters to use.\nOptional.",
                    "type": "object"
                },
                "servoDetectorParameters": {
                    "$ref": "VisionkitServoDetectorParameters",
                    "description": "The parameters needed to call Servomatic for detection. Required."
                }
            },
            "type": "object"
        },
        "VisionkitSearcherCascadeOptions": {
            "description": "Message to configure a searcher cascade made for embedding-based retrieval.\n\nIt currently supports three different setups:\n\n    #1 On-device only: embedding extraction and KNN search are achieved\n    on-device,\n\n    #2 Hybrid: embedding extracted on-device then KNN search performed\n    in-cloud,\n\n    #3 Hybrid with local cache: embedding extracted on-device, KNN search\n    first performed on-device with fallback to cloud if no confident results.\n\nBy default it uses the entire input camera frame for the processing. It is\npossible to perform the search on a specific bounding box by providing\ndetection data or by specifying a Mobile SSD model used for object\ndetection(see options below for more details).\n\nIt is currently limited to a single detection, i.e. it does not (yet) support\nperforming multiple KNN searches over multiple detected objects. By default\nany detection is accepted even if it is not object-centric.\n\nNEXT ID: 6",
            "id": "VisionkitSearcherCascadeOptions",
            "properties": {
                "cloudSearcherOptions": {
                    "$ref": "VisionkitCloudSearcherOptions",
                    "description": "Optional cloud-based searcher parameters. If it is not specified, then\nedge_searcher_options must be set, otherwise a runtime error is returned.\n\nIf specified in addition to edge_searcher_options, this corresponds to\nsetup #3 (hybrid with local cache).\n\nSome clients will want to compute the embedding on-device and send it\ndirectly to the cloud (no on-device search). In such a case, leave\nedge_searcher_options empty which corresponds to setup #2 (hybrid)."
                },
                "detectionType": {
                    "enum": [
                        "NONE",
                        "EXTERNAL",
                        "MOBILE_SSD"
                    ],
                    "enumDescriptions": [
                        "The Embedder will be run on the entire frame.",
                        "A detection is provided externally by a call to ReceiveDetections.",
                        "A detection is performed by internal Mobile SSD detector client."
                    ],
                    "type": "string"
                },
                "detectorClientOptions": {
                    "$ref": "MobileSsdClientOptions",
                    "description": "Must be provided if detection_type is set to MOBILE_SSD. If provided, the\nMobile SSD model used to detect objects in the incoming camera frames."
                },
                "edgeSearcherOptions": {
                    "$ref": "VisionkitSearcherOptions",
                    "description": "Optional on-device (a.k.a. edge) searcher parameters. If it is not\nspecified, then cloud_searcher_options must be set, otherwise a runtime\nerror is returned.\n\nThis is where the path to the database and other parameters to the search\nare specified."
                },
                "embedderOptions": {
                    "$ref": "VisionkitEmbedderOptions",
                    "description": "Mandatory on-device embedder parameters.\n\nA runtime error is returned if this is not specified."
                }
            },
            "type": "object"
        },
        "VisionkitSearcherDatabaseOptions": {
            "description": "Contains the options to customize the type of database on which the search is\nperformed.",
            "id": "VisionkitSearcherDatabaseOptions",
            "properties": {
                "databaseType": {
                    "enum": [
                        "UNDEFINED",
                        "LEVELDB_UNCOMPRESSED",
                        "LEVELDB_HASHED"
                    ],
                    "enumDescriptions": [
                        "",
                        "Database containing floating point embeddings.",
                        "Product quantized database. The embeddings are hashed in chunks where\neach chunk is encoded by a byte (uint8) representing the index of the\nclosest codebook centroid. This is useful to minimize the database\nfootprint and speed up querying."
                    ],
                    "type": "string"
                },
                "leveldbFilename": {
                    "description": "Absolute path to the file containing the LevelDB.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSearcherOptions": {
            "description": "Contains the options for the VisionKit Searcher. It embeds all parameters\nrelated to the components of the Searcher (leaf searcher, database,\npartitioner ...).",
            "id": "VisionkitSearcherOptions",
            "properties": {
                "databaseOptions": {
                    "$ref": "VisionkitSearcherDatabaseOptions"
                },
                "knnOptions": {
                    "$ref": "VisionkitKnnOptions"
                },
                "leafSearcherOptions": {
                    "$ref": "VisionkitLeafSearcherOptions"
                },
                "partitionerOptions": {
                    "$ref": "VisionkitPartitionerOptions"
                }
            },
            "type": "object"
        },
        "VisionkitSearcherParameters": {
            "description": "Convenience message grouping the parameters needed to perform embedding\nextraction in Servo followed by k-NN search in a ScaM index.",
            "id": "VisionkitSearcherParameters",
            "properties": {
                "scamIndexParameters": {
                    "$ref": "VisionkitScamIndexParameters",
                    "description": "The parameters needed to perform a k-NN search in a ScaM index."
                },
                "servoEmbedderParameters": {
                    "$ref": "VisionkitServoEmbedderParameters",
                    "description": "The parameters needed to perform embedding extraction through Servo."
                }
            },
            "type": "object"
        },
        "VisionkitSegmenterOptions": {
            "description": "Segmenter options.\nNext Id: 5",
            "id": "VisionkitSegmenterOptions",
            "properties": {
                "computeSettings": {
                    "$ref": "AccelerationAcceleration",
                    "description": "How to accelerate the model (e.g., via NNAPI)."
                },
                "externalFiles": {
                    "$ref": "VisionkitSegmenterOptionsExternalFiles"
                },
                "outputType": {
                    "description": "Optional output mask type.",
                    "enum": [
                        "UNSPECIFIED",
                        "CATEGORY_MASK",
                        "CONFIDENCE_MASK"
                    ],
                    "enumDescriptions": [
                        "",
                        "Gives a single output mask where each pixel represents the class which\nthe pixel in the original image was predicted to belong to.",
                        "Gives a list of output masks where, for each mask, each pixel represents\nthe prediction confidence, usually in the [0, 1] range."
                    ],
                    "type": "string"
                },
                "segmenterName": {
                    "description": "The class name corresponding to a registered segmenter, e.g. \"FooNet\"\nfor a segmenter registered via `REGISTER_SEGMENTER(FooNet);`.\nThis is also known as bundled models since the model files are embedded in\nthe final binary. See external_files for an alternative. Mutually exclusive\nwith external_files.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSegmenterOptionsExternalFiles": {
            "description": "The external model and label map files used to create the segmenter (both\nare required to be populated when using external files).\n\nThis is an alternative to registered models. Mutually exclusive with\nsegmenter_name.",
            "id": "VisionkitSegmenterOptionsExternalFiles",
            "properties": {
                "labelMapFile": {
                    "$ref": "VisionkitExternalFile"
                },
                "modelFile": {
                    "$ref": "VisionkitExternalFile"
                }
            },
            "type": "object"
        },
        "VisionkitServerConfiguration": {
            "description": "Defines how the server handles incoming AnnotateImageRequest-s.",
            "id": "VisionkitServerConfiguration",
            "properties": {
                "cloudAnnotator": {
                    "$ref": "VisionkitCloudAnnotator",
                    "description": "A standalone cloud annotator used when no on-device annotations are\nprovided in the request, or if none of those triggered a hybrid annotator."
                },
                "hybridAnnotator": {
                    "description": "A list of HybridAnnotators to handle requests that already provide some\nannotations inferred on-device.",
                    "items": {
                        "$ref": "VisionkitHybridAnnotator"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitServoClassifierParameters": {
            "description": "The parameters needed to perform classification through Servo. Note that the\nnumber of returned results is configured when the model is deployed on Servo,\nand cannot be overridden here.",
            "id": "VisionkitServoClassifierParameters",
            "properties": {
                "labelBlacklist": {
                    "description": "Optional blacklist of labels. If non-empty, results whose label is in this\nset will be filtered out. Duplicate or unknown labels are silently ignored.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to requests.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                },
                "scoreThreshold": {
                    "description": "A score threshold below which classifications are rejected. Optional.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitServoDetectorParameters": {
            "description": "The parameters needed to perform detection through Servo. Returns the single\nmost confident detection fitting the criterions defined below, if any.",
            "id": "VisionkitServoDetectorParameters",
            "properties": {
                "classThresholds": {
                    "$ref": "VisionkitClassThresholds",
                    "description": "Per-class thresholds below which detections are rejected. Optional."
                },
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to requests.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                },
                "objectCentric": {
                    "description": "Whether or not to filter-out non object-centric detections. If true,\nbounding boxes that don't contain the center of the frame are filtered-out.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitServoEmbedderParameters": {
            "description": "The parameters needed to perform embedding extraction through Servo.",
            "id": "VisionkitServoEmbedderParameters",
            "properties": {
                "featureType": {
                    "description": "The type of features to output. This is used at query time to convert the\nembedding provided by Servo to the format expected in the ScaM dataset.",
                    "enum": [
                        "FEATURE_TYPE_UNSPECIFIED",
                        "INT64",
                        "FLOAT"
                    ],
                    "enumDescriptions": [
                        "Unspecified feature type.",
                        "Integer features. This assumes the features have been scalar quantized\ninto bytes (int8) using a function like FixedByteQuantizedCompress [1].\n\n[1]:\ngoogle3/photos/vision/facenet/face_template_utils.h?q=FixedByteQuantizedCompress",
                        "Floating-point features."
                    ],
                    "type": "string"
                },
                "modelSpec": {
                    "$ref": "TensorflowServingModelSpec",
                    "description": "The ModelSpec [1] of the Servo model to requests.\n\n[1]: google3/third_party/tensorflow_serving/apis/model.proto."
                },
                "scalarQuantizationFactor": {
                    "description": "If feature_type is set to INT64: the quantization factor used to convert\nfrom float to scalar-quantized integer embedding. Must be in [1, 128]: the\nrecommended default value used at indexing time [1] is 128, which quantizes\nvalues in [-128, 127].\n\n[1]:\ngoogle3/photos/vision/visionkit/indexing/build/utils/scam_utils.cc?q=IweToQuantizedGfv",
                    "format": "uint32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitSsdQuadDetectionOptions": {
            "description": "Options for the MobileSSD quad detectors.",
            "id": "VisionkitSsdQuadDetectionOptions",
            "properties": {
                "minQuadDetectionConfidence": {
                    "description": "Minimum confidence for Quad detection.\nPhotos Quad detection returns a confidence in [0..18].",
                    "format": "float",
                    "type": "number"
                },
                "mobileSsdOptions": {
                    "$ref": "MobileSsdClientOptions",
                    "description": "MobileSSD options."
                }
            },
            "type": "object"
        },
        "VisionkitSubgraph": {
            "description": "Subgraph that can be injected to Drishti DAG\nNext ID: 5",
            "id": "VisionkitSubgraph",
            "properties": {
                "inputSidePacketName": {
                    "description": "List of subgraph input side packet names.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "inputStreamName": {
                    "description": "List of subgraph input stream names.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "ocrOptions": {
                    "$ref": "VisionkitOcrOptions",
                    "description": "If ocr_options is set, OcrLifeCycleCalculator initializes the PhotoOCR\nlibrary using these settings. This is useful if the subgraph contains a\ncalculator that needs OCR, like PhotoOcrCalculator or ScreenOcrCalculator.\nThese ocr_options are mutually exclusive with the ocr_options specified in\nthe PipelineConfig. They cannot be both set at the same time."
                },
                "subgraphName": {
                    "description": "Name of custom subgraph.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSubpipelineOptions": {
            "description": "This proto is designed for use cases when a single VKP instance is shared by\nmultiple subpipelines. During the initialization of VKP instance, the\nSubpipelineManager needs to be configured by predefining an id and a set of\nnecessary engines for the subpipeline. Then each subpipeline is able to\nenable/disable its corresponding engines at the runtime.",
            "id": "VisionkitSubpipelineOptions",
            "properties": {
                "subpipelineConfigs": {
                    "description": "For each subpipeline, config with an id and define a set of required\nengines.",
                    "items": {
                        "$ref": "VisionkitSubpipelineOptionsSubpipelineConfig"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitSubpipelineOptionsSubpipelineConfig": {
            "id": "VisionkitSubpipelineOptionsSubpipelineConfig",
            "properties": {
                "engineNames": {
                    "description": "The engines in this subpipeline. SubpipelineOptions and DutyCycleOptions\nuse the same naming conventions for engine names.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                "id": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSuperpack": {
            "description": "Defines a superpack packaging all dependencies required for an overlay. This\nallows the VisionKit Server to act as configuration system for Superpacks.\ng3doc/java/com/google/android/libraries/micore/superpacks/g3doc/manual/getting_started#registering-manifests",
            "id": "VisionkitSuperpack",
            "properties": {
                "integerVersion": {
                    "description": "The version of the superpack. Must be > 0 or sanity checks will fail. It\nuniquely identifies a superpack version, so only equality matters, not any\nordering.\ng3doc/java/com/google/android/libraries/micore/superpacks/g3doc/manual/getting_started#manifest-versioning",
                    "format": "int32",
                    "type": "integer"
                },
                "manifestUrl": {
                    "description": "The manifest URL for the superpack. If you change this field and the\nmanifest contents changed, make sure to change the version field alongside,\nas the new URL would be ignored by Superpacks. On the other end, if you\nchange this field but the manifest contents stayed the same, don't update\nthe version field, as Superpacks would re-download all the packs.",
                    "type": "string"
                },
                "name": {
                    "description": "The name of the superpack. Usually, it's the same as the name of the\noverlay using it, but if reused across different overlays, it can be named\naccordingly.\nFor example, two overlays using the same models, labelmaps, indexes, etc.)\nbut different configurations (for example score thresholds, class name\nwhite/blacklists, etc.).",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitSymbolAlignerConfig": {
            "id": "VisionkitSymbolAlignerConfig",
            "properties": {
                "gapCost": {
                    "description": "The cost values for string alignment.",
                    "format": "int32",
                    "type": "integer"
                },
                "matchCost": {
                    "format": "int32",
                    "type": "integer"
                },
                "maxPixelDistanceForMatch": {
                    "description": "We will do matches tokes that are more than this apart.",
                    "format": "float",
                    "type": "number"
                },
                "mismatchCost": {
                    "format": "int32",
                    "type": "integer"
                },
                "weightedMatch": {
                    "description": "If true, we will use symbol confidence as the cost of the match.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitSymbolMergerConfig": {
            "id": "VisionkitSymbolMergerConfig",
            "properties": {
                "alignerConfig": {
                    "$ref": "VisionkitSymbolAlignerConfig"
                },
                "matchesConfidenceThreshold": {
                    "description": "If we have more than this many matches, we consider it a good span.",
                    "format": "float",
                    "type": "number"
                },
                "maxWidthFractionDeltaForRepetition": {
                    "description": "If the delta / min of the width of repeated characters is above this, we\nskip the repeated characters.",
                    "format": "float",
                    "type": "number"
                },
                "minFirstSymbolConfidence": {
                    "description": "If the symbols at the beginning / ending have less than this confidence we\nskip them.",
                    "format": "float",
                    "type": "number"
                },
                "minGoodSymbolConfidence": {
                    "description": "The minimum confidence value for a good symbol.",
                    "format": "float",
                    "type": "number"
                },
                "minGoodWordConfidence": {
                    "description": "The minimum confidence value for a good word.",
                    "format": "float",
                    "type": "number"
                },
                "minMatchFractionForMerge": {
                    "description": "This fraction of symbols in the input strings must match to merge.",
                    "format": "float",
                    "type": "number"
                },
                "minOverlapFractionForMerge": {
                    "description": "If two lines overlap for more than this number, we will merge no matter\nwhat.",
                    "format": "float",
                    "type": "number"
                },
                "minProjectionLineLength": {
                    "description": "The minimum length (in characters) of a line that is required for geomertry\nprojection. Lines with fewer characters than this tend to have inaccurate\nbounding boxes. When we project active lines onto such small lines, we\nget more inaccurate bounding boxes.",
                    "format": "int32",
                    "type": "integer"
                },
                "minSingleCharacterInsertionThreshold": {
                    "description": "The symbol confidence value for a single character insetion must be above\nthis value to be accepted.",
                    "format": "float",
                    "type": "number"
                },
                "similarConfidenceThreshold": {
                    "description": "Confidence values within this amount are considered equivalent.",
                    "format": "float",
                    "type": "number"
                },
                "symbolConfidenceSeparationThreshold": {
                    "description": "If symbol confidences are more different than this, we use the higher\nconfidence symbols.",
                    "format": "float",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitSynchronousApiOptions": {
            "id": "VisionkitSynchronousApiOptions",
            "properties": {
                "blockingMode": {
                    "description": "Specifies what the pipeline should block on before returning results.",
                    "enum": [
                        "NONE",
                        "BLOCK_ON_ALL"
                    ],
                    "enumDescriptions": [
                        "",
                        "Blocks on all engines."
                    ],
                    "type": "string"
                },
                "cacheResults": {
                    "description": "Cache previously seen results.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitTextObjectManagerOptions": {
            "id": "VisionkitTextObjectManagerOptions",
            "properties": {
                "maxEditDistanceRatio": {
                    "description": "Allowable max edit distance ratio. If the edit distance is larger than\nthis, the strings are not considered as potential correspondences.",
                    "format": "float",
                    "type": "number"
                },
                "maxStringSize": {
                    "description": "Max string size used for edit distance comparison.\nthis determines the space allocation for an internal buffer.\nMore detail is available at //strings/editdistance.h",
                    "format": "int32",
                    "type": "integer"
                },
                "minOverlapFraction": {
                    "description": "Minimum overlapping fraction of two OCR boxes to be consider as a potential\nmatching candidate. If it's set to 0, every box will be accepted.",
                    "format": "float",
                    "type": "number"
                },
                "minTextLength": {
                    "description": "If the line contains characters less than this, the line is not added to\nthe tracking list.",
                    "format": "int32",
                    "type": "integer"
                },
                "searchExtensionRatio": {
                    "description": "This is used to extend the bounding box to seach for overlap with other\nboxes. That is, the box will be extended by this fraction of their\nwidth/height to find potential correspondences.",
                    "format": "float",
                    "type": "number"
                },
                "unseenTimeoutUs": {
                    "description": "Delete the box if the box hasn't been detected for this amount of time.",
                    "format": "int32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitTextOrientationTrackerOptions": {
            "description": "Next ID: 9",
            "id": "VisionkitTextOrientationTrackerOptions",
            "properties": {
                "disambiguationRecheckMaxDutyCycle": {
                    "description": "When the coarse orientation confidence is above\norientation_classifier_hysteresis_threshold, if the max coarse text\norientation is in the previous or opposite direction, generally OCR does\nnot need to be re-run as the prior result can be used. However, it is run\nperiodically to ensure a wrong decision is not kept indefinitely. It is\nonly rechecked if the time to do the last disambiguation divided by the\ntime since that last disambiguation (ie. the fraction of time or duty cycle\ndoing the disambiguation) is equal to or less than\ndisambiguation_recheck_max_duty_cycle.",
                    "format": "float",
                    "type": "number"
                },
                "highResConfidenceThreshold": {
                    "format": "float",
                    "type": "number"
                },
                "imuOnly": {
                    "description": "Use IMU only.",
                    "type": "boolean"
                },
                "minOrientationConfidence": {
                    "description": "If the highest text orientation confidence is equal or less than this,\ndeem the orientation unknown.",
                    "format": "float",
                    "type": "number"
                },
                "noTextConfidenceThreshold": {
                    "description": "If the confidence of \"no-text\" is higher than this, consider there is no\ntext.",
                    "format": "float",
                    "type": "number"
                },
                "orientationClassifierHysteresisThreshold": {
                    "description": "If the text orientation confidence of the current orientation is above this\nvalue, that orientation is retained.",
                    "format": "float",
                    "type": "number"
                },
                "resetPreviousValues": {
                    "description": "Reset the previously estimated values.",
                    "type": "boolean"
                },
                "useDeviceOrientationOnly": {
                    "description": "When true, image based orientation tracking is disabled and it returns\nthe orientation only based on the device orientation.",
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "VisionkitV1AnnotateImageRequest": {
            "description": "Defines the request format expected by the AnnotateImage endpoint.\nNext ID: 10.",
            "id": "VisionkitV1AnnotateImageRequest",
            "properties": {
                "category": {
                    "description": "DEPRECATED, please use `object_detection` instead.",
                    "type": "string"
                },
                "embedding": {
                    "$ref": "VisionkitV1Embedding",
                    "description": "A query embedding pre-computed on the client-side. Mutually exclusive with\n`image_bytes` and `image_url`: exactly one of these three fields must be\nprovided.\n\nThis only applies to overlays using a `Searcher` cloud annotator. The\ncaller is responsible for providing features of the type [1] and size\n[2] specified in this cloud annotator; otherwise an error will be\nreturned at query time.\n"
                },
                "filterQueries": {
                    "description": "Optional. The filters that can be applied to restrict the set of search\nresults returned from an AnnotateImageRequest, typically based on\nfine-grained attributes, e.g. to return only items of a given color among\nan apparel dataset:\n\n- Values associated with different keys are combined with a logical AND.\nE.g.:\n\n  filter_queries { key: \"color\" values: \"red\" }\n  filter_queries { key: \"category\" values: \"shoe\" }\n\nWill match all items that are (\"red\" && \"shoe\").\n\n- Values that belong to the same key are combined with a logical OR.\nE.g.:\n\n  filter_queries { key: \"color\" values: \"red\" values: \"blue\" }\n\nWill match all items that are (\"red\" || \"blue\"). Please note that this is\nnot valid to specify a key more than once. A runtime error is returned when\nthis happens.\nE.g.:\n\n  # Invalid\n  filter_queries { key: \"category\" values: \"shoe\" }\n  filter_queries { key: \"category\" values: \"sneaker\" }\n\n  # Valid\n  filter_queries { key: \"category\" values: \"shoe\" values: \"sneaker\" }\n\nNOTE: Filter queries are not supported by all overlays: see `FilterQuery`\ndocumentation below.",
                    "items": {
                        "$ref": "VisionkitV1FilterQuery"
                    },
                    "type": "array"
                },
                "imageBytes": {
                    "description": "The query image bytes, as a JPEG blob. Mutually exclusive with `image_url`\nand `embedding`: exactly one of these three fields must be provided.",
                    "format": "byte",
                    "type": "string"
                },
                "imageUrl": {
                    "description": "The query image URL, which is expected to be publicly accessible. Mutually\nexclusive with `image_bytes` and `embedding`: exactly one of these three\nfields must be provided.",
                    "type": "string"
                },
                "locationContext": {
                    "$ref": "VisionkitV1LocationContext",
                    "description": "Optional. The location context of the query, used to restrict the set of\nresults returned from an AnnotateImageRequest based on geolocation.\n\nThis is not supported by all overlays: see `LocationContext` documentation\nbelow."
                },
                "objectDetection": {
                    "$ref": "VisionkitV1ObjectDetection",
                    "description": "Optional object detection pre-computed on the client-side (only the most\nconfident class is considered). Providing this will cause the configured\ndetector (if any) to be skipped. This has no effect if an embedding is\nprovided as input instead of image bytes. An Invalid Argument Error will be\nreturned if the underlying bounding box is invalid."
                },
                "overlayName": {
                    "description": "Mandatory. The name of the overlay (see go/visionkit-server#overlays) to\nuse. This must correspond to one of the `overlay.name` values returned by\nthe ListOverlays() endpoint and defined in [1]. Otherwise, an\nInvalidArgument error is thrown.\n\n[1]: google3/photos/vision/visionkit/server/overlays",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1AnnotateImageResponse": {
            "description": "Defines the response format returned by the AnnotateImage endpoint.\nNext ID: 5.",
            "id": "VisionkitV1AnnotateImageResponse",
            "properties": {
                "annotations": {
                    "description": "DEPRECATED, use `annotation_list` below instead.\nThe list of results ranked by decreasing score. May be empty if no results\nwere found, e.g. if all results were filtered out by a confidence threshold\nor if metadata could not be retrieved for any result.",
                    "items": {
                        "$ref": "VisionkitV1Annotation"
                    },
                    "type": "array"
                },
                "humanAnnotationList": {
                    "$ref": "VisionkitV1HumanAnnotationList",
                    "description": "Human annotations returned by a human sensing cascade."
                },
                "objectAnnotationList": {
                    "$ref": "VisionkitV1ObjectAnnotationList",
                    "description": "Object annotations returned by an object recognition cascade."
                },
                "textAnnotationList": {
                    "$ref": "VisionkitV1TextAnnotationList",
                    "description": "Text annotations returned by a text recognition cascade."
                }
            },
            "type": "object"
        },
        "VisionkitV1Annotation": {
            "description": "Defines an annotation result. All fields are optional.\nNext ID: 9.",
            "id": "VisionkitV1Annotation",
            "properties": {
                "boundingBox": {
                    "$ref": "VisionkitRectF",
                    "description": "The region of the input image where this result originates from.\nCoordinates are normalized in [0,1]."
                },
                "displayName": {
                    "description": "An English name for this result, ready for display to the end user.",
                    "type": "string"
                },
                "geolocationDistance": {
                    "description": "The geolocation distance in kilometers between this result (e.g.\nrestaurant, storefront, etc) and the coordinates specified in the request\nlocation context (where the user is located).",
                    "format": "double",
                    "type": "number"
                },
                "link": {
                    "description": "A web page URL where the user can get more information about this result.",
                    "type": "string"
                },
                "score": {
                    "description": "The confidence score in [0, 1] of this result.",
                    "format": "float",
                    "type": "number"
                },
                "source": {
                    "description": "The source this result originates from, ready for display to the end user\nfor attribution purposes. This is typically a merchant name (e.g. \"Nike\"),\na website (e.g. \"Wikipedia\"), or a citizen science app name and individual\ncontributor username (e.g. \"iNaturalist (username)\").",
                    "type": "string"
                },
                "thumbnailContent": {
                    "description": "Same as above, except the thumbnail is provided inline e.g. as JPEG bytes.",
                    "format": "byte",
                    "type": "string"
                },
                "thumbnailUrl": {
                    "description": "The URL of a thumbnail for this result, to display to the end user for\nillustration purposes.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1Attribute": {
            "description": "Defines a single attribute result.",
            "id": "VisionkitV1Attribute",
            "properties": {
                "classes": {
                    "description": "Mandatory list of labels (e.g. \"blue\") with their scores for this\nattribute.",
                    "items": {
                        "$ref": "VisionkitV1Class"
                    },
                    "type": "array"
                },
                "name": {
                    "description": "Mandatory name for this attribute. E.g. \"color\" or \"material\".",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1AttributeList": {
            "description": "Defines attributes results.",
            "id": "VisionkitV1AttributeList",
            "properties": {
                "attributes": {
                    "description": "The list of attributes, ranked by descending score.",
                    "items": {
                        "$ref": "VisionkitV1Attribute"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1Class": {
            "description": "Defines a single class result.",
            "id": "VisionkitV1Class",
            "properties": {
                "displayName": {
                    "description": "Optional display name for this class.",
                    "type": "string"
                },
                "label": {
                    "description": "Mandatory label for this class, e.g. MID.",
                    "type": "string"
                },
                "score": {
                    "description": "Mandatory probability score for this class.",
                    "format": "double",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitV1Classification": {
            "description": "Defines a single classification.",
            "id": "VisionkitV1Classification",
            "properties": {
                "class": {
                    "$ref": "VisionkitV1Class",
                    "description": "Mandatory class result for this classification."
                },
                "objectMetadata": {
                    "$ref": "VisionkitV1ObjectMetadata",
                    "description": "Optional object metadata describing the object represented by this class\nresult."
                }
            },
            "type": "object"
        },
        "VisionkitV1ClassificationHead": {
            "description": "Defines a single classification head.",
            "id": "VisionkitV1ClassificationHead",
            "properties": {
                "classifications": {
                    "description": "The list of classifications, ranked by descending score.",
                    "items": {
                        "$ref": "VisionkitV1Classification"
                    },
                    "type": "array"
                },
                "name": {
                    "description": "The name of the classification head which indicates what this set of\nclasses represent (e.g. \"colors\", \"cars\", ...). This is primarily meant for\nUI purpose. Can be empty when there's only one output head.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1ClassificationList": {
            "description": "Defines classification results.",
            "id": "VisionkitV1ClassificationList",
            "properties": {
                "classificationHeads": {
                    "description": "The list of classifications, grouped by head.",
                    "items": {
                        "$ref": "VisionkitV1ClassificationHead"
                    },
                    "type": "array"
                },
                "classifications": {
                    "description": "DEPRECATED - use classification_heads below.",
                    "items": {
                        "$ref": "VisionkitV1Classification"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1ClientConfiguration": {
            "description": "Defines the client-side configuration associated to a given server-side\nconfiguration.",
            "id": "VisionkitV1ClientConfiguration",
            "properties": {
                "pipelineConfig": {
                    "$ref": "VisionkitPipelineConfig",
                    "description": "The PipelineConfig to use, defining which on-device models should be used\nas well as the parameters to configure them."
                }
            },
            "type": "object"
        },
        "VisionkitV1ColoredLabel": {
            "description": "Defines a label associated with an RGB color.",
            "id": "VisionkitV1ColoredLabel",
            "properties": {
                "b": {
                    "format": "uint32",
                    "type": "integer"
                },
                "g": {
                    "format": "uint32",
                    "type": "integer"
                },
                "label": {
                    "description": "Optional label for the class.",
                    "type": "string"
                },
                "r": {
                    "description": "RGB color components for the label. It should be in range [0, 255].",
                    "format": "uint32",
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "VisionkitV1CreateOverlayRequest": {
            "description": "Defines the request format expected by CreateOverlay.",
            "id": "VisionkitV1CreateOverlayRequest",
            "properties": {
                "overlayConfiguration": {
                    "$ref": "VisionkitOverlayConfiguration",
                    "description": "The overlay configuration to be created. Its `name` field must be unique.\nIf the `name` field matches an already existing config, or if the config is\ninvalid, an InvalidArgument error is returned."
                },
                "owners": {
                    "description": "The list of owners LDAPs for the overlay. Those will be able to make\nchanges to this overlay once created.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1CreateOverlayResponse": {
            "description": "Defines the response format returned by CreateOverlay.",
            "id": "VisionkitV1CreateOverlayResponse",
            "properties": {},
            "type": "object"
        },
        "VisionkitV1Embedding": {
            "description": "Defines an embedding, either float or quantized.",
            "id": "VisionkitV1Embedding",
            "properties": {
                "floatEmbedding": {
                    "$ref": "VisionkitV1FloatEmbedding"
                },
                "int64Embedding": {
                    "$ref": "VisionkitV1Int64Embedding"
                }
            },
            "type": "object"
        },
        "VisionkitV1FilterQuery": {
            "description": "Defines filters that can be applied to restrict the set of search results\nreturned from a similar image search.\n\nThis only applies to overlays performing similar image search in a ScaM\ndataset specifying such attributes as V3Restrict tokens [1].\n\nCurrent limitations:\n- the key must be one of the `supported_keys` listed in the overlay\n  configurations [2] for the provided category and overlay name, e.g. the\n  \"bag\" category may only support using \"color\" and \"brand\" keys.\n- the only supported comparison operator is string equality.\n  Example: key: \"color\", values: \"red\" will reject all results with a \"color\"\n  attribute != \"red\".\n\n[1]: google3/research/scam/data_format/features.proto?q=V3Restrict\n[2]: google3/photos/vision/visionkit/server/overlays",
            "id": "VisionkitV1FilterQuery",
            "properties": {
                "key": {
                    "description": "Mandatory. The key to filter by, e.g. key: \"color\".",
                    "type": "string"
                },
                "values": {
                    "description": "Mandatory. The corresponding value(s), e.g. values: \"red\". It multiple\nvalues are set, they are combined with a logical OR. E.g.:\n\n values: \"red\"\n values: \"blue\"\n\nWill match all items that are (\"red\" || \"blue\").",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1FloatEmbedding": {
            "description": "Defines a float embedding.",
            "id": "VisionkitV1FloatEmbedding",
            "properties": {
                "values": {
                    "description": "The float embedding values.",
                    "items": {
                        "format": "float",
                        "type": "number"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1GetOverlayRequest": {
            "description": "Defines the request format expected by GetOverlay.",
            "id": "VisionkitV1GetOverlayRequest",
            "properties": {
                "overlayName": {
                    "description": "The name of the requested overlay.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1GetOverlayResponse": {
            "description": "Defines the response format returned by GetOverlay.",
            "id": "VisionkitV1GetOverlayResponse",
            "properties": {
                "overlay": {
                    "$ref": "VisionkitV1Overlay",
                    "description": "The overlay."
                }
            },
            "type": "object"
        },
        "VisionkitV1HumanAnnotationList": {
            "description": "Defines a list of annotations returned by human sensing cascades.",
            "id": "VisionkitV1HumanAnnotationList",
            "properties": {},
            "type": "object"
        },
        "VisionkitV1InspectOverlayRequest": {
            "description": "Defines the request format expected by InspectOverlay.",
            "id": "VisionkitV1InspectOverlayRequest",
            "properties": {
                "overlayName": {
                    "description": "Mandatory. The name of the overlay[1] to inspect. This must correspond to\none of the `overlay.name` values returned by the ListOverlays endpoint[2].\n\n[1]: go/visionkit-server#overlays and\n[2]: google3/google/internal/visionkit/v1/list_overlays.proto",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1InspectOverlayResponse": {
            "description": "Defines the response format returned by InspectOverlay.",
            "id": "VisionkitV1InspectOverlayResponse",
            "properties": {
                "configuration": {
                    "description": "Human-readable representation of the overlay configuration, as defined on\nthe server-side (go/visionkit-server/configure-your-overlay).\n\nModulo some fields that are explicitly cleared (e.g. display_name), this\nexactly corresponds to the return value of DebugString() applied to the\noverlay configuration proto that corresponds to the overlay specified in\nthe InspectOverlayRequest.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1Int64Embedding": {
            "description": "Defines an integer (quantized) embedding.",
            "id": "VisionkitV1Int64Embedding",
            "properties": {
                "values": {
                    "description": "The integer embedding values.",
                    "items": {
                        "format": "int64",
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1ListOverlaysRequest": {
            "description": "Defines the request format expected by ListOverlays. Empty for now.",
            "id": "VisionkitV1ListOverlaysRequest",
            "properties": {},
            "type": "object"
        },
        "VisionkitV1ListOverlaysResponse": {
            "description": "Defines the response format returned by ListOverlays.",
            "id": "VisionkitV1ListOverlaysResponse",
            "properties": {
                "overlays": {
                    "$ref": "VisionkitV1OverlayList",
                    "description": "The list of available overlays."
                }
            },
            "type": "object"
        },
        "VisionkitV1LocationContext": {
            "description": "Defines the location context of a query.\n\nThis only applies to overlays performing similar image search in a ScaM\ndataset specifying S2 Cell IDs as V3Restrict tokens [1].\n\n[1]: google3/research/scam/data_format/features.proto?q=V3Restrict",
            "id": "VisionkitV1LocationContext",
            "properties": {
                "latLng": {
                    "$ref": "GoogleTypeLatLng",
                    "description": "Mandatory. The latitude and longitude where the query originates from."
                }
            },
            "type": "object"
        },
        "VisionkitV1MatchedImage": {
            "description": "Defines a single matched (i.e. near-duplicate) image.",
            "id": "VisionkitV1MatchedImage",
            "properties": {
                "objectMetadata": {
                    "$ref": "VisionkitV1ObjectMetadata",
                    "description": "Mandatory object metadata between the query and this image."
                },
                "score": {
                    "description": "Mandatory confidence score between the query and this image in [0, 1].",
                    "format": "double",
                    "type": "number"
                }
            },
            "type": "object"
        },
        "VisionkitV1MatchedImageList": {
            "description": "Defines a list of matched (i.e. near-duplicate) images.",
            "id": "VisionkitV1MatchedImageList",
            "properties": {
                "matchedImages": {
                    "description": "The list of matched images, ranked by descending score.",
                    "items": {
                        "$ref": "VisionkitV1MatchedImage"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1ObjectAnnotation": {
            "description": "Defines an object annotation.",
            "id": "VisionkitV1ObjectAnnotation",
            "properties": {
                "attributeList": {
                    "$ref": "VisionkitV1AttributeList",
                    "description": "Optional attributes for the query image. Those are returned when attribute\nextraction is configured in the cascade."
                },
                "classificationList": {
                    "$ref": "VisionkitV1ClassificationList",
                    "description": "Optional classifications for the query image. Those are returned when a\nclassifier is configured in the cascade."
                },
                "detection": {
                    "$ref": "VisionkitV1ObjectDetection",
                    "description": "Optional detection result defining the region of the input image where this\nannotation originates from. If omitted, this annotation applies to\nthe entire query image."
                },
                "matchedImageList": {
                    "$ref": "VisionkitV1MatchedImageList",
                    "description": "Optional matched (i.e. near-duplicate) images to the query image. Those are\nreturned when a matcher is configured in the cascade."
                },
                "segmentationList": {
                    "$ref": "VisionkitV1SegmentationList",
                    "description": "Optional segmentations for the query image. Those are returned when a\nsegmenter is configured in the cascade."
                },
                "similarImageList": {
                    "$ref": "VisionkitV1SimilarImageList",
                    "description": "Optional similar images to the query image. Those are returned when a\nsearcher is configured in the cascade."
                }
            },
            "type": "object"
        },
        "VisionkitV1ObjectAnnotationList": {
            "description": "Defines a list of annotations returned by object recognition cascades.",
            "id": "VisionkitV1ObjectAnnotationList",
            "properties": {
                "annotations": {
                    "description": "The list of annotations, ranked by descending detection score. If no\ndetection was performed, this list will contain a single element with no\nObjectDetection field. May be empty if detection was configured to be\nmandatory but no confident detection results could be found.",
                    "items": {
                        "$ref": "VisionkitV1ObjectAnnotation"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1ObjectDetection": {
            "description": "Defines an object detection.",
            "id": "VisionkitV1ObjectDetection",
            "properties": {
                "boundingBox": {
                    "$ref": "VisionkitRectF",
                    "description": "Mandatory bounding box around the detected object. Coordinates are\nnormalized in [0, 1]."
                },
                "classes": {
                    "description": "Optional class results of the detected object.",
                    "items": {
                        "$ref": "VisionkitV1Class"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1ObjectMetadata": {
            "description": "Defines an object metadata.",
            "id": "VisionkitV1ObjectMetadata",
            "properties": {
                "displayName": {
                    "description": "An English name for this object, ready for display to the end user.",
                    "type": "string"
                },
                "link": {
                    "description": "A web page URL where the user can get more information about this object.",
                    "type": "string"
                },
                "source": {
                    "description": "The source this object originates from, ready for display to the end user\nfor attribution purposes. This is typically a merchant name (e.g. \"Nike\"),\na website (e.g. \"Wikipedia\"), or a citizen science app name and individual\ncontributor username (e.g. \"iNaturalist (username)\").",
                    "type": "string"
                },
                "thumbnailContent": {
                    "description": "Same as above, except the thumbnail is inlined (e.g. as JPEG bytes).",
                    "format": "byte",
                    "type": "string"
                },
                "thumbnailUrl": {
                    "description": "The thumbnail URL for this object, to display to the end user for\nillustration purposes.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1Overlay": {
            "description": "Defines an overlay. This allows customizing the client behavior to a\nspecific end-to-end visual search experience.\nNext ID: 17",
            "id": "VisionkitV1Overlay",
            "properties": {
                "base64Icon": {
                    "description": "A base64-encoded icon for this overlay. The string only contains the raw\nbase64-encoded icon (i.e. without \"data:image/png;base64\" prefix). The\nimage is either in JPEG or PNG format.",
                    "type": "string"
                },
                "clientConfiguration": {
                    "$ref": "VisionkitV1ClientConfiguration",
                    "description": "The client-side configuration to use with this Overlay."
                },
                "cloudObjectRecognitionCascade": {
                    "$ref": "VisionkitCloudObjectRecognitionCascade"
                },
                "description": {
                    "description": "A description for this overlay, written in English by default. This is\nuseful for clients that support multiple overlays and need to list those in\na UI.",
                    "type": "string"
                },
                "displayName": {
                    "description": "A display name for this overlay, written in English by default. This is\nuseful for clients that support multiple overlays and need to list those in\na UI.",
                    "type": "string"
                },
                "edgeObjectRecognitionCascade": {
                    "$ref": "VisionkitEdgeObjectRecognitionCascade",
                    "description": "Edge, hybrid or cloud cascades dedicated to object recognition.\nTypical use cases: products, natural world or landmark recognition."
                },
                "hybridObjectRecognitionCascade": {
                    "$ref": "VisionkitHybridObjectRecognitionCascade"
                },
                "internal": {
                    "description": "Whether this overlay is reserved for internal use or not. This is useful\nfor internal clients that support multiple overlays and need to list those\nin a UI.",
                    "type": "boolean"
                },
                "modelType": {
                    "description": "Types of models on which this overlay is relying.",
                    "enumDescriptions": [
                        "Unknown model type.",
                        "Classifier model type.",
                        "Detector model type.",
                        "Embedder model type.",
                        "Matcher model type.",
                        "Segmenter model type."
                    ],
                    "items": {
                        "enum": [
                            "MODEL_TYPE_UNKNOWN",
                            "CLASSIFIER",
                            "DETECTOR",
                            "EMBEDDER",
                            "MATCHER",
                            "SEGMENTER"
                        ],
                        "type": "string"
                    },
                    "type": "array"
                },
                "name": {
                    "description": "A unique identifier for this overlay, to pass at query time to the\nAnnotateImage() endpoint in order to use the server-side configuration\ncorresponding to this end-to-end visual search experience.",
                    "type": "string"
                },
                "requiresLocationContext": {
                    "description": "Whether this overlay requires access to the user location. The user\nlocation must then be passed to the AnnotateImage() endpoint for such\noverlays.",
                    "type": "boolean"
                },
                "superpack": {
                    "$ref": "VisionkitSuperpack",
                    "description": "An optional superpack configuration for dynamic model downloading.\n\nDEPRECATED: each edge annotator now specifies its own `Superpack`. See [1].\n\n[1]:\ngoogle3/photos/vision/visionkit/server/boq/proto/edge_annotators.proto?q=superpack\n"
                },
                "tag": {
                    "description": "An optional list of tags describing this overlay based on the Mobile ICA\nset of labels (go/mobile-ica-labels, Mobile ICA V2). Those are meant to\npower an \"overlay suggestion\" feature, i.e. extract ICA labels from the\nlive viewfinder and find the list of overlays that share some of them.",
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1OverlayList": {
            "id": "VisionkitV1OverlayList",
            "properties": {
                "overlay": {
                    "description": "A list of Overlays.",
                    "items": {
                        "$ref": "VisionkitV1Overlay"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1Segmentation": {
            "id": "VisionkitV1Segmentation",
            "properties": {
                "categoryMask": {
                    "description": "One channel image which is encoded as an 8-bit lossless PNG. Once decoded,\nthis gives a flattened 2D-array in row major order. Each pixel in this\nmask represents the category which the pixel in the original image was\npredicted to belong to (e.g. 0 = person, 1 = dog).",
                    "format": "byte",
                    "type": "string"
                },
                "coloredLabels": {
                    "description": "List of colored labels covering all the categories that could be present in\nthe mask. For a given pixel value `i`, `colored_labels[i]` gives the\ncorresponding colored label.",
                    "items": {
                        "$ref": "VisionkitV1ColoredLabel"
                    },
                    "type": "array"
                },
                "height": {
                    "description": "Height of the mask.",
                    "format": "int64",
                    "type": "string"
                },
                "width": {
                    "description": "Width of the mask.",
                    "format": "int64",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1SegmentationList": {
            "description": "Defines segmentation results. For semantic segmentation models, the list\ncontains a single element.",
            "id": "VisionkitV1SegmentationList",
            "properties": {
                "segmentations": {
                    "description": "The list of segmentations.",
                    "items": {
                        "$ref": "VisionkitV1Segmentation"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1SimilarImage": {
            "description": "Defines a single similar image.",
            "id": "VisionkitV1SimilarImage",
            "properties": {
                "cosineSimilarity": {
                    "description": "Mandatory cosine similarity between the query and this image in [-1, 1].",
                    "format": "double",
                    "type": "number"
                },
                "geolocationDistance": {
                    "description": "Optional geolocation distance in kilometers between the location\nrepresented by this image (e.g. restaurant, storefront, etc) and the\ncoordinates specified in the request location context (where the user is\nlocated).",
                    "format": "double",
                    "type": "number"
                },
                "objectMetadata": {
                    "$ref": "VisionkitV1ObjectMetadata",
                    "description": "Mandatory object metadata describing the object represented by this image."
                }
            },
            "type": "object"
        },
        "VisionkitV1SimilarImageList": {
            "description": "Defines similar images results.",
            "id": "VisionkitV1SimilarImageList",
            "properties": {
                "similarImages": {
                    "description": "The list of similar images, ranked by descending cosine similarity.",
                    "items": {
                        "$ref": "VisionkitV1SimilarImage"
                    },
                    "type": "array"
                }
            },
            "type": "object"
        },
        "VisionkitV1TextAnnotationList": {
            "description": "Defines a list of annotations returned by text recognition cascades.",
            "id": "VisionkitV1TextAnnotationList",
            "properties": {},
            "type": "object"
        },
        "VisionkitV1UpdateOverlayRequest": {
            "description": "Defines the request format expected by UpdateOverlay.",
            "id": "VisionkitV1UpdateOverlayRequest",
            "properties": {
                "clDescription": {
                    "description": "The CL description for the proposed change.",
                    "type": "string"
                },
                "overlayConfiguration": {
                    "$ref": "VisionkitOverlayConfiguration",
                    "description": "The overlay configuration to be updated. Its `name` field must match\nan existing overlay configuration in\n(google3/photos/vision/visionkit/server/overlays). Otherwise an\nArgumentError will be returned."
                },
                "requesterLdap": {
                    "description": "The LDAP of the requester for the proposed change.",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitV1UpdateOverlayResponse": {
            "description": "Defines the response format returned by UpdateOverlay.",
            "id": "VisionkitV1UpdateOverlayResponse",
            "properties": {
                "clNumber": {
                    "description": "If the request was successful, the number of the created CL. Otherwise\neither an InvalidArgument or Internal error will be thrown.",
                    "format": "uint64",
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitVslMixerParameters": {
            "description": "The parameters needed to annotate an image using the VSL Server v1 API.",
            "id": "VisionkitVslMixerParameters",
            "properties": {
                "vertical": {
                    "description": "The name of the vertical to use when querying the AnnotateImage endpoint\nof the VSL Server v1.",
                    "enum": [
                        "UNSPECIFIED",
                        "UNIVERSAL",
                        "APPAREL",
                        "LANDMARK",
                        "TEXT",
                        "BARCODE",
                        "MEDIACOVER",
                        "ART",
                        "HARDWARE",
                        "NATURAL_WORLD",
                        "LOCAL_LANDMARK",
                        "HOMEGOODS",
                        "ALL_VERTICALS"
                    ],
                    "enumDescriptions": [
                        "Default, behaving as if no vertical is given.\nVSL server will trigger necessary modules after analyzing the image.\nNote that this cannot be specified together with any other vertical in\nVSL service request.",
                        "Triggers universal visual search.",
                        "Triggers necessary module for shoes/handbags etc.",
                        "Triggers necessary modules for landmark.",
                        "Triggers necessary modules for text.",
                        "Triggers necessary modules for barcode.",
                        "Triggers necessary modules for mediacover.",
                        "Triggers necessary modules for art.",
                        "Triggers necessary modules for hardware.",
                        "Triggers necessary modules for natural world.",
                        "Triggers necessary modules for local landmark.",
                        "Triggers necessary modules for homegoods.",
                        "Triggers all verticals which VSL supports, except UNIVERSAL.\nThis is mainly used for debugging purpose. NOT recommend for production.\nIf this is specified together with other verticals in VSL service request,\nits behavior is shown in the following examples:\n ALL_VERTICALS + BARCODE (or any other regular verticals) = ALL_VERTICALS\n ALL_VERTICALS + UNIVERSAL: Trigger all verticals which VSL supports,\n     plus UNIVERSAL.\n ALL_VERTICALS + UNSPECIFIED: Invalid input."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        },
        "VisionkitVssMatcherParameters": {
            "description": "The parameters needed to call VSS for local feature-based matching.",
            "id": "VisionkitVssMatcherParameters",
            "properties": {
                "vssMatcherModule": {
                    "enum": [
                        "VSS_MODULE_UNSPECIFIED",
                        "VE_ART_RECOGNITION",
                        "DELF_PHILEAS_LANDMARK_RECOGNITION"
                    ],
                    "enumDescriptions": [
                        "Unspecified VSS module. Specifying this as module will cause an\ninitialization-time error.",
                        "Calls the \"ve\" module for artworks recognition by specifying\n`OUTPUT_TYPE_ART` as OutputType [1].\n\n[1]:\ngoogle3/vision/visualsearch/proto/visual_search_service.proto?q=output_type_wanted",
                        "Calls the \"delf_phileas\" module for landmark recognition."
                    ],
                    "type": "string"
                }
            },
            "type": "object"
        }
    },
    "servicePath": "",
    "title": "Autopush VisionKit private API",
    "version": "v1",
    "version_module": true
}